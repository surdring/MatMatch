# 自动化定时任务配置指南

## 📋 概述

本指南说明如何配置自动化定时任务，定期执行完整的数据同步和知识库更新流程。

---

## 🎯 核心脚本

### `run_complete_pipeline.py`

**位置**：`D:\develop\python\MatMatch\run_complete_pipeline.py`

**功能**：一键执行完整流程
1. ✅ ETL数据同步（Oracle → PostgreSQL）
2. ✅ 知识库生成（基于PostgreSQL清洗后数据）
3. ✅ 知识库导入（导入到数据库）

**参数**：
```bash
# 完整流程（推荐用于定时任务）
python run_complete_pipeline.py

# 跳过ETL（如果只需更新知识库）
python run_complete_pipeline.py --skip-etl

# 跳过知识库导入（如果只需生成文件）
python run_complete_pipeline.py --skip-import
```

---

## 🚀 方案一：Windows 任务计划程序（推荐）

### 优点
- ✅ Windows原生支持，无需安装额外软件
- ✅ 可视化配置界面
- ✅ 支持失败重试和日志记录
- ✅ 可设置运行账户和权限

### 配置步骤

#### 1. 创建批处理脚本

创建 `auto_sync_pipeline.bat`：

```batch
@echo off
REM ============================================
REM 自动化数据同步和知识库更新脚本
REM 用途：定时执行完整的ETL和知识库更新流程
REM ============================================

REM 设置项目路径
cd /d D:\develop\python\MatMatch

REM 激活虚拟环境并执行完整流程
call .\venv\Scripts\activate.bat
python run_complete_pipeline.py

REM 记录执行时间
echo.
echo ============================================
echo 执行完成时间: %date% %time%
echo ============================================

REM 保持窗口打开（如果手动运行）
REM pause
```

**保存位置**：`D:\develop\python\MatMatch\auto_sync_pipeline.bat`

#### 2. 打开任务计划程序

1. 按 `Win + R`，输入 `taskschd.msc`，回车
2. 点击右侧"创建基本任务"

#### 3. 配置任务

**步骤1：常规**
- 名称：`物料查重系统-自动数据同步`
- 描述：`定时执行ETL数据同步和知识库更新`

**步骤2：触发器**

选择以下任一触发方式：

**方式A：每日执行（推荐）**
```
触发器类型：每日
开始时间：凌晨 2:00（建议在系统空闲时段）
重复执行：否
```

**方式B：每周执行**
```
触发器类型：每周
星期：星期一、星期三、星期五
开始时间：凌晨 2:00
```

**方式C：开机时执行**
```
触发器类型：启动计算机时
延迟任务：5分钟（等待系统稳定）
```

**步骤3：操作**
```
操作：启动程序
程序或脚本：D:\develop\python\MatMatch\auto_sync_pipeline.bat
起始于（可选）：D:\develop\python\MatMatch
```

**步骤4：条件**
- ☑ 只有在计算机使用交流电源时才启动此任务
- ☑ 如果计算机改用电池电源，则停止任务
- ☐ 唤醒计算机运行此任务（根据需要勾选）

**步骤5：设置**
- ☑ 如果任务失败，则每隔 10分钟 重新启动一次
- ☑ 重新启动尝试次数：3次
- ☑ 如果请求后任务还在运行，则强行停止任务
- 停止任务运行时间：2小时（根据数据量调整）

#### 4. 高级配置（可选）

右键任务 → 属性：

**常规选项卡**：
- ☑ 使用最高权限运行（如果需要）
- 配置：Windows 10

**设置选项卡**：
- ☑ 允许按需运行任务
- ☑ 如果过了计划开始时间，立即启动任务
- ☑ 如果任务连续运行时间超过以下时间，停止任务：2小时

#### 5. 测试任务

1. 右键任务 → "运行"
2. 观察任务状态
3. 检查日志文件：`logs/etl_full_sync_*.log`

---

## 🔧 方案二：Python 定时任务（APScheduler）

### 优点
- ✅ 跨平台支持（Windows/Linux）
- ✅ 灵活的定时规则
- ✅ 与Python项目集成度高

### 实现方案

创建 `scheduled_pipeline.py`：

```python
"""
自动化定时任务调度器
用途：定期执行数据同步和知识库更新
"""
import logging
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime
import subprocess
import sys
from pathlib import Path

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/scheduled_pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# 项目根目录
PROJECT_ROOT = Path(__file__).parent
PYTHON_EXE = PROJECT_ROOT / 'venv' / 'Scripts' / 'python.exe'
PIPELINE_SCRIPT = PROJECT_ROOT / 'run_complete_pipeline.py'


def run_pipeline():
    """执行完整的数据同步和知识库更新流程"""
    try:
        logger.info("=" * 60)
        logger.info("开始执行定时任务：数据同步和知识库更新")
        logger.info("=" * 60)
        
        # 调用完整流程脚本
        result = subprocess.run(
            [str(PYTHON_EXE), str(PIPELINE_SCRIPT)],
            cwd=str(PROJECT_ROOT),
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        
        if result.returncode == 0:
            logger.info("✅ 任务执行成功")
            logger.info(f"输出:\n{result.stdout}")
        else:
            logger.error(f"❌ 任务执行失败 (返回码: {result.returncode})")
            logger.error(f"错误输出:\n{result.stderr}")
        
        logger.info("=" * 60)
        logger.info("任务执行完成")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"❌ 任务执行异常: {e}", exc_info=True)


def main():
    """主函数：配置并启动调度器"""
    scheduler = BlockingScheduler()
    
    # 定时规则配置（可根据需要修改）
    
    # 方式1：每天凌晨2点执行
    scheduler.add_job(
        run_pipeline,
        CronTrigger(hour=2, minute=0),
        id='daily_sync',
        name='每日数据同步',
        replace_existing=True
    )
    
    # 方式2：每周一、三、五凌晨2点执行（可选）
    # scheduler.add_job(
    #     run_pipeline,
    #     CronTrigger(day_of_week='mon,wed,fri', hour=2, minute=0),
    #     id='weekly_sync',
    #     name='每周数据同步',
    #     replace_existing=True
    # )
    
    # 方式3：每4小时执行一次（可选）
    # scheduler.add_job(
    #     run_pipeline,
    #     CronTrigger(hour='*/4'),
    #     id='hourly_sync',
    #     name='定时数据同步',
    #     replace_existing=True
    # )
    
    logger.info("📅 定时任务调度器已启动")
    logger.info("当前已配置的任务：")
    for job in scheduler.get_jobs():
        logger.info(f"  - {job.name} (ID: {job.id})")
        logger.info(f"    下次执行时间: {job.next_run_time}")
    
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("调度器已停止")


if __name__ == '__main__':
    main()
```

### 安装依赖

```bash
# 激活虚拟环境
.\venv\Scripts\activate

# 安装APScheduler
pip install apscheduler

# 更新requirements.txt
echo apscheduler==3.10.4 >> backend\requirements.txt
```

### 运行方式

**前台运行（调试）**：
```bash
python scheduled_pipeline.py
```

**后台运行（Windows服务方式）**：

1. 安装 `pywin32`：
```bash
pip install pywin32
```

2. 创建Windows服务脚本 `install_service.py`：

```python
import win32serviceutil
import win32service
import win32event
import servicemanager
import socket
import sys
from scheduled_pipeline import main as run_scheduler


class PipelineSchedulerService(win32serviceutil.ServiceFramework):
    _svc_name_ = "MaterialSearchPipelineScheduler"
    _svc_display_name_ = "物料查重系统 - 定时任务调度器"
    _svc_description_ = "定时执行数据同步和知识库更新任务"

    def __init__(self, args):
        win32serviceutil.ServiceFramework.__init__(self, args)
        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)
        socket.setdefaulttimeout(60)

    def SvcStop(self):
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        win32event.SetEvent(self.hWaitStop)

    def SvcDoRun(self):
        servicemanager.LogMsg(
            servicemanager.EVENTLOG_INFORMATION_TYPE,
            servicemanager.PYS_SERVICE_STARTED,
            (self._svc_name_, '')
        )
        run_scheduler()


if __name__ == '__main__':
    if len(sys.argv) == 1:
        servicemanager.Initialize()
        servicemanager.PrepareToHostSingle(PipelineSchedulerService)
        servicemanager.StartServiceCtrlDispatcher()
    else:
        win32serviceutil.HandleCommandLine(PipelineSchedulerService)
```

3. 安装服务：
```bash
python install_service.py install
python install_service.py start
```

---

## 🐧 方案三：Linux Crontab（如部署到Linux服务器）

### 配置步骤

1. **编辑crontab**：
```bash
crontab -e
```

2. **添加定时任务**：

```bash
# 每天凌晨2点执行
0 2 * * * cd /path/to/MatMatch && ./venv/bin/python run_complete_pipeline.py >> logs/cron.log 2>&1

# 每周一、三、五凌晨2点执行
0 2 * * 1,3,5 cd /path/to/MatMatch && ./venv/bin/python run_complete_pipeline.py >> logs/cron.log 2>&1

# 每4小时执行一次
0 */4 * * * cd /path/to/MatMatch && ./venv/bin/python run_complete_pipeline.py >> logs/cron.log 2>&1
```

3. **查看已配置的任务**：
```bash
crontab -l
```

---

## 📊 监控和日志

### 日志文件位置

1. **ETL同步日志**：
   - 路径：`logs/etl_full_sync_YYYYMMDD_HHMMSS.log`
   - 内容：数据同步详细过程

2. **知识库生成日志**：
   - 路径：`database/knowledge_generation_YYYYMMDD_HHMMSS.log`
   - 内容：知识库生成统计

3. **知识库导入日志**：
   - 路径：`logs/knowledge_import_YYYYMMDD_HHMMSS.log`
   - 内容：导入结果验证

4. **调度器日志**（如使用APScheduler）：
   - 路径：`logs/scheduled_pipeline.log`
   - 内容：定时任务执行记录

### 监控脚本

创建 `check_pipeline_status.py`：

```python
"""
监控脚本：检查最近的流程执行状态
"""
import sys
from pathlib import Path
from datetime import datetime, timedelta

def check_latest_logs():
    """检查最近的日志文件"""
    logs_dir = Path('logs')
    today = datetime.now()
    
    print("=" * 60)
    print(f"流程执行状态检查 - {today.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # 检查ETL日志
    etl_logs = sorted(logs_dir.glob('etl_full_sync_*.log'), reverse=True)
    if etl_logs:
        latest_etl = etl_logs[0]
        print(f"\n✅ 最新ETL同步日志: {latest_etl.name}")
        # 读取最后几行
        with open(latest_etl, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print("  最后状态:")
            for line in lines[-5:]:
                print(f"    {line.strip()}")
    else:
        print("\n❌ 未找到ETL同步日志")
    
    # 检查知识库生成
    db_dir = Path('database')
    kb_logs = sorted(db_dir.glob('knowledge_generation_*.log'), reverse=True)
    if kb_logs:
        latest_kb = kb_logs[0]
        print(f"\n✅ 最新知识库生成日志: {latest_kb.name}")
    else:
        print("\n❌ 未找到知识库生成日志")
    
    print("\n" + "=" * 60)

if __name__ == '__main__':
    check_latest_logs()
```

---

## ⚙️ 高级配置选项

### 1. 失败通知

在 `run_complete_pipeline.py` 中添加邮件通知：

```python
import smtplib
from email.mime.text import MIMEText

def send_failure_notification(error_msg):
    """发送失败通知邮件"""
    msg = MIMEText(f"数据同步任务失败:\n{error_msg}", 'plain', 'utf-8')
    msg['Subject'] = '【告警】物料查重系统 - 数据同步失败'
    msg['From'] = 'system@company.com'
    msg['To'] = 'admin@company.com'
    
    try:
        with smtplib.SMTP('smtp.company.com', 25) as server:
            server.send_message(msg)
    except Exception as e:
        logger.error(f"发送邮件失败: {e}")
```

### 2. 性能优化

如果数据量巨大，可以配置增量同步：

```bash
# 仅同步最近7天的数据
python run_complete_pipeline.py --incremental --days=7
```

需在 `run_complete_pipeline.py` 中实现增量逻辑。

### 3. 资源限制

在任务计划程序中设置：
- CPU使用率限制
- 内存使用限制
- 网络带宽限制

---

## 🔍 故障排查

### 问题1：任务未执行

**检查步骤**：
1. 确认任务计划程序服务已启动：`services.msc` → "Task Scheduler"
2. 查看任务历史记录：任务计划程序 → 右键任务 → 历史记录
3. 检查触发器配置是否正确

### 问题2：任务执行失败

**检查步骤**：
1. 查看日志文件：`logs/etl_full_sync_*.log`
2. 检查数据库连接：Oracle和PostgreSQL是否可访问
3. 检查虚拟环境：`venv` 是否损坏
4. 手动执行脚本测试：`python run_complete_pipeline.py`

### 问题3：任务执行时间过长

**优化方案**：
1. 调整批处理大小：修改 `BATCH_SIZE` 参数
2. 启用增量同步：只同步变更数据
3. 优化数据库索引：确保查询性能
4. 使用并行处理：多线程/多进程

---

## 📋 推荐配置

### 生产环境推荐配置

**执行频率**：
- 每天凌晨2:00执行完整同步（推荐）
- 或每周一、三、五凌晨2:00执行

**监控配置**：
- 启用邮件通知
- 保留最近30天的日志
- 配置失败重试（3次，间隔10分钟）

**资源配置**：
- 最大执行时间：2小时
- 内存限制：4GB
- CPU优先级：低（不影响业务系统）

**备份策略**：
- 同步前自动备份数据库
- 保留最近7天的备份

---

## 📝 配置检查清单

部署前检查：

- [ ] `run_complete_pipeline.py` 脚本测试通过
- [ ] `auto_sync_pipeline.bat` 路径配置正确
- [ ] 虚拟环境依赖完整
- [ ] Oracle和PostgreSQL连接正常
- [ ] 日志目录有写入权限
- [ ] 任务计划程序配置完成
- [ ] 触发器时间设置合理
- [ ] 失败重试机制已启用
- [ ] 日志轮转策略已配置
- [ ] 监控告警已配置（可选）

---

## 🎯 下一步

配置完成后：

1. **首次测试**：手动运行任务，观察执行情况
2. **监控验证**：连续观察3-5次自动执行
3. **性能优化**：根据实际数据量调整参数
4. **文档更新**：记录配置细节和运维经验

---

**最后更新**：2025-10-07
**维护人员**：[您的名字]

