"""
Backendå¼‚æ­¥çŸ¥è¯†åº“å¯¼å…¥è„šæœ¬

è¿™æ˜¯éªŒè¯"å¯¹ç§°å¤„ç†"åŸåˆ™çš„å…³é”®è„šæœ¬ï¼š
- ä½¿ç”¨ä¸SQLå¯¼å…¥å®Œå…¨ç›¸åŒçš„æ•°æ®æºï¼ˆmaterial_knowledge_generator.pyç”Ÿæˆçš„JSONæ–‡ä»¶ï¼‰
- ä½¿ç”¨Backendçš„SQLAlchemyå¼‚æ­¥æ¨¡å‹è¿›è¡Œå¯¼å…¥
- å¯¹æ¯”SQLå¯¼å…¥å’ŒPythonå¯¼å…¥çš„ç»“æœï¼ŒéªŒè¯ä¸¤ç§æ–¹å¼çš„ä¸€è‡´æ€§

å…³è”æ¸…å•ç‚¹:
- [T.1] éªŒè¯Backendæ¨¡å‹ä¸æ•°æ®åº“schemaçš„ä¸€è‡´æ€§
- [T.2] éªŒè¯å¼‚æ­¥å¯¼å…¥çš„æ­£ç¡®æ€§
- [I.2] å®ç°å¯¹ç§°å¤„ç†åŸåˆ™
"""

import asyncio
import json
import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from sqlalchemy import select, func, text
from sqlalchemy.ext.asyncio import AsyncSession

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'backend'))

from backend.database.session import db_manager
from backend.models.base import Base
from backend.models.materials import ExtractionRule, Synonym, KnowledgeCategory

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/knowledge_base_import_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class KnowledgeBaseImporter:
    """
    çŸ¥è¯†åº“å¼‚æ­¥å¯¼å…¥å™¨
    
    ä½¿ç”¨Backendçš„SQLAlchemyæ¨¡å‹å°†çŸ¥è¯†åº“æ•°æ®å¯¼å…¥PostgreSQL
    å®ç°"å¯¹ç§°å¤„ç†"åŸåˆ™
    """
    
    def __init__(self, data_dir: Path):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        
        Args:
            data_dir: çŸ¥è¯†åº“JSONæ–‡ä»¶æ‰€åœ¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.stats = {
            'rules_imported': 0,
            'synonyms_imported': 0,
            'categories_imported': 0,
            'rules_skipped': 0,
            'synonyms_skipped': 0,
            'categories_skipped': 0,
            'errors': []
        }
    
    def find_latest_files(self) -> Dict[str, Path]:
        """
        æŸ¥æ‰¾æœ€æ–°çš„çŸ¥è¯†åº“æ–‡ä»¶
        
        Returns:
            åŒ…å«æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        logger.info("ğŸ” æŸ¥æ‰¾æœ€æ–°çš„çŸ¥è¯†åº“æ–‡ä»¶...")
        
        files = {
            'rules': None,
            'synonyms': None,
            'categories': None
        }
        
        # æŸ¥æ‰¾æå–è§„åˆ™æ–‡ä»¶
        rules_files = list(self.data_dir.glob('standardized_extraction_rules_*.json'))
        if rules_files:
            files['rules'] = max(rules_files, key=lambda p: p.stat().st_mtime)
            logger.info(f"  âœ… æå–è§„åˆ™: {files['rules'].name}")
        
        # æŸ¥æ‰¾åŒä¹‰è¯è®°å½•æ–‡ä»¶ï¼ˆæ³¨æ„ï¼šä½¿ç”¨recordsæ–‡ä»¶ï¼Œä¸æ˜¯dictionaryæ–‡ä»¶ï¼‰
        synonym_files = list(self.data_dir.glob('standardized_synonym_records_*.json'))
        if synonym_files:
            files['synonyms'] = max(synonym_files, key=lambda p: p.stat().st_mtime)
            logger.info(f"  âœ… åŒä¹‰è¯è®°å½•: {files['synonyms'].name}")
        
        # æŸ¥æ‰¾åˆ†ç±»å…³é”®è¯æ–‡ä»¶
        category_files = list(self.data_dir.glob('standardized_category_keywords_*.json'))
        if category_files:
            files['categories'] = max(category_files, key=lambda p: p.stat().st_mtime)
            logger.info(f"  âœ… åˆ†ç±»å…³é”®è¯: {files['categories'].name}")
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–‡ä»¶éƒ½æ‰¾åˆ°
        missing = [k for k, v in files.items() if v is None]
        if missing:
            raise FileNotFoundError(f"ç¼ºå°‘çŸ¥è¯†åº“æ–‡ä»¶: {', '.join(missing)}")
        
        return files
    
    async def clear_existing_data(self, session: AsyncSession):
        """
        æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆç”¨äºé‡æ–°å¯¼å…¥ï¼‰
        
        Args:
            session: æ•°æ®åº“ä¼šè¯
        """
        logger.info("ğŸ—‘ï¸ æ¸…ç©ºç°æœ‰çŸ¥è¯†åº“æ•°æ®...")
        
        try:
            # åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®
            await session.execute(text("DELETE FROM extraction_rules"))
            await session.execute(text("DELETE FROM synonyms"))
            await session.execute(text("DELETE FROM knowledge_categories"))
            await session.commit()
            logger.info("  âœ… ç°æœ‰æ•°æ®å·²æ¸…ç©º")
        except Exception as e:
            logger.error(f"  âŒ æ¸…ç©ºæ•°æ®å¤±è´¥: {e}")
            await session.rollback()
            raise
    
    async def import_extraction_rules(self, session: AsyncSession, file_path: Path):
        """
        å¯¼å…¥æå–è§„åˆ™
        
        Args:
            session: æ•°æ®åº“ä¼šè¯
            file_path: è§„åˆ™JSONæ–‡ä»¶è·¯å¾„
        """
        logger.info("ğŸ”§ å¼€å§‹å¯¼å…¥æå–è§„åˆ™...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                rules_data = json.load(f)
            
            logger.info(f"  ğŸ“Š è¯»å–åˆ° {len(rules_data)} æ¡è§„åˆ™")
            
            for rule_dict in rules_data:
                try:
                    # åˆ›å»ºExtractionRuleå¯¹è±¡
                    # æ³¨æ„ï¼šä¸è®¾ç½®idï¼Œè®©æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆ
                    rule = ExtractionRule(
                        rule_name=rule_dict['rule_name'],
                        material_category=rule_dict['material_category'],
                        attribute_name=rule_dict['attribute_name'],
                        regex_pattern=rule_dict['regex_pattern'],
                        priority=rule_dict.get('priority', 50),
                        confidence=rule_dict.get('confidence', 0.8),
                        is_active=rule_dict.get('is_active', True),
                        version=rule_dict.get('version', 1),
                        description=rule_dict.get('description', ''),
                        example_input=rule_dict.get('example_input'),
                        example_output=rule_dict.get('example_output'),
                        created_by=rule_dict.get('created_by', 'system')
                    )
                    
                    session.add(rule)
                    self.stats['rules_imported'] += 1
                    
                except Exception as e:
                    logger.warning(f"  âš ï¸ è·³è¿‡è§„åˆ™ {rule_dict.get('rule_name', 'unknown')}: {e}")
                    self.stats['rules_skipped'] += 1
                    self.stats['errors'].append({
                        'type': 'rule',
                        'data': rule_dict,
                        'error': str(e)
                    })
            
            await session.commit()
            logger.info(f"  âœ… æˆåŠŸå¯¼å…¥ {self.stats['rules_imported']} æ¡è§„åˆ™")
            
            if self.stats['rules_skipped'] > 0:
                logger.warning(f"  âš ï¸ è·³è¿‡ {self.stats['rules_skipped']} æ¡è§„åˆ™")
        
        except Exception as e:
            logger.error(f"  âŒ å¯¼å…¥è§„åˆ™å¤±è´¥: {e}")
            await session.rollback()
            raise
    
    async def import_synonyms(self, session: AsyncSession, file_path: Path):
        """
        å¯¼å…¥åŒä¹‰è¯
        
        Args:
            session: æ•°æ®åº“ä¼šè¯
            file_path: åŒä¹‰è¯è®°å½•JSONæ–‡ä»¶è·¯å¾„
        """
        logger.info("ğŸ“š å¼€å§‹å¯¼å…¥åŒä¹‰è¯...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                synonyms_data = json.load(f)
            
            logger.info(f"  ğŸ“Š è¯»å–åˆ° {len(synonyms_data)} æ¡åŒä¹‰è¯è®°å½•")
            
            # æ‰¹é‡å¯¼å…¥ï¼Œæ¯1000æ¡æäº¤ä¸€æ¬¡
            batch_size = 1000
            for i, syn_dict in enumerate(synonyms_data):
                try:
                    # åˆ›å»ºSynonymå¯¹è±¡
                    synonym = Synonym(
                        original_term=syn_dict['original_term'],
                        standard_term=syn_dict['standard_term'],
                        category=syn_dict.get('category', 'general'),
                        synonym_type=syn_dict.get('synonym_type', 'general'),
                        confidence=syn_dict.get('confidence', 1.0)
                    )
                    
                    session.add(synonym)
                    self.stats['synonyms_imported'] += 1
                    
                    # æ‰¹é‡æäº¤
                    if (i + 1) % batch_size == 0:
                        await session.commit()
                        logger.info(f"  â³ å·²å¯¼å…¥ {i + 1}/{len(synonyms_data)} æ¡åŒä¹‰è¯...")
                    
                except Exception as e:
                    logger.warning(f"  âš ï¸ è·³è¿‡åŒä¹‰è¯ {syn_dict.get('original_term', 'unknown')}: {e}")
                    self.stats['synonyms_skipped'] += 1
                    if len(self.stats['errors']) < 100:  # åªè®°å½•å‰100ä¸ªé”™è¯¯
                        self.stats['errors'].append({
                            'type': 'synonym',
                            'data': syn_dict,
                            'error': str(e)
                        })
            
            # æäº¤å‰©ä½™çš„è®°å½•
            await session.commit()
            logger.info(f"  âœ… æˆåŠŸå¯¼å…¥ {self.stats['synonyms_imported']} æ¡åŒä¹‰è¯")
            
            if self.stats['synonyms_skipped'] > 0:
                logger.warning(f"  âš ï¸ è·³è¿‡ {self.stats['synonyms_skipped']} æ¡åŒä¹‰è¯")
        
        except Exception as e:
            logger.error(f"  âŒ å¯¼å…¥åŒä¹‰è¯å¤±è´¥: {e}")
            await session.rollback()
            raise
    
    async def import_categories(self, session: AsyncSession, file_path: Path):
        """
        å¯¼å…¥åˆ†ç±»å…³é”®è¯ï¼ˆä½¿ç”¨åŸå§‹SQLï¼Œä¸SQLå¯¼å…¥è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
        
        Args:
            session: æ•°æ®åº“ä¼šè¯
            file_path: åˆ†ç±»å…³é”®è¯JSONæ–‡ä»¶è·¯å¾„
        """
        logger.info("ğŸ·ï¸ å¼€å§‹å¯¼å…¥åˆ†ç±»å…³é”®è¯...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                categories_data = json.load(f)
            
            logger.info(f"  ğŸ“Š è¯»å–åˆ° {len(categories_data)} ä¸ªåˆ†ç±»")
            
            for category_name, keywords in categories_data.items():
                try:
                    # ç¡®å®šåˆ†ç±»ç±»å‹å’Œä¼˜å…ˆçº§
                    category_type = 'material_type'
                    priority = 50
                    detection_confidence = 0.8
                    
                    # æ ¹æ®åˆ†ç±»åç§°è°ƒæ•´ä¼˜å…ˆçº§
                    high_priority_categories = ['bearing', 'valve', 'pump', 'motor']
                    if category_name in high_priority_categories:
                        priority = 80
                    
                    # keywordså¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸
                    if isinstance(keywords, dict):
                        keywords_list = keywords.get('keywords', [])
                    elif isinstance(keywords, list):
                        keywords_list = keywords
                    else:
                        keywords_list = []
                    
                    # ä½¿ç”¨åŸå§‹SQLæ’å…¥ï¼ˆä¸SQLè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
                    from sqlalchemy import text
                    sql = text("""
                        INSERT INTO knowledge_categories 
                        (category_name, keywords, detection_confidence, category_type, priority, is_active, created_by) 
                        VALUES (:name, :keywords, :confidence, :type, :priority, true, 'system')
                    """)
                    
                    await session.execute(sql, {
                        'name': category_name,
                        'keywords': keywords_list,
                        'confidence': detection_confidence,
                        'type': category_type,
                        'priority': priority
                    })
                    
                    self.stats['categories_imported'] += 1
                    
                except Exception as e:
                    logger.warning(f"  âš ï¸ è·³è¿‡åˆ†ç±» {category_name}: {e}")
                    self.stats['categories_skipped'] += 1
                    self.stats['errors'].append({
                        'type': 'category',
                        'data': {'category_name': category_name, 'keywords': keywords},
                        'error': str(e)
                    })
            
            await session.commit()
            logger.info(f"  âœ… æˆåŠŸå¯¼å…¥ {self.stats['categories_imported']} ä¸ªåˆ†ç±»")
            
            if self.stats['categories_skipped'] > 0:
                logger.warning(f"  âš ï¸ è·³è¿‡ {self.stats['categories_skipped']} ä¸ªåˆ†ç±»")
        
        except Exception as e:
            logger.error(f"  âŒ å¯¼å…¥åˆ†ç±»å¤±è´¥: {e}")
            await session.rollback()
            raise
    
    async def verify_import(self, session: AsyncSession):
        """
        éªŒè¯å¯¼å…¥ç»“æœ
        
        Args:
            session: æ•°æ®åº“ä¼šè¯
        """
        logger.info("=" * 80)
        logger.info("ğŸ” éªŒè¯å¯¼å…¥ç»“æœ...")
        logger.info("=" * 80)
        
        try:
            # ç»Ÿè®¡å„è¡¨è®°å½•æ•°
            rules_count = await session.scalar(
                select(func.count()).select_from(ExtractionRule).where(ExtractionRule.is_active == True)
            )
            synonyms_count = await session.scalar(
                select(func.count()).select_from(Synonym).where(Synonym.is_active == True)
            )
            # ä½¿ç”¨åŸå§‹SQLæŸ¥è¯¢åˆ†ç±»æ•°ï¼ˆä¸è¡¨ç»“æ„åŒ¹é…ï¼‰
            categories_result = await session.execute(
                text("SELECT COUNT(*) FROM knowledge_categories WHERE is_active = TRUE")
            )
            categories_count = categories_result.scalar()
            
            logger.info(f"ğŸ“Š æ•°æ®åº“è®°å½•ç»Ÿè®¡:")
            logger.info(f"  - æå–è§„åˆ™: {rules_count} æ¡")
            logger.info(f"  - åŒä¹‰è¯: {synonyms_count} æ¡")
            logger.info(f"  - åˆ†ç±»å…³é”®è¯: {categories_count} ä¸ª")
            
            # æŸ¥è¯¢å‡ æ¡æ ·ä¾‹æ•°æ®
            logger.info(f"\nğŸ“‹ æå–è§„åˆ™æ ·ä¾‹ (å‰5æ¡):")
            result = await session.execute(
                select(ExtractionRule)
                .where(ExtractionRule.is_active == True)
                .order_by(ExtractionRule.priority.desc())
                .limit(5)
            )
            rules = result.scalars().all()
            for rule in rules:
                logger.info(f"  - [{rule.id}] {rule.rule_name} (ä¼˜å…ˆçº§: {rule.priority}, ç½®ä¿¡åº¦: {rule.confidence})")
            
            logger.info(f"\nğŸ“š åŒä¹‰è¯ç±»å‹ç»Ÿè®¡:")
            result = await session.execute(
                select(Synonym.synonym_type, func.count(Synonym.id))
                .where(Synonym.is_active == True)
                .group_by(Synonym.synonym_type)
                .order_by(func.count(Synonym.id).desc())
            )
            syn_stats = result.all()
            for syn_type, count in syn_stats:
                logger.info(f"  - {syn_type}: {count} æ¡")
            
            logger.info(f"\nğŸ·ï¸ åˆ†ç±»å…³é”®è¯æ ·ä¾‹:")
            # ä½¿ç”¨åŸå§‹SQLæŸ¥è¯¢åˆ†ç±»æ ·ä¾‹
            categories_result = await session.execute(
                text("""
                    SELECT category_name, keywords, priority 
                    FROM knowledge_categories 
                    WHERE is_active = TRUE 
                    ORDER BY category_name 
                    LIMIT 5
                """)
            )
            categories = categories_result.fetchall()
            for cat in categories:
                keyword_count = len(cat.keywords) if cat.keywords else 0
                logger.info(f"  - {cat.category_name}: {keyword_count} ä¸ªå…³é”®è¯ (ä¼˜å…ˆçº§: {cat.priority})")
            
            logger.info("=" * 80)
            
            # å¯¹æ¯”å¯¼å…¥ç»Ÿè®¡
            logger.info(f"ğŸ“ˆ å¯¼å…¥ç»Ÿè®¡å¯¹æ¯”:")
            logger.info(f"  æå–è§„åˆ™: å¯¼å…¥ {self.stats['rules_imported']}, æ•°æ®åº“ {rules_count}")
            logger.info(f"  åŒä¹‰è¯: å¯¼å…¥ {self.stats['synonyms_imported']}, æ•°æ®åº“ {synonyms_count}")
            logger.info(f"  åˆ†ç±»å…³é”®è¯: å¯¼å…¥ {self.stats['categories_imported']}, æ•°æ®åº“ {categories_count}")
            
            # éªŒè¯ä¸€è‡´æ€§
            if (self.stats['rules_imported'] == rules_count and
                self.stats['synonyms_imported'] == synonyms_count and
                self.stats['categories_imported'] == categories_count):
                logger.info("âœ… å¯¼å…¥æ•°æ®å®Œå…¨ä¸€è‡´ï¼")
                return True
            else:
                logger.warning("âš ï¸ å¯¼å…¥æ•°æ®ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                return False
        
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return False
    
    async def run(self, clear_existing: bool = False):
        """
        æ‰§è¡Œå®Œæ•´çš„å¯¼å…¥æµç¨‹
        
        Args:
            clear_existing: æ˜¯å¦æ¸…ç©ºç°æœ‰æ•°æ®
        """
        logger.info("=" * 80)
        logger.info("ğŸš€ BackendçŸ¥è¯†åº“å¼‚æ­¥å¯¼å…¥å™¨å¯åŠ¨")
        logger.info("=" * 80)
        
        try:
            # æŸ¥æ‰¾æ–‡ä»¶
            files = self.find_latest_files()
            
            # åˆå§‹åŒ–æ•°æ®åº“
            logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
            engine = db_manager.create_engine()
            session_maker = db_manager.create_session_maker()
            
            # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            async with engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            
            # åˆ›å»ºä¼šè¯
            async with session_maker() as session:
                # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if clear_existing:
                    await self.clear_existing_data(session)
                
                # å¯¼å…¥æ•°æ®
                await self.import_extraction_rules(session, files['rules'])
                await self.import_synonyms(session, files['synonyms'])
                await self.import_categories(session, files['categories'])
                
                # éªŒè¯å¯¼å…¥
                success = await self.verify_import(session)
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            logger.info("=" * 80)
            logger.info("ğŸ‰ å¯¼å…¥å®Œæˆï¼æœ€ç»ˆç»Ÿè®¡:")
            logger.info("=" * 80)
            logger.info(f"âœ… æˆåŠŸå¯¼å…¥:")
            logger.info(f"  - æå–è§„åˆ™: {self.stats['rules_imported']} æ¡")
            logger.info(f"  - åŒä¹‰è¯: {self.stats['synonyms_imported']} æ¡")
            logger.info(f"  - åˆ†ç±»å…³é”®è¯: {self.stats['categories_imported']} ä¸ª")
            
            if self.stats['rules_skipped'] or self.stats['synonyms_skipped'] or self.stats['categories_skipped']:
                logger.info(f"âš ï¸ è·³è¿‡è®°å½•:")
                logger.info(f"  - æå–è§„åˆ™: {self.stats['rules_skipped']} æ¡")
                logger.info(f"  - åŒä¹‰è¯: {self.stats['synonyms_skipped']} æ¡")
                logger.info(f"  - åˆ†ç±»å…³é”®è¯: {self.stats['categories_skipped']} ä¸ª")
            
            logger.info("=" * 80)
            
            return success
        
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            return False


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='BackendçŸ¥è¯†åº“å¼‚æ­¥å¯¼å…¥è„šæœ¬')
    parser.add_argument('--data-dir', type=str, default='../database',
                       help='çŸ¥è¯†åº“JSONæ–‡ä»¶æ‰€åœ¨ç›®å½•')
    parser.add_argument('--clear', action='store_true',
                       help='æ¸…ç©ºç°æœ‰æ•°æ®åé‡æ–°å¯¼å…¥')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯¼å…¥å™¨
    importer = KnowledgeBaseImporter(args.data_dir)
    
    # æ‰§è¡Œå¯¼å…¥
    success = await importer.run(clear_existing=args.clear)
    
    if success:
        print("\nğŸŠ æ­å–œï¼çŸ¥è¯†åº“å¯¼å…¥æˆåŠŸï¼")
        return 0
    else:
        print("\nğŸ’¥ å¯¼å…¥å¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
