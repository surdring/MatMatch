"""
PostgreSQLè§„åˆ™å’Œè¯å…¸åˆå§‹åŒ–è„šæœ¬
å°†ç”Ÿæˆçš„æå–è§„åˆ™å’ŒåŒä¹‰è¯å…¸å¯¼å…¥åˆ°PostgreSQLæ•°æ®åº“
"""

import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List
import asyncpg
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PostgreSQLRuleInitializer:
    """PostgreSQLè§„åˆ™åˆå§‹åŒ–å™¨"""
    
    def __init__(self, pg_config: Dict[str, str]):
        self.pg_config = pg_config
        self.connection = None
    
    async def connect(self):
        """è¿æ¥PostgreSQLæ•°æ®åº“"""
        try:
            self.connection = await asyncpg.connect(
                host=self.pg_config['host'],
                port=self.pg_config['port'],
                database=self.pg_config['database'],
                user=self.pg_config['username'],
                password=self.pg_config['password']
            )
            logger.info("âœ… PostgreSQLæ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ PostgreSQLè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.connection:
            await self.connection.close()
            logger.info("ğŸ”Œ PostgreSQLè¿æ¥å·²æ–­å¼€")
    
    async def create_tables(self):
        """åˆ›å»ºå¿…è¦çš„è¡¨ç»“æ„"""
        logger.info("ğŸ—ï¸ åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„...")
        
        # åˆ›å»ºæå–è§„åˆ™è¡¨
        create_rules_table = """
        CREATE TABLE IF NOT EXISTS extraction_rules (
            id SERIAL PRIMARY KEY,
            rule_name VARCHAR(100) NOT NULL,
            material_category VARCHAR(100) NOT NULL,
            attribute_name VARCHAR(50) NOT NULL,
            regex_pattern TEXT NOT NULL,
            priority INTEGER DEFAULT 100,
            is_active BOOLEAN DEFAULT TRUE,
            description TEXT,
            example_input TEXT,
            example_output TEXT,
            version INTEGER DEFAULT 1,
            created_by VARCHAR(50) DEFAULT 'system',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        # åˆ›å»ºåŒä¹‰è¯è¡¨
        create_synonyms_table = """
        CREATE TABLE IF NOT EXISTS synonyms (
            id SERIAL PRIMARY KEY,
            original_term VARCHAR(100) NOT NULL,
            standard_term VARCHAR(100) NOT NULL,
            category VARCHAR(50) DEFAULT 'general',
            synonym_type VARCHAR(20) DEFAULT 'general',
            is_active BOOLEAN DEFAULT TRUE,
            confidence DECIMAL(3,2) DEFAULT 1.0,
            description TEXT,
            created_by VARCHAR(50) DEFAULT 'system',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        # åˆ›å»ºç‰©æ–™ç±»åˆ«è¡¨
        create_categories_table = """
        CREATE TABLE IF NOT EXISTS material_categories (
            id SERIAL PRIMARY KEY,
            oracle_category_id VARCHAR(20) UNIQUE,
            category_code VARCHAR(40) NOT NULL,
            category_name VARCHAR(200) NOT NULL,
            parent_category_id VARCHAR(20),
            enable_state INTEGER DEFAULT 2,
            detection_keywords TEXT[],
            category_description TEXT,
            processing_rules JSONB DEFAULT '{}',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        try:
            await self.connection.execute(create_rules_table)
            await self.connection.execute(create_synonyms_table)
            await self.connection.execute(create_categories_table)
            logger.info("âœ… æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
            return False
    
    async def import_extraction_rules(self, rules_file: str):
        """å¯¼å…¥æå–è§„åˆ™"""
        logger.info(f"ğŸ“¥ å¯¼å…¥æå–è§„åˆ™: {rules_file}")
        
        with open(rules_file, 'r', encoding='utf-8') as f:
            rules = json.load(f)
        
        # æ¸…ç©ºç°æœ‰è§„åˆ™
        await self.connection.execute("DELETE FROM extraction_rules WHERE created_by = 'system'")
        
        # æ‰¹é‡æ’å…¥è§„åˆ™
        insert_query = """
        INSERT INTO extraction_rules 
        (rule_name, material_category, attribute_name, regex_pattern, priority, 
         description, example_input, example_output, created_by)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, 'system')
        """
        
        for rule in rules:
            await self.connection.execute(
                insert_query,
                rule['rule_name'],
                rule['material_category'],
                rule['attribute_name'],
                rule['regex_pattern'],
                rule['priority'],
                rule.get('description', ''),
                rule.get('example_input', ''),
                rule.get('example_output', '')
            )
        
        logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {len(rules)} æ¡æå–è§„åˆ™")
        return len(rules)
    
    async def import_synonym_dictionary(self, dict_file: str):
        """å¯¼å…¥åŒä¹‰è¯å…¸"""
        logger.info(f"ğŸ“¥ å¯¼å…¥åŒä¹‰è¯å…¸: {dict_file}")
        
        with open(dict_file, 'r', encoding='utf-8') as f:
            synonym_dict = json.load(f)
        
        # æ¸…ç©ºç°æœ‰åŒä¹‰è¯
        await self.connection.execute("DELETE FROM synonyms WHERE created_by = 'system'")
        
        # æ‰¹é‡æ’å…¥åŒä¹‰è¯
        insert_query = """
        INSERT INTO synonyms 
        (original_term, standard_term, category, synonym_type, created_by)
        VALUES ($1, $2, $3, $4, 'system')
        """
        
        total_synonyms = 0
        for standard_term, variants in synonym_dict.items():
            # ç¡®å®šåŒä¹‰è¯ç±»å‹
            synonym_type = self._detect_synonym_type(standard_term)
            category = self._detect_synonym_category(standard_term)
            
            for variant in variants:
                await self.connection.execute(
                    insert_query,
                    variant,
                    standard_term,
                    category,
                    synonym_type
                )
                total_synonyms += 1
        
        logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {total_synonyms} æ¡åŒä¹‰è¯")
        return total_synonyms
    
    async def import_material_categories(self, categories_data: List[Dict]):
        """å¯¼å…¥ç‰©æ–™åˆ†ç±»æ•°æ®"""
        logger.info("ğŸ“‚ å¯¼å…¥ç‰©æ–™åˆ†ç±»æ•°æ®...")
        
        # æ¸…ç©ºç°æœ‰åˆ†ç±»
        await self.connection.execute("DELETE FROM material_categories WHERE oracle_category_id IS NOT NULL")
        
        # æ‰¹é‡æ’å…¥åˆ†ç±»
        insert_query = """
        INSERT INTO material_categories 
        (oracle_category_id, category_code, category_name, parent_category_id, 
         enable_state, detection_keywords, category_description)
        VALUES ($1, $2, $3, $4, $5, $6, $7)
        """
        
        for category in categories_data:
            # ç”Ÿæˆæ£€æµ‹å…³é”®è¯
            keywords = self._generate_category_keywords(category['CATEGORY_NAME'])
            
            await self.connection.execute(
                insert_query,
                category['CATEGORY_ID'],
                category['CATEGORY_CODE'],
                category['CATEGORY_NAME'],
                category.get('PARENT_CATEGORY_ID'),
                category.get('ENABLE_STATE', 2),
                keywords,
                f"åŸºäºOracleåˆ†ç±»: {category['CATEGORY_NAME']}"
            )
        
        logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {len(categories_data)} ä¸ªç‰©æ–™åˆ†ç±»")
        return len(categories_data)
    
    def _detect_synonym_type(self, term: str) -> str:
        """æ£€æµ‹åŒä¹‰è¯ç±»å‹"""
        term_lower = term.lower()
        
        # å“ç‰Œç±»å‹
        brands = ['skf', 'nsk', 'fag', 'siemens', 'abb', 'schneider', 'omron', 'parker']
        if any(brand in term_lower for brand in brands):
            return 'brand'
        
        # æè´¨ç±»å‹
        materials = ['304', '316', 'ä¸é”ˆé’¢', 'ç¢³é’¢', 'åˆé‡‘é’¢', 'é“¸é“', 'é“œ', 'é“']
        if any(material in term_lower for material in materials):
            return 'material'
        
        # è§„æ ¼ç±»å‹
        if re.search(r'\d+[Ã—*xX]\d+|DN\d+|Î¦\d+|M\d+', term):
            return 'specification'
        
        # å•ä½ç±»å‹
        units = ['mm', 'kg', 'mpa', 'bar', 'ä¸ª', 'åª', 'ä»¶', 'å¥—']
        if any(unit in term_lower for unit in units):
            return 'unit'
        
        return 'general'
    
    def _detect_synonym_category(self, term: str) -> str:
        """æ£€æµ‹åŒä¹‰è¯æ‰€å±ç±»åˆ«"""
        term_lower = term.lower()
        
        for category, keywords in {
            'bearing': ['è½´æ‰¿', 'bearing'],
            'bolt': ['èºæ “', 'èºé’‰', 'bolt', 'screw'],
            'valve': ['é˜€', 'valve'],
            'pipe': ['ç®¡', 'pipe'],
            'electrical': ['æ¥è§¦å™¨', 'ç»§ç”µå™¨', 'æ–­è·¯å™¨', 'contactor', 'relay']
        }.items():
            if any(keyword in term_lower for keyword in keywords):
                return category
        
        return 'general'
    
    def _generate_category_keywords(self, category_name: str) -> List[str]:
        """ä¸ºåˆ†ç±»ç”Ÿæˆæ£€æµ‹å…³é”®è¯"""
        keywords = [category_name]
        
        # åŸºäºåˆ†ç±»åç§°ç”Ÿæˆç›¸å…³å…³é”®è¯
        name_lower = category_name.lower()
        
        # æ·»åŠ å¸¸è§å˜ä½“
        if 'è½´æ‰¿' in name_lower:
            keywords.extend(['bearing', 'è»¸æ‰¿', 'æ»šåŠ¨', 'æ»‘åŠ¨'])
        elif 'èºæ “' in name_lower or 'èºé’‰' in name_lower:
            keywords.extend(['bolt', 'screw', 'ç´§å›ºä»¶'])
        elif 'é˜€' in name_lower:
            keywords.extend(['valve', 'é–¥'])
        elif 'ç®¡' in name_lower:
            keywords.extend(['pipe', 'tube'])
        elif 'ç”µ' in name_lower:
            keywords.extend(['electrical', 'electric'])
        
        return list(set(keywords))
    
    async def create_indexes(self):
        """åˆ›å»ºå¿…è¦çš„ç´¢å¼•"""
        logger.info("ğŸ” åˆ›å»ºæ•°æ®åº“ç´¢å¼•...")
        
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_extraction_rules_category ON extraction_rules (material_category, priority) WHERE is_active = TRUE;",
            "CREATE INDEX IF NOT EXISTS idx_synonyms_original ON synonyms (original_term) WHERE is_active = TRUE;",
            "CREATE INDEX IF NOT EXISTS idx_synonyms_category_type ON synonyms (category, synonym_type) WHERE is_active = TRUE;",
            "CREATE INDEX IF NOT EXISTS idx_categories_code ON material_categories (category_code);",
            "CREATE INDEX IF NOT EXISTS idx_categories_keywords ON material_categories USING gin (detection_keywords);"
        ]
        
        for index_sql in indexes:
            try:
                await self.connection.execute(index_sql)
            except Exception as e:
                logger.warning(f"åˆ›å»ºç´¢å¼•æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        logger.info("âœ… æ•°æ®åº“ç´¢å¼•åˆ›å»ºå®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹PostgreSQLè§„åˆ™å’Œè¯å…¸åˆå§‹åŒ–")
    
    # PostgreSQLé…ç½®
    pg_config = {
        'host': os.getenv('PG_HOST', 'localhost'),
        'port': int(os.getenv('PG_PORT', 5432)),
        'database': os.getenv('PG_DATABASE', 'matmatch'),
        'username': os.getenv('PG_USERNAME', 'matmatch'),
        'password': os.getenv('PG_PASSWORD', 'matmatch')
    }
    
    initializer = PostgreSQLRuleInitializer(pg_config)
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not await initializer.connect():
            return False
        
        # åˆ›å»ºè¡¨ç»“æ„
        if not await initializer.create_tables():
            return False
        
        # æŸ¥æ‰¾æœ€æ–°çš„ç”Ÿæˆæ–‡ä»¶
        output_dir = './output'
        if not os.path.exists(output_dir):
            logger.error(f"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
            logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ python generate_rules_and_dictionary.py ç”Ÿæˆè§„åˆ™å’Œè¯å…¸")
            return False
        
        # æŸ¥æ‰¾æœ€æ–°çš„è§„åˆ™å’Œè¯å…¸æ–‡ä»¶
        rule_files = [f for f in os.listdir(output_dir) if f.startswith('extraction_rules_') and f.endswith('.json')]
        dict_files = [f for f in os.listdir(output_dir) if f.startswith('synonym_dictionary_') and f.endswith('.json')]
        
        if not rule_files or not dict_files:
            logger.error("âŒ æœªæ‰¾åˆ°è§„åˆ™æˆ–è¯å…¸æ–‡ä»¶")
            logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ python generate_rules_and_dictionary.py ç”Ÿæˆè§„åˆ™å’Œè¯å…¸")
            return False
        
        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        latest_rules_file = os.path.join(output_dir, sorted(rule_files)[-1])
        latest_dict_file = os.path.join(output_dir, sorted(dict_files)[-1])
        
        logger.info(f"ğŸ“ ä½¿ç”¨è§„åˆ™æ–‡ä»¶: {latest_rules_file}")
        logger.info(f"ğŸ“ ä½¿ç”¨è¯å…¸æ–‡ä»¶: {latest_dict_file}")
        
        # å¯¼å…¥è§„åˆ™å’Œè¯å…¸
        rules_count = await initializer.import_extraction_rules(latest_rules_file)
        synonyms_count = await initializer.import_synonym_dictionary(latest_dict_file)
        
        # åˆ›å»ºç´¢å¼•
        await initializer.create_indexes()
        
        logger.info("ğŸ‰ PostgreSQLè§„åˆ™å’Œè¯å…¸åˆå§‹åŒ–å®Œæˆï¼")
        logger.info(f"ğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        logger.info(f"  - æå–è§„åˆ™: {rules_count} æ¡")
        logger.info(f"  - åŒä¹‰è¯: {synonyms_count} æ¡")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False
    finally:
        await initializer.disconnect()


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
