"""
åˆ†æOracleçœŸå®æ•°æ®è„šæœ¬
è¿æ¥Oracleæ•°æ®åº“ï¼ŒæŸ¥è¯¢å¹¶åˆ†æçœŸå®ç‰©æ–™æ•°æ®ï¼Œç”ŸæˆåŸºäºå®é™…æ•°æ®çš„ç±»åˆ«å…³é”®è¯å’Œæå–è§„åˆ™
"""

import logging
from collections import defaultdict, Counter
import re
import json
from datetime import datetime
from typing import Dict, List, Set, Tuple

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_oracle_data():
    """åˆ†æOracleçœŸå®æ•°æ®"""
    logger.info("ğŸ” å¼€å§‹åˆ†æOracleçœŸå®ç‰©æ–™æ•°æ®...")
    
    # å¯¼å…¥è¿æ¥å™¨
    from oracledb_connector import OracleDBConnector
    from oracle_config import OracleConfig, MaterialQueries
    
    # åˆ›å»ºè¿æ¥
    config_params = OracleConfig.get_connection_params()
    # åªä¼ é€’OracleDBConnectoréœ€è¦çš„å‚æ•°
    connector_params = {
        'host': config_params['host'],
        'port': config_params['port'],
        'service_name': config_params['service_name'],
        'username': config_params['username'],
        'password': config_params['password']
    }
    connector = OracleDBConnector(**connector_params)
    
    if not connector.connect():
        logger.error("âŒ Oracleæ•°æ®åº“è¿æ¥å¤±è´¥")
        return False
    
    try:
        # 1. æŸ¥è¯¢ç‰©æ–™åŸºæœ¬ä¿¡æ¯æ ·æœ¬
        logger.info("ğŸ“Š æŸ¥è¯¢ç‰©æ–™åŸºæœ¬ä¿¡æ¯æ ·æœ¬...")
        sample_query = """
        SELECT 
            m.code as erp_code,
            m.name as material_name,
            m.materialspec as specification,
            m.materialtype as material_type,
            m.materialmnecode as mnemonic_code,
            m.materialshortname as short_name,
            m.ename as english_name,
            c.name as category_name,
            c.code as category_code,
            u.name as unit_name,
            m.enablestate,
            m.materialmgt
        FROM DHNC65.bd_material m
        LEFT JOIN DHNC65.bd_marbasclass c ON m.pk_marbasclass = c.pk_marbasclass
        LEFT JOIN DHNC65.bd_measdoc u ON m.pk_measdoc = u.pk_measdoc
        -- è¯»å–æ‰€æœ‰ç‰©æ–™æ•°æ®ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
        ORDER BY m.code
        """
        
        # ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢è·å–æ‰€æœ‰ç‰©æ–™æ•°æ®
        materials_sample = connector.execute_query_batch(sample_query, batch_size=5000)
        logger.info(f"âœ… è·å–åˆ° {len(materials_sample)} æ¡ç‰©æ–™æ•°æ®ï¼ˆå…¨é‡æ•°æ®ï¼‰")
        
        # 2. æŸ¥è¯¢æ‰€æœ‰ç‰©æ–™åˆ†ç±»
        logger.info("ğŸ“‚ æŸ¥è¯¢ç‰©æ–™åˆ†ç±»ä¿¡æ¯...")
        categories = connector.execute_query(MaterialQueries.MATERIAL_CATEGORIES_QUERY)
        logger.info(f"âœ… è·å–åˆ° {len(categories)} ä¸ªç‰©æ–™åˆ†ç±»")
        
        # 3. æŸ¥è¯¢è®¡é‡å•ä½
        logger.info("ğŸ“ æŸ¥è¯¢è®¡é‡å•ä½ä¿¡æ¯...")
        units = connector.execute_query(MaterialQueries.UNIT_QUERY)
        logger.info(f"âœ… è·å–åˆ° {len(units)} ä¸ªè®¡é‡å•ä½")
        
        # 4. åˆ†ææ•°æ®å¹¶ç”Ÿæˆç»“æœ
        analysis_result = perform_data_analysis(materials_sample, categories, units)
        
        # 5. ä¿å­˜åˆ†æç»“æœ
        save_analysis_results(analysis_result)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False
    finally:
        connector.disconnect()

def perform_data_analysis(materials: List[Dict], categories: List[Dict], units: List[Dict]) -> Dict:
    """æ‰§è¡Œæ•°æ®åˆ†æ"""
    logger.info("ğŸ§  å¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æ...")
    
    analysis = {
        'material_patterns': analyze_material_patterns(materials),
        'category_keywords': generate_category_keywords(materials, categories),
        'extraction_rules': generate_extraction_rules_from_data(materials),
        'synonym_mappings': generate_synonym_mappings(materials, units),
        'statistics': generate_statistics(materials, categories, units)
    }
    
    return analysis

def analyze_material_patterns(materials: List[Dict]) -> Dict:
    """åˆ†æç‰©æ–™æè¿°æ¨¡å¼"""
    logger.info("ğŸ” åˆ†æç‰©æ–™æè¿°æ¨¡å¼...")
    
    patterns = {
        'size_patterns': [],
        'brand_patterns': [],
        'material_patterns': [],
        'model_patterns': [],
        'common_words': []
    }
    
    all_descriptions = []
    
    for material in materials:
        # ç»„åˆå®Œæ•´æè¿°ï¼Œå¤„ç†Noneå€¼
        name = material.get('MATERIAL_NAME') or ''
        spec = material.get('SPECIFICATION') or ''
        material_type = material.get('MATERIAL_TYPE') or ''
        
        full_desc = f"{name} {spec} {material_type}".strip()
        if full_desc:
            all_descriptions.append(full_desc)
        
        # æå–å°ºå¯¸æ¨¡å¼
        size_matches = re.findall(r'\b(?:M|Î¦|Ï†|DN)?\d+(?:\.\d+)?[Ã—*xX]\d+(?:\.\d+)?(?:[Ã—*xX]\d+(?:\.\d+)?)?\b', full_desc)
        patterns['size_patterns'].extend(size_matches)
        
        # æå–å¯èƒ½çš„å“ç‰Œæ¨¡å¼ï¼ˆå¤§å†™å­—æ¯å¼€å¤´çš„è¯ï¼‰
        brand_matches = re.findall(r'\b[A-Z][A-Z0-9]{2,}\b', full_desc)
        patterns['brand_patterns'].extend(brand_matches)
        
        # æå–æè´¨æ¨¡å¼
        material_matches = re.findall(r'\b(?:304|316L?|201|430|ä¸é”ˆé’¢|ç¢³é’¢|åˆé‡‘é’¢|é“¸é“|é“œ|é“)\b', full_desc)
        patterns['material_patterns'].extend(material_matches)
        
        # æå–å‹å·æ¨¡å¼
        model_matches = re.findall(r'\b[A-Z0-9]{3,}(?:-[A-Z0-9]+)?\b', full_desc)
        patterns['model_patterns'].extend(model_matches)
    
    # ç»Ÿè®¡å¸¸è§è¯æ±‡
    all_text = ' '.join(all_descriptions)
    words = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z0-9]+', all_text)
    word_counter = Counter(words)
    patterns['common_words'] = word_counter.most_common(50)
    
    # ç»Ÿè®¡å„ç±»æ¨¡å¼
    for key in ['size_patterns', 'brand_patterns', 'material_patterns', 'model_patterns']:
        patterns[key] = Counter(patterns[key]).most_common(20)
    
    logger.info(f"âœ… åˆ†æå®Œæˆï¼Œå‘ç° {len(patterns['common_words'])} ä¸ªå¸¸è§è¯æ±‡")
    return patterns

def generate_category_keywords(materials: List[Dict], categories: List[Dict]) -> Dict:
    """åŸºäºçœŸå®æ•°æ®ç”Ÿæˆç±»åˆ«å…³é”®è¯"""
    logger.info("ğŸ·ï¸ åŸºäºçœŸå®æ•°æ®ç”Ÿæˆç±»åˆ«å…³é”®è¯...")
    
    category_keywords = {}
    category_materials = defaultdict(list)
    
    # æŒ‰åˆ†ç±»ç»„ç»‡ç‰©æ–™
    for material in materials:
        category_name = material.get('CATEGORY_NAME', '')
        if category_name:
            name = material.get('MATERIAL_NAME', '') or ''
            spec = material.get('SPECIFICATION', '') or ''
            model = material.get('MODEL', '') or ''
            full_desc = f"{name} {spec} {model}".strip()
            category_materials[category_name].append(full_desc)
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»æå–å…³é”®è¯
    for category_name, descriptions in category_materials.items():
        if len(descriptions) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬æ‰åˆ†æ
            keywords = extract_category_keywords(category_name, descriptions)
            if keywords:
                category_keywords[category_name] = keywords
    
    logger.info(f"âœ… ä¸º {len(category_keywords)} ä¸ªåˆ†ç±»ç”Ÿæˆäº†å…³é”®è¯")
    return category_keywords

def extract_category_keywords(category_name: str, descriptions: List[str]) -> List[str]:
    """ä¸ºç‰¹å®šåˆ†ç±»æå–å…³é”®è¯"""
    keywords = set([category_name])  # åˆ†ç±»åæœ¬èº«
    
    # æå–æ‰€æœ‰æè¿°ä¸­çš„è¯æ±‡
    all_words = []
    for desc in descriptions:
        words = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z]+', desc)
        all_words.extend(words)
    
    # ç»Ÿè®¡è¯é¢‘ï¼Œé€‰æ‹©é«˜é¢‘è¯ä½œä¸ºå…³é”®è¯
    word_counter = Counter(all_words)
    
    # é€‰æ‹©å‡ºç°é¢‘ç‡ >= 20% çš„è¯ä½œä¸ºå…³é”®è¯
    threshold = max(1, len(descriptions) * 0.2)
    for word, count in word_counter.items():
        if count >= threshold and len(word) >= 2:
            keywords.add(word)
    
    # é™åˆ¶å…³é”®è¯æ•°é‡
    return list(keywords)[:10]

def generate_extraction_rules_from_data(materials: List[Dict]) -> List[Dict]:
    """åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæå–è§„åˆ™"""
    logger.info("ğŸ”§ åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæå–è§„åˆ™...")
    
    rules = []
    
    # åˆ†ææ‰€æœ‰æè¿°ï¼Œæ‰¾å‡ºå¸¸è§æ¨¡å¼
    all_descriptions = []
    for material in materials:
        name = material.get('MATERIAL_NAME') or ''
        spec = material.get('SPECIFICATION') or ''
        material_type = material.get('MATERIAL_TYPE') or ''
        full_desc = f"{name} {spec} {material_type}".strip()
        if full_desc:
            all_descriptions.append(full_desc)
    
    # 1. å°ºå¯¸è§„æ ¼è§„åˆ™ï¼ˆåŸºäºå®é™…æ•°æ®æ¨¡å¼ï¼‰
    size_patterns = set()
    for desc in all_descriptions:
        # æŸ¥æ‰¾å„ç§å°ºå¯¸è¡¨ç¤º
        patterns = re.findall(r'\b(?:M|Î¦|Ï†|DN)?\d+(?:\.\d+)?[Ã—*xX]\d+(?:\.\d+)?(?:[Ã—*xX]\d+(?:\.\d+)?)?\b', desc)
        size_patterns.update(patterns)
    
    if size_patterns:
        rules.append({
            'rule_name': 'çœŸå®æ•°æ®_å°ºå¯¸è§„æ ¼æå–',
            'material_category': 'general',
            'attribute_name': 'size_specification',
            'regex_pattern': r'\b(?:M|Î¦|Ï†|DN)?(\d+(?:\.\d+)?[Ã—*xX]\d+(?:\.\d+)?(?:[Ã—*xX]\d+(?:\.\d+)?)?)\b',
            'priority': 100,
            'description': f'åŸºäº{len(size_patterns)}ä¸ªçœŸå®å°ºå¯¸æ ·æœ¬ç”Ÿæˆ',
            'data_samples': list(size_patterns)[:5]
        })
    
    # 2. æè´¨è§„åˆ™ï¼ˆåŸºäºå®é™…å‡ºç°çš„æè´¨ï¼‰
    material_patterns = set()
    for desc in all_descriptions:
        patterns = re.findall(r'\b(304L?|316L?|201|430|ä¸é”ˆé’¢|ç¢³é’¢|åˆé‡‘é’¢|é“¸é“|é“¸é’¢|é“œ|é»„é“œ|é’é“œ|é“|é“åˆé‡‘|é’›|é’›åˆé‡‘)\b', desc)
        material_patterns.update(patterns)
    
    if material_patterns:
        material_regex = '|'.join(re.escape(m) for m in material_patterns)
        rules.append({
            'rule_name': 'çœŸå®æ•°æ®_æè´¨æå–',
            'material_category': 'general',
            'attribute_name': 'material_type',
            'regex_pattern': f'\\b({material_regex})\\b',
            'priority': 90,
            'description': f'åŸºäº{len(material_patterns)}ç§çœŸå®æè´¨æ ·æœ¬ç”Ÿæˆ',
            'data_samples': list(material_patterns)[:5]
        })
    
    # 3. å“ç‰Œè§„åˆ™ï¼ˆåŸºäºå®é™…å‡ºç°çš„å“ç‰Œï¼‰
    brand_patterns = set()
    for desc in all_descriptions:
        # æŸ¥æ‰¾å¯èƒ½çš„å“ç‰Œï¼ˆè¿ç»­å¤§å†™å­—æ¯ï¼‰
        patterns = re.findall(r'\b[A-Z]{2,}[A-Z0-9]*\b', desc)
        brand_patterns.update(patterns)
    
    # è¿‡æ»¤æ‰å¯èƒ½ä¸æ˜¯å“ç‰Œçš„è¯ï¼ˆå¦‚å•ä½ã€è§„æ ¼ç­‰ï¼‰
    exclude_words = {'DN', 'PN', 'MPa', 'BAR', 'MM', 'KG', 'PCS', 'SET'}
    brand_patterns = brand_patterns - exclude_words
    
    if brand_patterns and len(brand_patterns) >= 5:
        brand_regex = '|'.join(re.escape(b) for b in list(brand_patterns)[:20])  # é™åˆ¶æ•°é‡
        rules.append({
            'rule_name': 'çœŸå®æ•°æ®_å“ç‰Œæå–',
            'material_category': 'general',
            'attribute_name': 'brand_name',
            'regex_pattern': f'\\b({brand_regex})\\b',
            'priority': 85,
            'description': f'åŸºäº{len(brand_patterns)}ä¸ªçœŸå®å“ç‰Œæ ·æœ¬ç”Ÿæˆ',
            'data_samples': list(brand_patterns)[:5]
        })
    
    # 4. å‹å·è§„åˆ™ï¼ˆåŸºäºå®é™…å‹å·æ¨¡å¼ï¼‰
    model_patterns = set()
    for desc in all_descriptions:
        # æŸ¥æ‰¾å‹å·æ¨¡å¼ï¼ˆå­—æ¯æ•°å­—ç»„åˆï¼‰
        patterns = re.findall(r'\b[A-Z0-9]{3,}(?:-[A-Z0-9]+)*\b', desc)
        model_patterns.update(patterns)
    
    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å‹å·çš„è¯
    model_patterns = model_patterns - exclude_words - brand_patterns
    
    if model_patterns and len(model_patterns) >= 10:
        rules.append({
            'rule_name': 'çœŸå®æ•°æ®_å‹å·æå–',
            'material_category': 'general',
            'attribute_name': 'model_number',
            'regex_pattern': r'\b([A-Z0-9]{3,}(?:-[A-Z0-9]+)*)\b',
            'priority': 80,
            'description': f'åŸºäº{len(model_patterns)}ä¸ªçœŸå®å‹å·æ ·æœ¬ç”Ÿæˆ',
            'data_samples': list(model_patterns)[:5]
        })
    
    logger.info(f"âœ… åŸºäºçœŸå®æ•°æ®ç”Ÿæˆäº† {len(rules)} æ¡æå–è§„åˆ™")
    return rules

def generate_synonym_mappings(materials: List[Dict], units: List[Dict]) -> Dict:
    """åŸºäºçœŸå®æ•°æ®ç”ŸæˆåŒä¹‰è¯æ˜ å°„"""
    logger.info("ğŸ“š åŸºäºçœŸå®æ•°æ®ç”ŸæˆåŒä¹‰è¯æ˜ å°„...")
    
    synonyms = {}
    
    # 1. åŸºäºè®¡é‡å•ä½æ•°æ®ç”ŸæˆåŒä¹‰è¯
    for unit in units:
        unit_name = (unit.get('UNIT_NAME') or '').strip()
        unit_code = (unit.get('UNIT_CODE') or '').strip()
        english_name = (unit.get('ENGLISH_NAME') or '').strip()
        
        if unit_name:
            variants = []
            if unit_code and unit_code != unit_name:
                variants.append(unit_code)
            if english_name and english_name != unit_name:
                variants.append(english_name)
            
            if variants:
                synonyms[unit_name] = variants
    
    # 2. åŸºäºç‰©æ–™æè¿°ä¸­çš„å˜ä½“ç”ŸæˆåŒä¹‰è¯
    description_variants = defaultdict(set)
    
    for material in materials:
        name = material.get('MATERIAL_NAME') or ''
        spec = material.get('SPECIFICATION') or ''
        
        # æŸ¥æ‰¾å°ºå¯¸è¡¨ç¤ºçš„å˜ä½“
        size_matches = re.findall(r'\d+[Ã—*xX]\d+', f"{name} {spec}")
        for match in size_matches:
            # æ ‡å‡†åŒ–ä¸ºxæ ¼å¼
            normalized = re.sub(r'[Ã—*X]', 'x', match)
            description_variants[normalized].add(match)
    
    # è½¬æ¢ä¸ºåŒä¹‰è¯æ ¼å¼
    for standard, variants in description_variants.items():
        if len(variants) > 1:
            synonyms[standard] = list(variants - {standard})
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(synonyms)} ä¸ªåŒä¹‰è¯æ˜ å°„")
    return synonyms

def generate_statistics(materials: List[Dict], categories: List[Dict], units: List[Dict]) -> Dict:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    logger.info("ğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    
    stats = {
        'total_materials': len(materials),
        'total_categories': len(categories),
        'total_units': len(units),
        'category_distribution': {},
        'enable_state_distribution': {},
        'material_mgt_distribution': {}
    }
    
    # åˆ†ç±»åˆ†å¸ƒ
    category_counter = Counter()
    enable_state_counter = Counter()
    material_mgt_counter = Counter()
    
    for material in materials:
        category_name = material.get('CATEGORY_NAME', 'æœªåˆ†ç±»')
        enable_state = material.get('ENABLESTATE', 0)
        material_mgt = material.get('MATERIALMGT', 0)
        
        category_counter[category_name] += 1
        enable_state_counter[enable_state] += 1
        material_mgt_counter[material_mgt] += 1
    
    stats['category_distribution'] = dict(category_counter.most_common(10))
    stats['enable_state_distribution'] = dict(enable_state_counter)
    stats['material_mgt_distribution'] = dict(material_mgt_counter)
    
    return stats

def save_analysis_results(analysis: Dict):
    """ä¿å­˜åˆ†æç»“æœ"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜å®Œæ•´åˆ†æç»“æœ
    analysis_file = f"oracle_data_analysis_{timestamp}.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info(f"ğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜: {analysis_file}")
    
    # ä¿å­˜å¯è¯»æŠ¥å‘Š
    report_file = f"oracle_data_analysis_report_{timestamp}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# OracleçœŸå®æ•°æ®åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = analysis['statistics']
        f.write("## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n")
        f.write(f"- **ç‰©æ–™æ€»æ•°:** {stats['total_materials']:,}\n")
        f.write(f"- **åˆ†ç±»æ€»æ•°:** {stats['total_categories']:,}\n")
        f.write(f"- **å•ä½æ€»æ•°:** {stats['total_units']:,}\n\n")
        
        # åˆ†ç±»åˆ†å¸ƒ
        f.write("### ä¸»è¦åˆ†ç±»åˆ†å¸ƒ\n\n")
        for category, count in stats['category_distribution'].items():
            f.write(f"- {category}: {count:,} æ¡\n")
        f.write("\n")
        
        # ç”Ÿæˆçš„å…³é”®è¯
        f.write("## ğŸ·ï¸ ç”Ÿæˆçš„ç±»åˆ«å…³é”®è¯\n\n")
        for category, keywords in analysis['category_keywords'].items():
            f.write(f"### {category}\n")
            f.write(f"å…³é”®è¯: {', '.join(keywords)}\n\n")
        
        # ç”Ÿæˆçš„è§„åˆ™
        f.write("## ğŸ”§ ç”Ÿæˆçš„æå–è§„åˆ™\n\n")
        for rule in analysis['extraction_rules']:
            f.write(f"### {rule['rule_name']}\n")
            f.write(f"- **å±æ€§:** {rule['attribute_name']}\n")
            f.write(f"- **æ­£åˆ™:** `{rule['regex_pattern']}`\n")
            f.write(f"- **æè¿°:** {rule['description']}\n")
            if 'data_samples' in rule:
                f.write(f"- **æ ·æœ¬:** {', '.join(rule['data_samples'])}\n")
            f.write("\n")
        
        # ç‰©æ–™æ¨¡å¼
        f.write("## ğŸ” å‘ç°çš„ç‰©æ–™æ¨¡å¼\n\n")
        patterns = analysis['material_patterns']
        
        f.write("### å°ºå¯¸æ¨¡å¼ (å‰10ä¸ª)\n")
        for pattern, count in patterns['size_patterns'][:10]:
            f.write(f"- {pattern}: {count} æ¬¡\n")
        f.write("\n")
        
        f.write("### å“ç‰Œæ¨¡å¼ (å‰10ä¸ª)\n")
        for pattern, count in patterns['brand_patterns'][:10]:
            f.write(f"- {pattern}: {count} æ¬¡\n")
        f.write("\n")
        
        f.write("### æè´¨æ¨¡å¼ (å‰10ä¸ª)\n")
        for pattern, count in patterns['material_patterns'][:10]:
            f.write(f"- {pattern}: {count} æ¬¡\n")
        f.write("\n")
    
    logger.info(f"ğŸ“„ å¯è¯»åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return analysis_file, report_file

if __name__ == "__main__":
    logger.info("ğŸš€ å¼€å§‹OracleçœŸå®æ•°æ®åˆ†æ")
    success = analyze_oracle_data()
    
    if success:
        logger.info("ğŸ‰ Oracleæ•°æ®åˆ†æå®Œæˆï¼")
    else:
        logger.error("ğŸ’¥ Oracleæ•°æ®åˆ†æå¤±è´¥ï¼")
