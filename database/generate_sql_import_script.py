"""
åŸºäºç”Ÿæˆçš„æ ‡å‡†åŒ–æ–‡ä»¶åŠ¨æ€ç”ŸæˆPostgreSQLå¯¼å…¥SQLè„šæœ¬
"""

import json
import os
import logging
from datetime import datetime
from typing import Dict, List
import glob

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def cleanup_old_sql_files():
    """æ¸…ç†æ—§çš„SQLå¯¼å…¥æ–‡ä»¶"""
    logger.info("ğŸ§¹ æ¸…ç†æ—§çš„SQLå¯¼å…¥æ–‡ä»¶...")
    
    patterns = [
        'postgresql_import_*.sql',
        'postgresql_import_usage_*.md'
    ]
    
    cleaned_count = 0
    for pattern in patterns:
        files = glob.glob(pattern)
        if len(files) > 1:
            files.sort()
            old_files = files[:-1]  # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶
            
            for old_file in old_files:
                try:
                    os.remove(old_file)
                    logger.info(f"  âœ… åˆ é™¤æ—§æ–‡ä»¶: {old_file}")
                    cleaned_count += 1
                except Exception as e:
                    logger.warning(f"  âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {old_file}: {e}")
    
    if cleaned_count > 0:
        logger.info(f"ğŸ‰ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ—§SQLæ–‡ä»¶")
    else:
        logger.info("âœ¨ SQLæ–‡ä»¶ç›®å½•å·²ç»æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ¸…ç†")


def find_latest_files():
    """æŸ¥æ‰¾æœ€æ–°çš„æ ‡å‡†åŒ–æ–‡ä»¶"""
    files = {
        'rules': None,
        'synonyms': None,
        'categories': None
    }
    
    # æŸ¥æ‰¾æå–è§„åˆ™æ–‡ä»¶
    rules_files = [f for f in os.listdir('.') if f.startswith('standardized_extraction_rules_') and f.endswith('.json')]
    if rules_files:
        files['rules'] = sorted(rules_files)[-1]
    
    # æŸ¥æ‰¾åŒä¹‰è¯å…¸æ–‡ä»¶
    synonym_files = [f for f in os.listdir('.') if f.startswith('standardized_synonym_dictionary_') and f.endswith('.json')]
    if synonym_files:
        files['synonyms'] = sorted(synonym_files)[-1]
    
    # æŸ¥æ‰¾ç±»åˆ«å…³é”®è¯æ–‡ä»¶
    category_files = [f for f in os.listdir('.') if f.startswith('standardized_category_keywords_') and f.endswith('.json')]
    if category_files:
        files['categories'] = sorted(category_files)[-1]
    
    return files


def escape_sql_string(text: str) -> str:
    """è½¬ä¹‰SQLå­—ç¬¦ä¸²"""
    if text is None:
        return 'NULL'
    return "'" + text.replace("'", "''").replace("\\", "\\\\") + "'"


def generate_rules_sql(rules_file: str) -> List[str]:
    """ç”Ÿæˆæå–è§„åˆ™çš„SQLæ’å…¥è¯­å¥"""
    logger.info(f"ğŸ“Š å¤„ç†æå–è§„åˆ™æ–‡ä»¶: {rules_file}")
    
    with open(rules_file, 'r', encoding='utf-8') as f:
        rules = json.load(f)
    
    sql_statements = []
    
    for rule in rules:
        # æ„å»ºç¤ºä¾‹æ•°ç»„ï¼ˆå¦‚æœæœ‰example_inputå’Œexample_outputï¼Œåˆ™ä½¿ç”¨å®ƒä»¬ï¼‰
        examples = []
        if rule.get('example_input') and rule.get('example_output'):
            examples = [f"{rule['example_input']} -> {rule['example_output']}"]
        examples_array = "ARRAY[" + ", ".join([escape_sql_string(ex) for ex in examples]) + "]"
        
        # ç¬¦åˆDesign.md - ä¸æŒ‡å®šidï¼Œè®©æ•°æ®åº“SERIALè‡ªåŠ¨ç”Ÿæˆ
        sql = f"""INSERT INTO extraction_rules (rule_name, material_category, attribute_name, regex_pattern, priority, confidence, is_active, version, description, example_input, example_output, created_by) VALUES (
    {escape_sql_string(rule['rule_name'])},
    {escape_sql_string(rule['material_category'])},
    {escape_sql_string(rule['attribute_name'])},
    {escape_sql_string(rule['regex_pattern'])},
    {rule['priority']},
    {rule['confidence']},
    {str(rule.get('is_active', True)).upper()},
    {rule.get('version', 1)},
    {escape_sql_string(rule.get('description', ''))},
    {escape_sql_string(rule.get('example_input', ''))},
    {escape_sql_string(rule.get('example_output', ''))},
    {escape_sql_string(rule.get('created_by', 'system'))}
);"""
        sql_statements.append(sql)
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(sql_statements)} æ¡è§„åˆ™æ’å…¥è¯­å¥")
    return sql_statements


def generate_synonyms_sql(synonyms_file: str) -> List[str]:
    """ç”ŸæˆåŒä¹‰è¯çš„SQLæ’å…¥è¯­å¥"""
    logger.info(f"ğŸ“š å¤„ç†åŒä¹‰è¯å…¸æ–‡ä»¶: {synonyms_file}")
    
    with open(synonyms_file, 'r', encoding='utf-8') as f:
        synonym_dict = json.load(f)
    
    sql_statements = []
    
    for standard_term, variants in synonym_dict.items():
        # æ£€æµ‹åŒä¹‰è¯ç±»å‹å’Œç±»åˆ«
        synonym_type = detect_synonym_type(standard_term)
        category = detect_synonym_category(standard_term)
        
        for variant in variants:
            sql = f"""INSERT INTO synonyms (original_term, standard_term, category, synonym_type, created_by) VALUES (
    {escape_sql_string(variant)},
    {escape_sql_string(standard_term)},
    {escape_sql_string(category)},
    {escape_sql_string(synonym_type)},
    'system'
);"""
            sql_statements.append(sql)
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(sql_statements)} æ¡åŒä¹‰è¯æ’å…¥è¯­å¥")
    return sql_statements


def generate_categories_sql(categories_file: str) -> List[str]:
    """ç”Ÿæˆç±»åˆ«å…³é”®è¯çš„SQLæ’å…¥è¯­å¥"""
    logger.info(f"ğŸ·ï¸ å¤„ç†ç±»åˆ«å…³é”®è¯æ–‡ä»¶: {categories_file}")
    
    with open(categories_file, 'r', encoding='utf-8') as f:
        categories = json.load(f)
    
    sql_statements = []
    
    for category_name, keywords in categories.items():
        # å¦‚æœkeywordsæ˜¯å­—å…¸æ ¼å¼ï¼ˆæ—§æ ¼å¼ï¼‰ï¼Œåˆ™æå–keywordså­—æ®µ
        if isinstance(keywords, dict):
            keywords_list = keywords.get('keywords', [])
            detection_confidence = keywords.get('detection_confidence', 0.8)
            category_type = keywords.get('category_type', 'general')
            priority = keywords.get('priority', 50)
        else:
            # æ–°æ ¼å¼ï¼šç›´æ¥æ˜¯å…³é”®è¯åˆ—è¡¨
            keywords_list = keywords
            detection_confidence = 0.8
            category_type = 'general'
            priority = 50
            
        keywords_array = "ARRAY[" + ", ".join([escape_sql_string(kw) for kw in keywords_list]) + "]"
        
        sql = f"""INSERT INTO knowledge_categories (category_name, keywords, detection_confidence, category_type, priority, created_by) VALUES (
    {escape_sql_string(category_name)},
    {keywords_array},
    {detection_confidence},
    {escape_sql_string(category_type)},
    {priority},
    'system'
);"""
        sql_statements.append(sql)
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(sql_statements)} æ¡ç±»åˆ«æ’å…¥è¯­å¥")
    return sql_statements


def detect_synonym_type(term: str) -> str:
    """æ£€æµ‹åŒä¹‰è¯ç±»å‹"""
    term_lower = term.lower()
    
    # æè´¨ç±»å‹
    materials = ['304', '316', 'ä¸é”ˆé’¢', 'ç¢³é’¢', 'åˆé‡‘é’¢', 'é“¸é“', 'é“œ', 'é“']
    if any(material in term_lower for material in materials):
        return 'material'
    
    # å•ä½ç±»å‹
    units = ['mm', 'kg', 'mpa', 'bar', 'ä¸ª', 'åª', 'ä»¶', 'å¥—']
    if any(unit in term_lower for unit in units):
        return 'unit'
    
    # è§„æ ¼ç±»å‹
    if any(spec in term for spec in ['x', 'Ã—', '*', 'DN', 'PN', 'Î¦']):
        return 'specification'
    
    return 'general'


def detect_synonym_category(term: str) -> str:
    """æ£€æµ‹åŒä¹‰è¯æ‰€å±ç±»åˆ«"""
    term_lower = term.lower()
    
    category_mapping = {
        'material': ['304', '316', 'ä¸é”ˆé’¢', 'ç¢³é’¢', 'åˆé‡‘é’¢', 'é“¸é“', 'é“œ', 'é“'],
        'unit': ['mm', 'kg', 'mpa', 'bar', 'ä¸ª', 'åª', 'ä»¶', 'å¥—'],
        'specification': ['x', 'Ã—', '*', 'DN', 'PN', 'Î¦'],
        'fastener': ['èºæ “', 'èºé’‰', 'M'],
        'valve': ['é˜€', 'valve', 'PN'],
        'pipe': ['ç®¡', 'pipe', 'DN']
    }
    
    for category, keywords in category_mapping.items():
        if any(keyword in term_lower for keyword in keywords):
            return category
    
    return 'general'


def generate_complete_sql_script():
    """ç”Ÿæˆå®Œæ•´çš„SQLå¯¼å…¥è„šæœ¬"""
    logger.info("ğŸš€ å¼€å§‹ç”ŸæˆPostgreSQLå¯¼å…¥SQLè„šæœ¬")
    
    # æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶
    files = find_latest_files()
    
    if not all(files.values()):
        missing = [k for k, v in files.items() if v is None]
        logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing}")
        logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python generate_standardized_rules.py")
        return False
    
    logger.info("ğŸ“ ä½¿ç”¨æ–‡ä»¶:")
    for file_type, filename in files.items():
        logger.info(f"  - {file_type}: {filename}")
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"postgresql_import_{timestamp}.sql"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # å†™å…¥æ–‡ä»¶å¤´
        f.write(f"""-- PostgreSQLè§„åˆ™å’Œè¯å…¸å¯¼å…¥è„šæœ¬
-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- æ•°æ®æ¥æº: Oracle ERPç³»ç»Ÿ (230,421æ¡ç‰©æ–™æ•°æ®)
-- 
-- ä½¿ç”¨çš„æºæ–‡ä»¶:
--   è§„åˆ™æ–‡ä»¶: {files['rules']}
--   è¯å…¸æ–‡ä»¶: {files['synonyms']}
--   ç±»åˆ«æ–‡ä»¶: {files['categories']}

-- è®¾ç½®å®¢æˆ·ç«¯ç¼–ç 
SET client_encoding = 'UTF8';

-- è¿æ¥åˆ°matmatchæ•°æ®åº“
\\c matmatch;

-- æ³¨æ„: è¯·ä½¿ç”¨ psql --single-transaction å‚æ•°æ¥ç¡®ä¿åŸå­æ€§
-- å¦‚æœéœ€è¦æ‰‹åŠ¨äº‹åŠ¡ï¼Œè¯·å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
-- BEGIN;

-- åˆ›å»ºè¡¨ç»“æ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰- ç¬¦åˆDesign.mdå®šä¹‰
CREATE TABLE IF NOT EXISTS extraction_rules (
    id SERIAL PRIMARY KEY,
    rule_name VARCHAR(100) NOT NULL,
    material_category VARCHAR(100) NOT NULL,
    attribute_name VARCHAR(50) NOT NULL,
    regex_pattern TEXT NOT NULL,
    priority INTEGER DEFAULT 100,
    confidence DECIMAL(3,2) DEFAULT 1.0,
    is_active BOOLEAN DEFAULT TRUE,
    version INTEGER DEFAULT 1,
    description TEXT,
    example_input TEXT,
    example_output TEXT,
    created_by VARCHAR(50) DEFAULT 'system',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS synonyms (
    id SERIAL PRIMARY KEY,
    original_term VARCHAR(200) NOT NULL,
    standard_term VARCHAR(200) NOT NULL,
    category VARCHAR(50) DEFAULT 'general',
    synonym_type VARCHAR(20) DEFAULT 'general',
    is_active BOOLEAN DEFAULT TRUE,
    confidence DECIMAL(3,2) DEFAULT 1.0,
    description TEXT,
    created_by VARCHAR(50) DEFAULT 'system',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS knowledge_categories (
    id SERIAL PRIMARY KEY,
    category_name VARCHAR(200) NOT NULL,
    keywords TEXT[],
    detection_confidence DECIMAL(3,2) DEFAULT 0.8,
    category_type VARCHAR(50) DEFAULT 'general',
    priority INTEGER DEFAULT 50,
    is_active BOOLEAN DEFAULT TRUE,
    description TEXT,
    created_by VARCHAR(50) DEFAULT 'system',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ¸…ç©ºç°æœ‰æ•°æ®
DELETE FROM extraction_rules WHERE created_by = 'system';
DELETE FROM synonyms WHERE created_by = 'system';
DELETE FROM knowledge_categories WHERE created_by = 'system';

-- ========================================
-- å¯¼å…¥æå–è§„åˆ™
-- ========================================

""")
        
        # ç”Ÿæˆå¹¶å†™å…¥æå–è§„åˆ™
        rules_sql = generate_rules_sql(files['rules'])
        for sql in rules_sql:
            f.write(sql + "\n\n")
        
        f.write("""-- ========================================
-- å¯¼å…¥åŒä¹‰è¯å…¸
-- ========================================

""")
        
        # ç”Ÿæˆå¹¶å†™å…¥åŒä¹‰è¯
        synonyms_sql = generate_synonyms_sql(files['synonyms'])
        for sql in synonyms_sql:
            f.write(sql + "\n\n")
        
        f.write("""-- ========================================
-- å¯¼å…¥ç±»åˆ«å…³é”®è¯
-- ========================================

""")
        
        # ç”Ÿæˆå¹¶å†™å…¥ç±»åˆ«å…³é”®è¯
        categories_sql = generate_categories_sql(files['categories'])
        for sql in categories_sql:
            f.write(sql + "\n\n")
        
        # å†™å…¥ç´¢å¼•åˆ›å»ºå’ŒéªŒè¯
        f.write("""-- ========================================
-- åˆ›å»ºç´¢å¼•
-- ========================================

CREATE INDEX IF NOT EXISTS idx_extraction_rules_category ON extraction_rules (material_category, priority) WHERE is_active = TRUE;
CREATE INDEX IF NOT EXISTS idx_extraction_rules_name ON extraction_rules (rule_name) WHERE is_active = TRUE;
CREATE INDEX IF NOT EXISTS idx_synonyms_original ON synonyms (original_term) WHERE is_active = TRUE;
CREATE INDEX IF NOT EXISTS idx_synonyms_standard ON synonyms (standard_term) WHERE is_active = TRUE;
CREATE INDEX IF NOT EXISTS idx_synonyms_category_type ON synonyms (category, synonym_type) WHERE is_active = TRUE;
CREATE INDEX IF NOT EXISTS idx_knowledge_category_name ON knowledge_categories (category_name);
CREATE INDEX IF NOT EXISTS idx_knowledge_category_keywords_gin ON knowledge_categories USING gin (keywords);
CREATE INDEX IF NOT EXISTS idx_knowledge_category_active ON knowledge_categories (is_active) WHERE is_active = TRUE;

-- ========================================
-- éªŒè¯å¯¼å…¥ç»“æœ
-- ========================================

-- æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡
SELECT 'extraction_rules' as table_name, COUNT(*) as record_count FROM extraction_rules WHERE is_active = TRUE
UNION ALL
SELECT 'synonyms' as table_name, COUNT(*) as record_count FROM synonyms WHERE is_active = TRUE
UNION ALL
SELECT 'knowledge_categories' as table_name, COUNT(*) as record_count FROM knowledge_categories WHERE is_active = TRUE;

-- æ˜¾ç¤ºè§„åˆ™æ¦‚è§ˆ
SELECT id, rule_name, confidence, priority 
FROM extraction_rules 
WHERE is_active = TRUE 
ORDER BY priority DESC;

-- æ˜¾ç¤ºåŒä¹‰è¯ç±»åˆ«ç»Ÿè®¡
SELECT category, synonym_type, COUNT(*) as count 
FROM synonyms 
WHERE is_active = TRUE 
GROUP BY category, synonym_type 
ORDER BY count DESC;

-- æ˜¾ç¤ºç±»åˆ«å…³é”®è¯æ¦‚è§ˆ
SELECT category_name, category_type, priority, array_length(keywords, 1) as keyword_count
FROM knowledge_categories 
WHERE is_active = TRUE 
ORDER BY priority DESC 
LIMIT 10;

-- æäº¤äº‹åŠ¡ï¼ˆå¦‚æœä½¿ç”¨äº†BEGINï¼Œè¯·å–æ¶ˆæ³¨é‡Šï¼‰
-- COMMIT;

-- ========================================
-- ğŸ‰ å¯¼å…¥å®Œæˆï¼
-- ========================================
-- 
-- ğŸ“Š å¯¼å…¥ç»Ÿè®¡:
--   - åŸºäº230,421æ¡Oracleç‰©æ–™æ•°æ®ç”Ÿæˆ
--   - æå–è§„åˆ™: 6æ¡ (ç½®ä¿¡åº¦88%-98%)
--   - åŒä¹‰è¯: 37,223æ¡
--   - ç±»åˆ«å…³é”®è¯: 14ä¸ª
-- 
-- ğŸ§ª éªŒè¯æ•°æ®:
--   SELECT COUNT(*) FROM extraction_rules;
--   SELECT COUNT(*) FROM synonyms;
--   SELECT COUNT(*) FROM knowledge_categories;
-- 
-- æµ‹è¯•æŸ¥è¯¢ç¤ºä¾‹:
--   SELECT * FROM extraction_rules WHERE material_category = 'general';
--   SELECT * FROM synonyms LIMIT 10;
--   SELECT * FROM knowledge_categories;
""")
    
    logger.info(f"âœ… SQLå¯¼å…¥è„šæœ¬å·²ç”Ÿæˆ: {output_file}")
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    total_statements = len(rules_sql) + len(synonyms_sql) + len(categories_sql)
    
    logger.info("ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    logger.info(f"  - æå–è§„åˆ™æ’å…¥è¯­å¥: {len(rules_sql)} æ¡")
    logger.info(f"  - åŒä¹‰è¯æ’å…¥è¯­å¥: {len(synonyms_sql)} æ¡")
    logger.info(f"  - ç±»åˆ«æ’å…¥è¯­å¥: {len(categories_sql)} æ¡")
    logger.info(f"  - æ€»è®¡SQLè¯­å¥: {total_statements} æ¡")
    
    # ç”Ÿæˆä½¿ç”¨è¯´æ˜
    usage_file = f"postgresql_import_usage_{timestamp}.md"
    with open(usage_file, 'w', encoding='utf-8') as f:
        f.write(f"""# PostgreSQLå¯¼å…¥è„šæœ¬ä½¿ç”¨è¯´æ˜

## ç”Ÿæˆçš„æ–‡ä»¶
- **SQLè„šæœ¬**: `{output_file}`
- **ä½¿ç”¨è¯´æ˜**: `{usage_file}`

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ç›´æ¥æ‰§è¡ŒSQLè„šæœ¬
```bash
# ä½¿ç”¨psqlæ‰§è¡Œ
psql -h localhost -U matmatch -d matmatch -f {output_file}

# æˆ–è€…è¿æ¥åæ‰§è¡Œ
psql -h localhost -U matmatch -d matmatch
\\i {output_file}
```

### æ–¹æ³•2: åˆ†æ­¥æ‰§è¡Œ
```bash
# 1. è¿æ¥æ•°æ®åº“
psql -h localhost -U matmatch -d matmatch

# 2. æ‰§è¡Œè„šæœ¬
\\i {output_file}

# 3. éªŒè¯ç»“æœ
SELECT COUNT(*) FROM extraction_rules;
SELECT COUNT(*) FROM synonyms;
SELECT COUNT(*) FROM knowledge_categories;
```

## å¯¼å…¥å†…å®¹

### æå–è§„åˆ™ ({len(rules_sql)}æ¡)
åŸºäºçœŸå®Oracleæ•°æ®ç”Ÿæˆçš„é«˜è´¨é‡æå–è§„åˆ™ï¼ŒåŒ…æ‹¬ï¼š
- å…¬åˆ¶å°ºå¯¸è§„æ ¼æå– (ç½®ä¿¡åº¦95%)
- èºçº¹è§„æ ¼æå– (ç½®ä¿¡åº¦98%)
- æè´¨ç±»å‹æå– (ç½®ä¿¡åº¦92%)
- å“ç‰Œåç§°æå– (ç½®ä¿¡åº¦88%)
- å‹åŠ›ç­‰çº§æå– (ç½®ä¿¡åº¦90%)
- å…¬ç§°ç›´å¾„æå– (ç½®ä¿¡åº¦95%)

### åŒä¹‰è¯å…¸ ({len(synonyms_sql)}æ¡)
åŒ…å«ä»¥ä¸‹ç±»å‹çš„åŒä¹‰è¯ï¼š
- æè´¨åŒä¹‰è¯ (304â†”ä¸é”ˆé’¢ç­‰)
- å•ä½åŒä¹‰è¯ (mmâ†”æ¯«ç±³ç­‰)
- è§„æ ¼åŒä¹‰è¯ (Ã—â†”xç­‰)
- å“ç‰ŒåŒä¹‰è¯ (åŸºäºçœŸå®æ•°æ®)

### ç±»åˆ«å…³é”®è¯ ({len(categories_sql)}ä¸ª)
åŸºäº1,243ä¸ªçœŸå®ç‰©æ–™ç±»åˆ«ç”Ÿæˆçš„å…³é”®è¯ï¼Œæ”¯æŒï¼š
- æ™ºèƒ½ç±»åˆ«æ£€æµ‹
- å¤šå±‚çº§åˆ†ç±»
- ç½®ä¿¡åº¦è¯„åˆ†

## éªŒè¯å¯¼å…¥æˆåŠŸ

æ‰§è¡Œä»¥ä¸‹æŸ¥è¯¢éªŒè¯å¯¼å…¥ç»“æœï¼š
```sql
-- æ£€æŸ¥è®°å½•æ•°
SELECT COUNT(*) FROM extraction_rules;    -- åº”è¯¥æ˜¾ç¤º 6
SELECT COUNT(*) FROM synonyms;           -- åº”è¯¥æ˜¾ç¤º {len(synonyms_sql)}
SELECT COUNT(*) FROM knowledge_categories; -- åº”è¯¥æ˜¾ç¤º {len(categories_sql)}

-- æµ‹è¯•è§„åˆ™
SELECT rule_name, confidence FROM extraction_rules ORDER BY priority DESC;

-- æµ‹è¯•åŒä¹‰è¯
SELECT standard_term FROM synonyms WHERE original_term = '304';

-- æµ‹è¯•ç±»åˆ«
SELECT category_name FROM knowledge_categories WHERE 'èºæ “' = ANY(keywords);
```

## æ•…éšœæ’é™¤

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥PostgreSQLæœåŠ¡æ˜¯å¦è¿è¡Œ
2. ç¡®è®¤æ•°æ®åº“è¿æ¥å‚æ•°æ­£ç¡®
3. æ£€æŸ¥ç”¨æˆ·æƒé™æ˜¯å¦è¶³å¤Ÿ
4. æŸ¥çœ‹é”™è¯¯æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ•°æ®æ¥æº: Oracle ERPç³»ç»Ÿ (230,421æ¡ç‰©æ–™æ•°æ®)
""")
    
    logger.info(f"ğŸ“„ ä½¿ç”¨è¯´æ˜å·²ç”Ÿæˆ: {usage_file}")
    
    return output_file, usage_file


if __name__ == "__main__":
    # æ¸…ç†æ—§çš„SQLæ–‡ä»¶
    cleanup_old_sql_files()
    
    result = generate_complete_sql_script()
    if result:
        logger.info("ğŸ‰ SQLå¯¼å…¥è„šæœ¬ç”Ÿæˆå®Œæˆï¼")
        logger.info("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„SQLè„šæœ¬å¯¼å…¥åˆ°PostgreSQLæ•°æ®åº“")
    else:
        logger.error("ğŸ’¥ SQLå¯¼å…¥è„šæœ¬ç”Ÿæˆå¤±è´¥ï¼")
