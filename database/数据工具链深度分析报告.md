# 智能物料查重工具 - 数据工具链深度分析报告

**版本:** 1.3
**生成日期:** 2025年10月3日

## 1. 概述

本报告旨在深度解析 `database/` 目录下数据工具链的架构、执行流程与核心算法实现。该工具链是整个“智能物料查重工具”的基石，其核心使命是：**通过对超过23万条真实Oracle ERP物料数据进行自动化分析，构建一套高质量、数据驱动的“知识库”（包含提取规则、同义词典和类别关键词），为后续的物料查重功能提供准确、可靠的算法基础。**

工具链遵循清晰的分步执行流程，实现了从原始数据探索、模式发现、规则生成到最终测试验证的闭环，充分体现了“数据驱动”的设计思想。

### 1.1 整体执行流程

工具链的宏观执行流程如下，每个步骤都由一个独立的Python脚本负责：

```
Oracle数据库 (23万+真实物料)
    │
    └─> 1. [analyze_real_data.py] -> 进行深度数据分析与模式发现
        │
        ├─> oracle_data_analysis_*.json (机器可读的分析结果)
        └─> oracle_data_analysis_report_*.md (人工可读的分析报告)
            │
            └─> 2. [generate_standardized_rules.py] -> 生成标准化的规则与词典
                │
                ├─> standardized_extraction_rules_*.json (6条核心提取规则)
                ├─> standardized_synonym_dictionary_*.json (3484个同义词)
                └─> standardized_category_keywords_*.json (1243个类别关键词)
                    │
                    └─> 3. [generate_sql_import_script.py / import_to_postgresql.py] -> 导入知识库到数据库
                        │
                        └─> PostgreSQL 数据库 (extraction_rules, synonyms 等表)
                            │
                            └─> 4. [test_generated_rules.py] -> 对生成的规则和词典进行效果测试
                                │
                                └─> 控制台输出测试报告
```

---

## 2. 核心脚本功能详解

### 2.1 `analyze_real_data.py` - 数据分析与模式发现引擎

这是整个工具链的起点，扮演着“数据科学家”的角色。它负责连接真实的Oracle数据库，对海量数据进行探索性分析，并从中挖掘出有价值的模式和统计信息。

- **核心职责:**
  1.  连接Oracle数据库并批量拉取超过23万条物料数据。
  2.  对物料描述文本进行大规模的模式匹配和统计。
  3.  自动发现尺寸、材质、品牌、型号等关键属性的常见表示方式。
  4.  基于物料的分类信息，初步生成用于智能分类的关键词。
  5.  将所有分析结果输出为 `.json` 和 `.md` 两种格式的报告。

- **关键函数分析:**
  - **`analyze_oracle_data()`**: 主流程函数，负责协调数据库连接、数据查询、调用分析函数以及保存结果的完整过程。
  - **`perform_data_analysis()`**: 数据分析的顶层协调器，调用下属的各个具体分析函数。
  - **`analyze_material_patterns()`**: **(数据驱动的规则生成算法 - 发现阶段)**。此函数是模式发现的核心，它使用正则表达式 (`re.findall`) 遍历所有物料描述，从中捕获尺寸 (`M8*20`)、品牌 (大写字母组合)、材质 (`304`, `不锈钢`) 等模式，并使用 `collections.Counter` 进行频率统计，找出最高频的模式。
  - **`generate_category_keywords()`**: **(智能分类检测算法 - 学习阶段)**。此函数按物料的官方分类（`category_name`）对物料进行分组，然后对每个分组内的物料描述进行词频分析，提取出能代表该分类的核心关键词。
  - **`generate_extraction_rules_from_data()`**: 基于 `analyze_material_patterns` 发现的模式，将其形式化为初步的正则表达式规则。例如，将发现的大量尺寸模式，归纳为一条通用的尺寸提取正则。
  - **`save_analysis_results()`**: 将分析结果同时保存为两个文件：一个供后续脚本读取的 `json` 文件，和一个供开发人员阅读的 `md` 格式报告，设计非常人性化。

### 2.2 `generate_standardized_rules.py` - “知识库”生成与标准化器

该脚本是工具链的“大脑”，它承接 `analyze_real_data.py` 的分析结果，将其提炼、增强并最终生成可直接供应用程序使用的标准化“知识库”文件。

- **核心职责:**
  1.  读取分析结果 `json` 文件。
  2.  将初步的模式和规则，增强为包含优先级、置信度、全角半角兼容等属性的最终提取规则。
  3.  构建一个全面的、包含多种变体（全角/半角、大小写、常见别名）的同义词词典。
  4.  生成用于智能分类的类别关键词文件。
  5.  在每次生成新文件前，自动清理旧版本的文件，保持目录整洁。

- **关键函数分析:**
  - **`normalize_fullwidth_to_halfwidth()` / `normalize_text_standard()`**: **(标准化算法)**。这两个函数是文本标准化的核心。`normalize_fullwidth_to_halfwidth` 通过查询一个预定义的 `dict` (哈希表 `FULLWIDTH_TO_HALFWIDTH`)，高效地将全角字符替换为半角字符。`normalize_text_standard` 则封装了此功能，并增加了对多余空格的清理。
  - **`generate_extraction_rules()`**: **(结构化算法 - 规则定义)**。此函数将分析出的原始模式，细化为6条高置信度的正式规则。它为每条规则增加了`id`, `name`, `priority`, `confidence` 等元数据，并使正则表达式兼容全角和半角字符，例如 `[×*xX×＊ｘＸ]`，极大地增强了规则的鲁棒性。
  - **`generate_synonym_dictionary()`**: **(标准化算法 - 词典构建)**。这是构建同义词哈希表的核心。它不仅包含了从数据中分析出的同义词，还手动增强了大量内容，包括：
    - **全角/半角映射**：将 `FULLWIDTH_TO_HALFWIDTH` 的内容加入词典。
    - **材质同义词**：如 `不锈钢` -> `304`, `316L`, `SS`...
    - **品牌大小写变体**：如 `SKF` -> `skf`, `Skf`...
    - **单位同义词**：如 `mm` -> `毫米`, `MM`...
  - **`generate_category_keywords()`**: **(智能分类检测算法 - 知识库生成)**。此函数整合了自动发现的类别和一组预定义的标准类别（如轴承、螺栓等），为每个类别定义了关键词、优先级和置信度，形成了分类器的知识库。
  - **`cleanup_old_files()`**: 一个实用的辅助函数，通过 `glob` 查找并使用 `os.remove` 删除旧文件，确保了输出目录的整洁。

### 2.3 `intelligent_rule_generator.py` - 核心算法库与高级生成器

此脚本在 `README.md` 中被归类为“高级定制开发参考”，它在工具链中扮演着**“核心函数工具库”**和**“高级功能备用实现”**的双重角色。

- **核心职责:**
  1.  提供项目级的、可被主应用直接复用的文本标准化工具函数。
  2.  封装一个功能更全面、统一的规则生成器类，作为未来扩展的参考实现。

- **关键函数/类分析:**
  - **`normalize_text_comprehensive(text)`**: 一个通用的文本标准化函数，负责清理首尾和中间的多余空格。它被设计为可供整个项目复用的基础工具。
  - **`generate_case_variants(text)`**: 一个非常实用的工具函数，输入一个词（如 "SKF"），能自动生成其所有可能的大小写变体（如 `['SKF', 'skf', 'Skf']`）。这个函数在构建同义词典时至关重要，确保了匹配的全面性。
  - **`IntelligentRuleGenerator` (类)**: 这是一个面向对象的、功能更全的生成器。它将数据加载 (`load_all_data`)、规则生成 (`generate_extraction_rules`)、词典生成 (`generate_synonym_dictionary`) 等所有逻辑都封装在内部，展示了比主流程脚本更复杂的分析和生成能力，是未来进行定制化开发的重要参考。

### 2.4 `test_generated_rules.py` - 算法效果验证器

该脚本是工具链的“质检员”，它加载 `generate_standardized_rules.py` 生成的规则和词典，并用一组测试样本来验证这些“知识库”的实际效果。

- **核心职责:**
  1.  加载最新的规则和同义词典文件。
  2.  模拟应用程序的真实处理流程，对测试样本执行“标准化 -> 分类 -> 提取属性”的完整链路。
  3.  输出详细的测试报告，展示每一步的处理结果。

- **关键函数分析:**
  - **`RuleTester` (类)**: 封装了测试所需的全部环境（规则、词典）和方法。
  - **`apply_synonyms()`**: **(标准化算法 - 应用)**。该方法展示了如何**使用**同义词典（哈希表）来标准化输入的物料描述。
  - **`detect_category()`**: **(智能分类检测算法 - 应用)**。该方法展示了如何**使用**类别关键词库来对输入的描述进行分类。
  - **`extract_attributes()`**: **(结构化算法 - 应用)**。该方法展示了如何**使用**提取规则（正则表达式）来从标准化的文本中抽取出结构化属性。它清晰地体现了按优先级执行规则的逻辑。

---

## 3. 脚本协作与“对称处理”原则的实现

本章节旨在澄清 `database` 目录下各脚本之间的协作关系，并说明项目如何实现“数据标准化”和“结构化”处理的统一标准，即“对称处理”原则。

### 3.1 脚本间的关系：独立实现与功能互补

首先，`generate_standardized_rules.py` 和 `intelligent_rule_generator.py` 在工具链的执行流程中是**相互独立的**，前者并不依赖于后者。

它们更像两个功能互补的“工具箱”，为整个项目提供了丰富的文本处理函数库。二者的功能侧重点不同：

| 核心功能 | `generate_standardized_rules.py` | `intelligent_rule_generator.py` | 关系 |
| :--- | :--- | :--- | :--- |
| **全角/半角转换** | ✅ **实现** | ❌ 未实现 | 互补 |
| **大小写变体生成** | ❌ 未实现 | ✅ **实现** | 互补 |
| **同义词生成** | 基础实现 | **高级、自动化实现** | `intelligent`版更强 |

这种设计允许最终的应用程序根据需要，从两个“工具箱”中挑选最称手的工具。

### 3.2 “对称处理”的实现：构建统一的处理流水线

“对称处理”原则要求，无论是后台ETL还是在线API，都必须使用**完全相同**的标准和流程。项目通过在最终应用（即FastAPI后端的 `UniversalMaterialProcessor` 模块）中构建一个**唯一、标准的处理流水线**来实现这一点。

这个流水线会通过“组合复用”，按固定顺序调用来自不同脚本的函数，形成一套标准作业程序（SOP）：

**示例：统一处理流水线**

**输入**: 原始描述 `SKF轴承 ６２０１ＺＺ`

1.  **步骤一：全角转半角**
    *   **来源**: `generate_standardized_rules.py` -> `normalize_fullwidth_to_halfwidth()`
    *   **输出**: `SKF轴承 6201ZZ`

2.  **步骤二：大小写与空格处理**
    *   **来源**: `intelligent_rule_generator.py` -> `normalize_text_comprehensive()` / `generate_case_variants()`
    *   **输出**: `skf轴承 6201zz` (假设统一转小写)

3.  **步骤三：同义词替换**
    *   **来源**: 使用生成的同义词典（哈希表）
    *   **输出**: `skf bearing 6201zz` (假设“轴承”被标准化为“bearing”)

4.  **步骤四：结构化提取**
    *   **来源**: 使用生成的提取规则（正则表达式）
    *   **输出**: `{ brand: "skf", type: "bearing", model: "6201zz" }`

通过这种方式，尽管工具链中有多个功能相似的脚本，但最终应用只会构建**一条处理链路**。这条唯一的链路确保了所有数据源（历史数据和实时查询）都经过了完全一致的“标准化”和“结构化”处理，从而保证了“对称处理”原则的严格遵守。

### 3.3 设计风险与重构建议

#### 设计风险

在当前的工具链设计中，虽然 `design.md` 规划了最终应用应如何组合复用不同脚本的函数，但这依赖于开发者的手动、正确执行。这带来了一个潜在的工程风险：如果应用层未能完整、正确地按预定顺序组合所有标准化函数（例如，忘记了调用大小写变体生成），则会导致“对称处理”原则被破坏，使得线上查询和线下ETL的处理标准不一致。

#### 重构建议

为从根本上解决此风险，并强制保证标准的统一性，建议进行一次小规模重构，创建一个统一的 **核心处理模块** (`core_processing.py`)。

该模块将作为整个项目物料描述处理的**唯一入口**和**事实标准**。其实现思路如下：

1.  **集中依赖**: 在 `core_processing.py` 中，集中导入所有必要的函数，例如从 `generate_standardized_rules.py` 导入 `normalize_fullwidth_to_halfwidth`，从 `intelligent_rule_generator.py` 导入 `generate_case_variants`。
2.  **封装流水线**: 定义一个唯一的、公开的函数（例如 `process_description`），该函数内部以固定的顺序执行完整的标准化和结构化处理流水线。
3.  **明确接口**: 重构后，项目的所有其他部分（如ETL、API服务）都**只应**从 `core_processing.py` 导入并调用这一个函数。

通过此重构，“对称处理”原则将从一个“设计时”的约定，转变为一个“编码时”的强制约束，显著提升了系统的健壮性和可维护性。

---

## 4. 核心算法与代码实现映射

`README.md`中描述的核心算法均在以下文件和函数中得到了明确实现：

| 核心算法 | 原理 | 主要实现文件与函数 |
| :--- | :--- | :--- |
| **1. 标准化算法** | 基于哈希表的字符串替换 | **定义:** `generate_standardized_rules.py` (`normalize_*` 系列函数, `generate_synonym_dictionary`)<br>**应用:** `test_generated_rules.py` (`apply_synonyms`)<br>**核心工具:** `intelligent_rule_generator.py` (`generate_case_variants`) |
| **2. 结构化算法** | 正则表达式有限状态自动机 | **发现:** `analyze_real_data.py` (`analyze_material_patterns`)<br>**定义:** `generate_standardized_rules.py` (`generate_extraction_rules`)<br>**应用:** `test_generated_rules.py` (`extract_attributes`) |
| **3. 智能分类检测算法** | 基于关键词权重的分类器 | **学习:** `analyze_real_data.py` (`generate_category_keywords`)<br>**定义:** `generate_standardized_rules.py` (`generate_category_keywords`)<br>**应用:** `test_generated_rules.py` (`detect_category`) |
| **4. 数据驱动的规则生成** | 统计分析与模式发现 | **实现:** `analyze_real_data.py` (整个脚本的核心功能) |
| **5. 相似度与加权算法** | PostgreSQL `pg_trgm` 与 SQL加权 | **实现:** **数据库层面**。如 `design.md` 所示，此算法在SQL查询中实现，Python脚本的职责是为其准备高质量的输入数据。 |

---

## 5. 总结与评价

该数据工具链设计精良，职责清晰，体现了高度的工程化水平。

- **架构清晰**：通过将流程拆分为“分析”、“生成”、“导入”、“测试”四个独立的脚本，实现了关注点分离，每个脚本都有明确的输入和输出，易于理解和维护。
- **数据驱动**：所有核心规则和词典都源于对真实业务数据的深度分析，保证了“知识库”的实用性和高准确率。
- **代码复用性**：工具链中的核心函数（如 `normalize_fullwidth_to_halfwidth` 和 `generate_case_variants`）被设计为可复用的组件，这为后续主应用程序实现“对称处理”原则奠定了坚实的基础。
- **自动化与易用性**：提供了诸如 `cleanup_old_files` 等自动化辅助功能，并输出了详细的 `.md` 报告文档，极大地提升了开发和维护体验。

综上所述，`database/` 目录下的不仅是一个一次性的数据初始化工具，更是一套完整、严谨且可扩展的“知识工程”体系。它成功地将领域知识（物料描述的规范）从原始数据中提炼出来，并转化为机器可读、可用的结构化数据，为整个智能物料查重项目的成功奠定了坚实的数据和算法基础。