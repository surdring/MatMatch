"""
ç‰©æ–™çŸ¥è¯†åº“ç”Ÿæˆå™¨ v3.0 - ä»…æ”¯æŒPostgreSQLæ¸…æ´—åæ•°æ®

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. **å•ä¸€æ•°æ®æº**ï¼šä»…ä»PostgreSQLåŠ è½½å·²æ¸…æ´—çš„æ•°æ®
2. **å¯¹ç§°å¤„ç†ä¿è¯**ï¼šç”Ÿæˆçš„è§„åˆ™å¤©ç„¶åŒ¹é…13æ¡æ¸…æ´—è§„åˆ™çš„è¾“å‡º
3. **Oracleè§’è‰²æ˜ç¡®**ï¼šOracleä»…ç”¨äºETLåˆå§‹å¯¼å…¥ï¼Œä¸å‚ä¸çŸ¥è¯†åº“ç”Ÿæˆ

æ•°æ®æµæ¶æ„ï¼š
Oracle ERP â†’ ETLæ¸…æ´— â†’ PostgreSQL materials_master â†’ çŸ¥è¯†åº“ç”Ÿæˆå™¨ â†’ è§„åˆ™è¡¨

è‡ªåŠ¨ç”Ÿæˆï¼š
1. å±æ€§æå–è§„åˆ™ï¼ˆåŸºäºæ¸…æ´—åçš„specificationã€normalized_nameï¼‰
2. åŒä¹‰è¯è¯å…¸ï¼ˆåŸºäºæ¸…æ´—åçš„material_nameï¼‰
3. åˆ†ç±»å…³é”®è¯ï¼ˆåŸºäºæ¸…æ´—åçš„category_nameå’Œnormalized_nameï¼‰

è¿™ç¡®ä¿äº†åœ¨çº¿APIå’ŒETLä½¿ç”¨å®Œå…¨ä¸€è‡´çš„æ•°æ®å’Œè§„åˆ™
"""

import re
import json
import logging
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple, Optional
from datetime import datetime
import pandas as pd
import os
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(__file__))

def normalize_text_comprehensive(text: str) -> str:
    """
    ç»¼åˆæ–‡æœ¬æ ‡å‡†åŒ–å¤„ç†ï¼šå¤§å°å†™æ ‡å‡†åŒ– + ç©ºæ ¼æ¸…ç†
    
    è¿™æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæ ‡å‡†åŒ–å‡½æ•°ï¼ŒETLå’ŒAPIéƒ½å¿…é¡»ä½¿ç”¨æ­¤å‡½æ•°
    å®ç°"å¯¹ç§°å¤„ç†"åŸåˆ™
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        æ ‡å‡†åŒ–åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # 1. å»é™¤é¦–å°¾ç©ºæ ¼
    result = text.strip()
    
    # 2. å°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    result = re.sub(r'\s+', ' ', result)
    
    # 3. æ ‡å‡†åŒ–å¸¸è§çš„å¤§å°å†™é—®é¢˜ï¼ˆä¿æŒå“ç‰Œåç­‰çš„æ­£ç¡®å¤§å°å†™ï¼‰
    # è¿™é‡Œä¸åšå…¨å±€å¤§å°å†™è½¬æ¢ï¼Œå› ä¸ºå“ç‰Œåç­‰éœ€è¦ä¿æŒåŸæœ‰å¤§å°å†™
    
    return result

def normalize_fullwidth_to_halfwidth(text: str) -> str:
    """
    å…¨è§’è½¬åŠè§’æ ‡å‡†åŒ–å‡½æ•°
    
    è¿™æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæ ‡å‡†åŒ–å‡½æ•°ï¼ŒETLå’ŒAPIéƒ½å¿…é¡»ä½¿ç”¨æ­¤å‡½æ•°
    å®ç°"å¯¹ç§°å¤„ç†"åŸåˆ™
    """
    if not text:
        return text
    
    # å…¨è§’åˆ°åŠè§’çš„æ˜ å°„è¡¨
    FULLWIDTH_TO_HALFWIDTH = {
        'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
        'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9',
        'ï¼¡': 'A', 'ï¼¢': 'B', 'ï¼£': 'C', 'ï¼¤': 'D', 'ï¼¥': 'E',
        'ï¼¦': 'F', 'ï¼§': 'G', 'ï¼¨': 'H', 'ï¼©': 'I', 'ï¼ª': 'J',
        'ï¼«': 'K', 'ï¼¬': 'L', 'ï¼­': 'M', 'ï¼®': 'N', 'ï¼¯': 'O',
        'ï¼°': 'P', 'ï¼±': 'Q', 'ï¼²': 'R', 'ï¼³': 'S', 'ï¼´': 'T',
        'ï¼µ': 'U', 'ï¼¶': 'V', 'ï¼·': 'W', 'ï¼¸': 'X', 'ï¼¹': 'Y', 'ï¼º': 'Z',
        'ï½': 'a', 'ï½‚': 'b', 'ï½ƒ': 'c', 'ï½„': 'd', 'ï½…': 'e',
        'ï½†': 'f', 'ï½‡': 'g', 'ï½ˆ': 'h', 'ï½‰': 'i', 'ï½Š': 'j',
        'ï½‹': 'k', 'ï½Œ': 'l', 'ï½': 'm', 'ï½': 'n', 'ï½': 'o',
        'ï½': 'p', 'ï½‘': 'q', 'ï½’': 'r', 'ï½“': 's', 'ï½”': 't',
        'ï½•': 'u', 'ï½–': 'v', 'ï½—': 'w', 'ï½˜': 'x', 'ï½™': 'y', 'ï½š': 'z',
        'Ã—': 'x', 'ï¼Š': '*', 'ï¼': '-', 'ï¼‹': '+', 'ï¼': '=',
        'ï¼ˆ': '(', 'ï¼‰': ')', 'ï¼»': '[', 'ï¼½': ']', 'ï½›': '{', 'ï½': '}',
        'ï¼š': ':', 'ï¼›': ';', 'ï¼Œ': ',', 'ï¼': '.', 'ï¼Ÿ': '?', 'ï¼': '!',
        'ã€€': ' '  # å…¨è§’ç©ºæ ¼è½¬åŠè§’ç©ºæ ¼
    }
    
    result = text
    for fullwidth, halfwidth in FULLWIDTH_TO_HALFWIDTH.items():
        result = result.replace(fullwidth, halfwidth)
    
    return result

def generate_case_variants(text: str) -> List[str]:
    """
    ç”Ÿæˆæ–‡æœ¬çš„å¤§å°å†™å˜ä½“
    
    è¿™æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæ ‡å‡†åŒ–å‡½æ•°ï¼Œç”¨äºç”ŸæˆåŒä¹‰è¯è¯å…¸
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        å¤§å°å†™å˜ä½“åˆ—è¡¨
    """
    if not text:
        return []
    
    variants = [text]  # åŸæ–‡æœ¬
    
    # å…¨å°å†™
    lower_text = text.lower()
    if lower_text != text:
        variants.append(lower_text)
    
    # å…¨å¤§å†™
    upper_text = text.upper()
    if upper_text != text:
        variants.append(upper_text)
    
    # é¦–å­—æ¯å¤§å†™
    title_text = text.title()
    if title_text != text and title_text not in variants:
        variants.append(title_text)
    
    return variants

# é…ç½®æ—¥å¿—
import os
# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = 'logs'
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/material_knowledge_generation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class MaterialKnowledgeGenerator:
    """
    ç‰©æ–™çŸ¥è¯†åº“ç”Ÿæˆå™¨ (ç»Ÿä¸€ç‰ˆæœ¬)
    
    è¿™æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæ•°æ®å¤„ç†ç±»ï¼Œè´Ÿè´£ï¼š
    1. è¿æ¥Oracleæ•°æ®åº“
    2. åˆ†æç‰©æ–™æ•°æ®æ¨¡å¼
    3. ç”Ÿæˆæå–è§„åˆ™
    4. ç”ŸæˆåŒä¹‰è¯è¯å…¸
    5. ç”Ÿæˆåˆ†ç±»å…³é”®è¯
    
    å®ç°"å¯¹ç§°å¤„ç†"åŸåˆ™ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®å¤„ç†ä½¿ç”¨ç»Ÿä¸€æ ‡å‡†
    """
    
    def __init__(self, pg_session):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“ç”Ÿæˆå™¨ï¼ˆä»…æ”¯æŒPostgreSQLæ¸…æ´—åæ•°æ®ï¼‰
        
        Args:
            pg_session: PostgreSQLå¼‚æ­¥ä¼šè¯ï¼ˆç”¨äºåŠ è½½æ¸…æ´—åçš„æ•°æ®ï¼‰
            
        è®¾è®¡åŸåˆ™ï¼š
            çŸ¥è¯†åº“ç”Ÿæˆå¿…é¡»åŸºäºPostgreSQLä¸­å·²æ¸…æ´—çš„æ•°æ®ï¼Œç¡®ä¿ï¼š
            1. ç”Ÿæˆçš„è§„åˆ™å¤©ç„¶åŒ¹é…13æ¡æ¸…æ´—è§„åˆ™çš„è¾“å‡º
            2. å•ä¸€æ•°æ®æºï¼Œé¿å…ä¸ä¸€è‡´
            3. Oracleä»…ç”¨äºETLåˆå§‹å¯¼å…¥ï¼Œä¸å‚ä¸çŸ¥è¯†åº“ç”Ÿæˆ
        """
        if pg_session is None:
            raise ValueError("å¿…é¡»æä¾›PostgreSQLä¼šè¯ï¼ŒçŸ¥è¯†åº“ç”Ÿæˆå™¨åªæ”¯æŒæ¸…æ´—åçš„æ•°æ®")
        
        self.pg_session = pg_session
        self.materials_data = []
        self.categories_data = []
        self.units_data = []
        
        # åŠ¨æ€ç”Ÿæˆçš„ç‰©æ–™ç±»åˆ«å…³é”®è¯ï¼ˆåŸºäºæ¸…æ´—åæ•°æ®ï¼‰
        # è¿™å°†åœ¨ load_all_data() ä¸­é€šè¿‡åˆ†æçœŸå®ç‰©æ–™æ•°æ®ç”Ÿæˆ
        self.category_keywords = {}
    
    async def load_all_data(self):
        """åŠ è½½PostgreSQLæ¸…æ´—åçš„æ•°æ®"""
        logger.info("=" * 80)
        logger.info("ğŸš€ ç‰©æ–™çŸ¥è¯†åº“ç”Ÿæˆå™¨å¯åŠ¨ï¼ˆåŸºäºPostgreSQLæ¸…æ´—åæ•°æ®ï¼‰")
        logger.info("=" * 80)
        logger.info("ğŸ”„ å¼€å§‹ä»PostgreSQLåŠ è½½æ¸…æ´—åçš„æ•°æ®...")
        await self._load_from_postgresql()
    
    async def _load_from_postgresql(self):
        """ä»PostgreSQLåŠ è½½æ¸…æ´—åçš„æ•°æ®ï¼ˆæ¨èæ¨¡å¼ï¼‰"""
        from sqlalchemy import text
        
        try:
            # åŠ è½½ç‰©æ–™æ•°æ®ï¼ˆä½¿ç”¨æ¸…æ´—åçš„å­—æ®µï¼‰
            logger.info("ğŸ“Š ä»materials_masterè¡¨åŠ è½½æ¸…æ´—åçš„ç‰©æ–™æ•°æ®...")
            result = await self.pg_session.execute(text("""
                SELECT 
                    erp_code,
                    material_name,
                    specification,
                    normalized_name,        -- æ¸…æ´—åçš„åç§°
                    full_description,       -- æ¸…æ´—åçš„å®Œæ•´æè¿°
                    detected_category,
                    category_name,
                    unit_name
                FROM materials_master
                WHERE specification IS NOT NULL 
                  AND specification != ''
            """))
            
            rows = result.fetchall()
            self.materials_data = [
                {
                    'erp_code': row[0],
                    'material_name': row[1],
                    'specification': row[2],          # å·²æ¸…æ´—
                    'normalized_name': row[3],        # å·²æ¸…æ´—
                    'full_description': row[4],       # å·²æ¸…æ´—
                    'detected_category': row[5],
                    'category_name': row[6],
                    'unit_name': row[7]
                }
                for row in rows
            ]
            logger.info(f"âœ… å·²åŠ è½½ {len(self.materials_data):,} æ¡æ¸…æ´—åçš„ç‰©æ–™æ•°æ®")
            
            # åŠ è½½åˆ†ç±»æ•°æ®
            logger.info("ğŸ“‚ åŠ è½½åˆ†ç±»æ•°æ®...")
            result = await self.pg_session.execute(text("""
                SELECT DISTINCT category_name
                FROM materials_master
                WHERE category_name IS NOT NULL
            """))
            self.categories_data = [{'name': row[0]} for row in result.fetchall()]
            logger.info(f"âœ… å·²åŠ è½½ {len(self.categories_data):,} ä¸ªç‰©æ–™åˆ†ç±»")
            
            # åŠ è½½å•ä½æ•°æ®
            logger.info("ğŸ“ åŠ è½½è®¡é‡å•ä½æ•°æ®...")
            result = await self.pg_session.execute(text("""
                SELECT DISTINCT unit_name
                FROM materials_master
                WHERE unit_name IS NOT NULL
            """))
            self.units_data = [{'name': row[0]} for row in result.fetchall()]
            logger.info(f"âœ… å·²åŠ è½½ {len(self.units_data):,} ä¸ªè®¡é‡å•ä½")
            
            # ç”ŸæˆåŸºäºæ¸…æ´—åæ•°æ®çš„åˆ†ç±»å…³é”®è¯
            logger.info("ğŸ·ï¸ åŸºäºæ¸…æ´—åæ•°æ®ç”Ÿæˆåˆ†ç±»å…³é”®è¯...")
            self.category_keywords = self._generate_category_keywords_from_data()
            logger.info(f"âœ… å·²ç”Ÿæˆ {len(self.category_keywords):,} ä¸ªåˆ†ç±»çš„å…³é”®è¯")
            
        except Exception as e:
            logger.error(f"âŒ ä»PostgreSQLåŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
    
    
    def generate_extraction_rules(self) -> List[Dict]:
        """
        åŸºäºçœŸå®æ•°æ®ç”Ÿæˆå±æ€§æå–è§„åˆ™
        
        è¿™äº›è§„åˆ™å°†è¢«ETLå’ŒAPIå…±åŒä½¿ç”¨ï¼Œå®ç°"å¯¹ç§°å¤„ç†"
        """
        logger.info("ğŸ”§ å¼€å§‹åŸºäºçœŸå®æ•°æ®ç”Ÿæˆå±æ€§æå–è§„åˆ™...")
        
        rules = []
        
        if not self.materials_data:
            logger.warning("âš ï¸ æ²¡æœ‰ç‰©æ–™æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè§„åˆ™")
            return rules
        
        # åˆ†æç‰©æ–™æè¿°æ¨¡å¼
        patterns = self._analyze_description_patterns()
        
        # 1. åŸºäºæ•°æ®åˆ†æç”Ÿæˆé€šç”¨è§„åˆ™
        general_rules = self._generate_general_rules_from_data()
        rules.extend(general_rules)
        
        # 2. åŸºäºåˆ†ç±»æ•°æ®ç”Ÿæˆç±»åˆ«ç‰¹å®šè§„åˆ™
        category_rules = self._generate_category_specific_rules()
        rules.extend(category_rules)
        
        # 3. åŸºäºç‰©æ–™æ•°æ®ç”Ÿæˆå“ç‰Œæå–è§„åˆ™
        brand_rules = self._generate_brand_rules()
        rules.extend(brand_rules)
        
        # 4. åŸºäºç‰©æ–™æ•°æ®ç”Ÿæˆæè´¨æå–è§„åˆ™
        material_rules = self._generate_material_rules()
        rules.extend(material_rules)
        
        # æ³¨æ„ï¼šä¸å†ä¸ºè§„åˆ™æ·»åŠ IDå­—æ®µï¼Œæ•°æ®åº“å¯¼å…¥æ—¶ä¼šè‡ªåŠ¨ç”Ÿæˆè‡ªå¢ID
        # ç¬¦åˆDesign.mdä¸­ extraction_rules è¡¨çš„ id SERIAL PRIMARY KEY å®šä¹‰
        
        logger.info(f"âœ… åŸºäºçœŸå®æ•°æ®ç”Ÿæˆäº† {len(rules)} æ¡å±æ€§æå–è§„åˆ™")
        return rules
    
    def generate_synonym_dictionary(self) -> Dict[str, List[str]]:
        """
        åŸºäºçœŸå®æ•°æ®ç”ŸæˆåŒä¹‰è¯å…¸
        
        è¿™ä¸ªè¯å…¸å°†è¢«ETLå’ŒAPIå…±åŒä½¿ç”¨ï¼Œå®ç°"å¯¹ç§°å¤„ç†"
        è¿”å›æ ¼å¼ï¼š{æ ‡å‡†è¯: [åŒä¹‰è¯åˆ—è¡¨]}
        """
        logger.info("ğŸ“š å¼€å§‹åŸºäºçœŸå®æ•°æ®ç”ŸæˆåŒä¹‰è¯å…¸...")
        
        synonym_dict = {}
        
        if not self.materials_data:
            logger.warning("âš ï¸ æ²¡æœ‰ç‰©æ–™æ•°æ®ï¼Œæ— æ³•ç”ŸæˆåŒä¹‰è¯å…¸")
            return synonym_dict
        
        # 1. å…¨è§’åŠè§’æ˜ å°„
        fullwidth_synonyms = self._generate_fullwidth_synonyms()
        synonym_dict.update(fullwidth_synonyms)
        
        # 2. åŸºäºç‰©æ–™åç§°åˆ†æç”ŸæˆåŒä¹‰è¯
        name_variations = self._analyze_name_variations()
        synonym_dict.update(name_variations)
        
        # 3. åŸºäºå•ä½æ•°æ®ç”Ÿæˆå•ä½åŒä¹‰è¯
        unit_synonyms = self._generate_unit_synonyms()
        synonym_dict.update(unit_synonyms)
        
        # 4. åŸºäºè§„æ ¼æ•°æ®ç”Ÿæˆè§„æ ¼è¡¨ç¤ºåŒä¹‰è¯
        spec_synonyms = self._analyze_spec_variations()
        synonym_dict.update(spec_synonyms)
        
        # 5. åŸºäºå“ç‰Œæ•°æ®ç”Ÿæˆå“ç‰ŒåŒä¹‰è¯
        brand_synonyms = self._generate_brand_synonyms()
        synonym_dict.update(brand_synonyms)
        
        # 6. åŸºäºæè´¨æ•°æ®ç”Ÿæˆæè´¨åŒä¹‰è¯
        material_synonyms = self._generate_material_synonyms()
        synonym_dict.update(material_synonyms)
        
        logger.info(f"âœ… åŸºäºçœŸå®æ•°æ®ç”Ÿæˆäº† {len(synonym_dict)} ä¸ªåŒä¹‰è¯ç»„")
        return synonym_dict
    
    def generate_synonym_records(self) -> List[Dict]:
        """
        ç”Ÿæˆæ‰å¹³çš„åŒä¹‰è¯è®°å½•åˆ—è¡¨ï¼Œé€‚åˆç›´æ¥å¯¼å…¥æ•°æ®åº“
        
        å°†åµŒå¥—çš„åŒä¹‰è¯å­—å…¸è½¬æ¢ä¸ºæ‰å¹³çš„è®°å½•åˆ—è¡¨
        ç¬¦åˆDesign.mdä¸­ synonyms è¡¨çš„ç»“æ„å®šä¹‰
        
        Returns:
            List[Dict]: åŒä¹‰è¯è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å«ï¼š
                - original_term: åŸå§‹è¯æ±‡ï¼ˆåŒä¹‰è¯ï¼‰
                - standard_term: æ ‡å‡†è¯æ±‡
                - category: è¯æ±‡ç±»åˆ«
                - synonym_type: åŒä¹‰è¯ç±»å‹
                - confidence: æ˜ å°„ç½®ä¿¡åº¦
        """
        logger.info("ğŸ”„ å°†åŒä¹‰è¯å­—å…¸è½¬æ¢ä¸ºæ•°æ®åº“è®°å½•æ ¼å¼...")
        
        synonym_dict = self.generate_synonym_dictionary()
        records = []
        
        for standard_term, synonym_list in synonym_dict.items():
            for original_term in synonym_list:
                # åˆ¤æ–­åŒä¹‰è¯ç±»å‹
                synonym_type = 'general'
                if any(unit in standard_term.lower() for unit in ['mm', 'ç±³', 'å…‹', 'æ–¤', 'kg']):
                    synonym_type = 'unit'
                elif any(mat in standard_term for mat in ['304', '316', 'ä¸é”ˆé’¢', 'ç¢³é’¢', 'é“œ', 'é“']):
                    synonym_type = 'material'
                elif standard_term.isupper() and len(standard_term) >= 2:
                    synonym_type = 'brand'
                elif any(char in standard_term for char in ['Ã—', 'x', '*']):
                    synonym_type = 'specification'
                
                records.append({
                    'original_term': original_term,
                    'standard_term': standard_term,
                    'category': 'general',  # é»˜è®¤ç±»åˆ«
                    'synonym_type': synonym_type,
                    'confidence': 1.0
                })
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(records)} æ¡åŒä¹‰è¯è®°å½•")
        return records
    
    def _generate_fullwidth_synonyms(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆå…¨è§’åŠè§’åŒä¹‰è¯æ˜ å°„"""
        fullwidth_synonyms = {}
        
        # åŸºäºFULLWIDTH_TO_HALFWIDTHæ˜ å°„ç”ŸæˆåŒä¹‰è¯
        FULLWIDTH_TO_HALFWIDTH = {
            'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
            'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9',
            'ï¼¡': 'A', 'ï¼¢': 'B', 'ï¼£': 'C', 'ï¼¤': 'D', 'ï¼¥': 'E',
            'ï¼¦': 'F', 'ï¼§': 'G', 'ï¼¨': 'H', 'ï¼©': 'I', 'ï¼ª': 'J',
            'ï¼«': 'K', 'ï¼¬': 'L', 'ï¼­': 'M', 'ï¼®': 'N', 'ï¼¯': 'O',
            'ï¼°': 'P', 'ï¼±': 'Q', 'ï¼²': 'R', 'ï¼³': 'S', 'ï¼´': 'T',
            'ï¼µ': 'U', 'ï¼¶': 'V', 'ï¼·': 'W', 'ï¼¸': 'X', 'ï¼¹': 'Y', 'ï¼º': 'Z',
            'ï½': 'a', 'ï½‚': 'b', 'ï½ƒ': 'c', 'ï½„': 'd', 'ï½…': 'e',
            'ï½†': 'f', 'ï½‡': 'g', 'ï½ˆ': 'h', 'ï½‰': 'i', 'ï½Š': 'j',
            'ï½‹': 'k', 'ï½Œ': 'l', 'ï½': 'm', 'ï½': 'n', 'ï½': 'o',
            'ï½': 'p', 'ï½‘': 'q', 'ï½’': 'r', 'ï½“': 's', 'ï½”': 't',
            'ï½•': 'u', 'ï½–': 'v', 'ï½—': 'w', 'ï½˜': 'x', 'ï½™': 'y', 'ï½š': 'z',
            'Ã—': 'x', 'ï¼Š': '*', 'ï¼': '-', 'ï¼‹': '+', 'ï¼': '=',
        }
        
        for fullwidth, halfwidth in FULLWIDTH_TO_HALFWIDTH.items():
            fullwidth_synonyms[halfwidth] = [fullwidth]
        
        return fullwidth_synonyms
    
    def _analyze_description_patterns(self) -> Dict[str, List[str]]:
        """åˆ†æç‰©æ–™æè¿°ä¸­çš„å¸¸è§æ¨¡å¼"""
        patterns = defaultdict(list)
        
        for material in self.materials_data:
            name = material.get('MATERIAL_NAME', '')
            spec = material.get('SPECIFICATION', '')
            model = material.get('MODEL', '')
            
            # åº”ç”¨æ ‡å‡†åŒ–å¤„ç†
            name = normalize_fullwidth_to_halfwidth(normalize_text_comprehensive(name))
            spec = normalize_fullwidth_to_halfwidth(normalize_text_comprehensive(spec))
            model = normalize_fullwidth_to_halfwidth(normalize_text_comprehensive(model))
            
            # åˆ†æå®Œæ•´æè¿°
            full_desc = f"{name} {spec} {model}".strip()
            
            # æå–æ•°å­—+å­—æ¯çš„æ¨¡å¼ï¼ˆå¯èƒ½æ˜¯å‹å·ï¼‰
            model_patterns = re.findall(r'\b[A-Z0-9]{3,}(?:-[A-Z0-9]+)?\b', full_desc.upper())
            patterns['models'].extend(model_patterns)
            
            # æå–å°ºå¯¸æ¨¡å¼ï¼ˆæ”¯æŒå…¨è§’åŠè§’ï¼‰
            size_patterns = re.findall(r'\b\d+(?:\.\d+)?[Ã—*xXÃ—ï¼Šï½˜ï¼¸]\d+(?:\.\d+)?(?:[Ã—*xXÃ—ï¼Šï½˜ï¼¸]\d+(?:\.\d+)?)?\b', full_desc)
            patterns['sizes'].extend(size_patterns)
            
            # æå–æè´¨æ¨¡å¼
            material_patterns = re.findall(r'\b(304|316L?|201|430|ä¸é”ˆé’¢|ç¢³é’¢|åˆé‡‘é’¢|é“¸é“|é“œ|é“)\b', full_desc)
            patterns['materials'].extend(material_patterns)
        
        return dict(patterns)
    
    def _analyze_name_variations(self) -> Dict[str, List[str]]:
        """åˆ†æç‰©æ–™åç§°å˜ä½“ï¼ˆåŒ…å«å¤§å°å†™å’Œç©ºæ ¼æ ‡å‡†åŒ–ï¼‰"""
        variations = {}
        
        # æŒ‰ç›¸ä¼¼åç§°åˆ†ç»„
        name_groups = defaultdict(list)
        
        for material in self.materials_data:
            name = normalize_text_comprehensive(material.get('MATERIAL_NAME', ''))
            short_name = normalize_text_comprehensive(material.get('SHORT_NAME', ''))
            english_name = normalize_text_comprehensive(material.get('ENGLISH_NAME', ''))
            
            if name:
                # æå–æ ¸å¿ƒå…³é”®è¯
                core_keywords = self._extract_core_keywords(name)
                for keyword in core_keywords:
                    # æ·»åŠ åŸå§‹åç§°åŠå…¶å¤§å°å†™å˜ä½“
                    name_variants = generate_case_variants(name)
                    name_groups[keyword].extend(name_variants)
                    
                    if short_name and short_name != name:
                        short_variants = generate_case_variants(short_name)
                        name_groups[keyword].extend(short_variants)
                    
                    if english_name:
                        english_variants = generate_case_variants(english_name)
                        name_groups[keyword].extend(english_variants)
        
        # ç”ŸæˆåŒä¹‰è¯ç»„
        for keyword, names in name_groups.items():
            if len(names) > 1:
                unique_names = list(set(names))
                if len(unique_names) > 1:
                    # é€‰æ‹©æœ€å¸¸è§çš„å½¢å¼ä½œä¸ºæ ‡å‡†å½¢å¼
                    standard_name = max(unique_names, key=len)  # é€‰æ‹©æœ€é•¿çš„ä½œä¸ºæ ‡å‡†
                    variants = [n for n in unique_names if n != standard_name]
                    if variants:
                        variations[standard_name] = variants
        
        return variations
    
    def _generate_unit_synonyms(self) -> Dict[str, List[str]]:
        """åŸºäºbd_measdocç”Ÿæˆå•ä½åŒä¹‰è¯ï¼ˆåŒ…å«å¤§å°å†™å˜ä½“ï¼‰"""
        unit_synonyms = {}
        
        for unit in self.units_data:
            unit_name = normalize_text_comprehensive(unit.get('UNIT_NAME', ''))
            english_name = normalize_text_comprehensive(unit.get('ENGLISH_NAME', ''))
            unit_code = normalize_text_comprehensive(unit.get('UNIT_CODE', ''))
            
            if unit_name:
                synonyms = []
                
                # æ·»åŠ å•ä½åç§°çš„å¤§å°å†™å˜ä½“
                synonyms.extend(generate_case_variants(unit_name))
                
                if english_name and english_name != unit_name:
                    synonyms.extend(generate_case_variants(english_name))
                
                if unit_code and unit_code != unit_name:
                    synonyms.extend(generate_case_variants(unit_code))
                
                # å»é‡å¹¶è¿‡æ»¤æ‰æ ‡å‡†åç§°æœ¬èº«
                unique_synonyms = list(set(synonyms))
                variants = [s for s in unique_synonyms if s != unit_name]
                
                if variants:
                    unit_synonyms[unit_name] = variants
        
        return unit_synonyms
    
    def _analyze_spec_variations(self) -> Dict[str, List[str]]:
        """åˆ†æè§„æ ¼è¡¨ç¤ºçš„å˜ä½“ï¼ˆåŒ…å«æ ‡å‡†åŒ–å¤„ç†ï¼‰"""
        variations = {}
        
        # åˆ†æè§„æ ¼è¡¨ç¤ºçš„å˜ä½“
        spec_variations = defaultdict(set)
        
        for material in self.materials_data:
            spec = material.get('SPECIFICATION', '')
            if spec:
                # åº”ç”¨æ ‡å‡†åŒ–å¤„ç†
                spec = normalize_fullwidth_to_halfwidth(normalize_text_comprehensive(spec))
                
                # æŸ¥æ‰¾å°ºå¯¸è§„æ ¼çš„ä¸åŒè¡¨ç¤ºæ–¹æ³•
                size_matches = re.findall(r'\d+(?:\.\d+)?[Ã—*xX]\d+(?:\.\d+)?', spec)
                for match in size_matches:
                    # æ ‡å‡†åŒ–ä¸ºxæ ¼å¼
                    normalized = re.sub(r'[Ã—*X]', 'x', match)
                    spec_variations[normalized].add(match)
        
        # ç”Ÿæˆè§„æ ¼å˜ä½“åŒä¹‰è¯
        for normalized, variants in spec_variations.items():
            if len(variants) > 1:
                variations[normalized] = list(variants - {normalized})
        
        return variations
    
    def _generate_brand_synonyms(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆå“ç‰ŒåŒä¹‰è¯ï¼ˆåŒ…å«å¤§å°å†™å˜ä½“ï¼‰"""
        brand_synonyms = {}
        
        # ä»ç‰©æ–™æ•°æ®ä¸­æå–å“ç‰Œä¿¡æ¯
        brand_patterns = defaultdict(int)
        
        for material in self.materials_data:
            name = normalize_text_comprehensive(material.get('MATERIAL_NAME', ''))
            # æå–å¯èƒ½çš„å“ç‰Œåï¼ˆé€šå¸¸æ˜¯å¤§å†™å­—æ¯ç»„åˆï¼‰
            brands = re.findall(r'\b[A-Z]{2,}\b', name)
            for brand in brands:
                brand_patterns[brand] += 1
        
        # ä¸ºå¸¸è§å“ç‰Œç”Ÿæˆå¤§å°å†™å˜ä½“
        common_brands = [brand for brand, count in brand_patterns.items() if count >= 5]
        
        for brand in common_brands:
            variants = generate_case_variants(brand)
            if len(variants) > 1:
                brand_synonyms[brand] = [v for v in variants if v != brand]
        
        return brand_synonyms
    
    def _generate_material_synonyms(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆæè´¨åŒä¹‰è¯ï¼ˆåŒ…å«å¤§å°å†™å˜ä½“ï¼‰"""
        material_synonyms = {}
        
        # å¸¸è§æè´¨åŠå…¶å˜ä½“
        common_materials = {
            'ä¸é”ˆé’¢': ['304', '316', '316L', 'SS', 'stainless steel'],
            'ç¢³é’¢': ['CS', 'carbon steel', 'A105'],
            'åˆé‡‘é’¢': ['alloy steel', '40Cr', '42CrMo'],
            'é“¸é“': ['cast iron', 'CI', 'HT200', 'HT250'],
            'é“œ': ['Cu', 'copper', 'brass'],
            'é“': ['Al', 'aluminum', 'aluminium']
        }
        
        for standard, variants in common_materials.items():
            all_variants = []
            for variant in variants:
                all_variants.extend(generate_case_variants(variant))
            
            # å»é‡
            unique_variants = list(set(all_variants))
            if unique_variants:
                material_synonyms[standard] = unique_variants
        
        return material_synonyms
    
    def _generate_general_rules_from_data(self) -> List[Dict]:
        """åŸºäºæ•°æ®åˆ†æç”Ÿæˆé€šç”¨è§„åˆ™"""
        rules = []
        
        # åŸºäºç‰©æ–™æè¿°æ¨¡å¼ç”Ÿæˆè§„åˆ™
        patterns = self._analyze_description_patterns()
        
        # ç”Ÿæˆå°ºå¯¸è§„æ ¼æå–è§„åˆ™ï¼ˆæ¸…æ´—è§„åˆ™é€‚é…ç‰ˆï¼‰
        # å…³é”®æ”¹è¿›ï¼šé€‚é…13æ¡æ¸…æ´—è§„åˆ™çš„è¾“å‡ºæ ¼å¼
        if patterns.get('sizes'):
            rules.append({
                'rule_name': 'å°ºå¯¸è§„æ ¼æå–ï¼ˆæ¸…æ´—é€‚é…ï¼‰',
                'material_category': 'general',
                'attribute_name': 'size_specification',
                'regex_pattern': r'(\d+(?:\.\d+)?[_Ã—*x]\d+(?:\.\d+)?(?:[_Ã—*x]\d+(?:\.\d+)?)?)',  # å…³é”®ï¼šå¢åŠ _æ”¯æŒ
                'priority': 90,
                'confidence': 0.95,
                'is_active': True,
                'version': 2,  # ç‰ˆæœ¬å‡çº§
                'description': 'æå–å°ºå¯¸è§„æ ¼ï¼Œé€‚é…æ¸…æ´—è§„åˆ™è¾“å‡ºï¼ˆæ”¯æŒä¸‹åˆ’çº¿åˆ†éš”ç¬¦ï¼‰',
                'example_input': 'ä¸é”ˆé’¢ç®¡ ï¼•ï¼Ã—ï¼‘ï¼ï¼Ã—ï¼’',
                'example_input_cleaned': 'ä¸é”ˆé’¢ç®¡ 50_100_2',  # æ–°å¢ï¼šæ¸…æ´—åæ ¼å¼
                'example_output': '50Ã—100Ã—2',
                'created_by': 'system'
            })
        
        return rules
    
    def _generate_category_specific_rules(self) -> List[Dict]:
        """ç”Ÿæˆç±»åˆ«ç‰¹å®šè§„åˆ™"""
        rules = []
        
        # èºçº¹è§„æ ¼æå–è§„åˆ™ï¼ˆæ¸…æ´—è§„åˆ™é€‚é…ç‰ˆï¼‰
        # å…³é”®æ”¹è¿›ï¼šé€‚é…13æ¡æ¸…æ´—è§„åˆ™çš„è¾“å‡ºæ ¼å¼
        # - æ”¯æŒä¸‹åˆ’çº¿åˆ†éš”ç¬¦ï¼ˆæ¸…æ´—è§„åˆ™å°†Ã—*è½¬ä¸º_ï¼‰
        # - æ”¯æŒå°å†™åŒ¹é…ï¼ˆæ¸…æ´—è§„åˆ™ç»Ÿä¸€è½¬å°å†™ï¼‰
        # - åŒæ—¶ä¿ç•™å¯¹åŸå§‹æ ¼å¼çš„æ”¯æŒ
        rules.append({
            'rule_name': 'èºçº¹è§„æ ¼æå–ï¼ˆæ¸…æ´—é€‚é…ï¼‰',
            'material_category': 'fastener',
            'attribute_name': 'thread_specification',
            'regex_pattern': r'([Mm]\d+(?:\.\d+)?[_Ã—*xX]\d+(?:\.\d+)?)',  # å…³é”®ï¼šå¢åŠ _å’Œå°å†™æ”¯æŒ
            'priority': 95,
            'confidence': 0.98,
            'is_active': True,
            'version': 2,  # ç‰ˆæœ¬å‡çº§
            'description': 'æå–èºçº¹è§„æ ¼ï¼Œé€‚é…æ¸…æ´—è§„åˆ™è¾“å‡ºï¼ˆæ”¯æŒä¸‹åˆ’çº¿ã€å°å†™ï¼‰',
            'example_input': 'å†…å…­è§’èºæ “ ï¼­ï¼˜Ã—ï¼‘ï¼ï¼’ï¼•Ã—ï¼’ï¼',
            'example_input_cleaned': 'å†…å…­è§’èºæ “ m8_1.25_20',  # æ–°å¢ï¼šæ¸…æ´—åæ ¼å¼
            'example_output': 'M8Ã—1.25',
            'created_by': 'system'
        })
        
        # å‹åŠ›ç­‰çº§æå–è§„åˆ™
        rules.append({
            'rule_name': 'å‹åŠ›ç­‰çº§æå–',
            'material_category': 'valve',
            'attribute_name': 'pressure_rating',
            'regex_pattern': r'(PN\d+|(?:\d+(?:\.\d+)?(?:MPa|bar|å…¬æ–¤|kg)))',
            'priority': 88,
            'confidence': 0.90,
            'is_active': True,
            'version': 1,
            'description': 'æå–å‹åŠ›ç­‰çº§å¦‚PN16, 1.6MPa',
            'example_input': 'çƒé˜€ PN16 DN50',
            'example_output': 'PN16',
            'created_by': 'system'
        })
        
        # å…¬ç§°ç›´å¾„æå–è§„åˆ™ï¼ˆæ¸…æ´—è§„åˆ™é€‚é…ç‰ˆï¼‰
        # å…³é”®æ”¹è¿›ï¼šé€‚é…å¸Œè…Šå­—æ¯æ ‡å‡†åŒ–ï¼ˆÏ†/Î¦ â†’ phi/PHIï¼‰
        rules.append({
            'rule_name': 'å…¬ç§°ç›´å¾„æå–ï¼ˆæ¸…æ´—é€‚é…ï¼‰',
            'material_category': 'pipe',
            'attribute_name': 'nominal_diameter',
            'regex_pattern': r'([Dd][Nn]\d+|[Pp][Hh][Ii]\d+|Ï†\d+|Î¦\d+)',  # å…³é”®ï¼šå¢åŠ phi/PHIæ”¯æŒ
            'priority': 87,
            'confidence': 0.95,
            'is_active': True,
            'version': 2,  # ç‰ˆæœ¬å‡çº§
            'description': 'æå–å…¬ç§°ç›´å¾„ï¼Œé€‚é…æ¸…æ´—è§„åˆ™è¾“å‡ºï¼ˆæ”¯æŒphi/PHIæ ‡å‡†åŒ–ï¼‰',
            'example_input': 'ä¸é”ˆé’¢ç®¡ ï¼¤ï¼®ï¼•ï¼ Ï†100',
            'example_input_cleaned': 'ä¸é”ˆé’¢ç®¡ dn50 phi100',  # æ–°å¢ï¼šæ¸…æ´—åæ ¼å¼
            'example_output': 'DN50',
            'created_by': 'system'
        })
        
        return rules
    
    def _generate_brand_rules(self) -> List[Dict]:
        """ç”Ÿæˆå“ç‰Œæå–è§„åˆ™"""
        rules = []
        
        # ä»æ•°æ®ä¸­æå–å¸¸è§å“ç‰Œ
        brand_patterns = defaultdict(int)
        for material in self.materials_data:
            name = material.get('MATERIAL_NAME', '')
            brands = re.findall(r'\b[A-Z]{2,}\b', name)
            for brand in brands:
                brand_patterns[brand] += 1
        
        common_brands = [brand for brand, count in brand_patterns.items() if count >= 10]
        
        if common_brands:
            brand_regex = '|'.join(re.escape(brand) for brand in common_brands)
            rules.append({
                'rule_name': 'å“ç‰Œåç§°æå–',
                'material_category': 'general',
                'attribute_name': 'brand_name',
                'regex_pattern': f'\\b({brand_regex})\\b',
                'priority': 85,
                'confidence': 0.92,
                'is_active': True,
                'version': 1,
                'description': f'æå–å“ç‰Œåç§°ï¼Œæ”¯æŒ{len(common_brands)}ä¸ªå¸¸è§å“ç‰Œ',
                'example_input': 'SKFæ·±æ²Ÿçƒè½´æ‰¿ 6206',
                'example_output': 'SKF',
                'created_by': 'system'
            })
        
        return rules
    
    def _generate_material_rules(self) -> List[Dict]:
        """ç”Ÿæˆæè´¨æå–è§„åˆ™"""
        rules = []
        
        # æè´¨æå–è§„åˆ™
        rules.append({
            'rule_name': 'æè´¨ç±»å‹æå–',
            'material_category': 'general',
            'attribute_name': 'material_type',
            'regex_pattern': r'(304|316L?|201|430|ä¸é”ˆé’¢|ç¢³é’¢|åˆé‡‘é’¢|é“¸é“|é“œ|é“)',
            'priority': 88,
            'confidence': 0.90,
            'is_active': True,
            'version': 1,
            'description': 'æå–æè´¨ç±»å‹',
            'example_input': '304ä¸é”ˆé’¢ç®¡',
            'example_output': '304',
            'created_by': 'system'
        })
        
        return rules
    
    def _extract_core_keywords(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„æ ¸å¿ƒå…³é”®è¯"""
        # å»é™¤å¸¸è§çš„ä¿®é¥°è¯
        stop_words = {'çš„', 'ç”¨', 'å‹', 'å¼', 'ç§', 'ä¸ª', 'åª', 'ä»¶', 'å¥—', 'æ ¹', 'æ¡', 'ç‰‡'}
        
        # åˆ†è¯ï¼ˆç®€å•çš„åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹çš„åˆ†è¯ï¼‰
        words = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z0-9]+', text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = [word for word in words if len(word) >= 2 and word not in stop_words]
        
        return keywords
    
    def _generate_category_keywords_from_data(self) -> Dict[str, List[str]]:
        """
        åŸºäºOracleçœŸå®ç‰©æ–™æ•°æ®ç”Ÿæˆåˆ†ç±»å…³é”®è¯
        
        è¿™ä¸ªæ–¹æ³•åˆ†ææ¯ä¸ªåˆ†ç±»ä¸‹çš„æ‰€æœ‰ç‰©æ–™æè¿°ï¼Œæå–é«˜é¢‘è¯ä½œä¸ºè¯¥åˆ†ç±»çš„æ£€æµ‹å…³é”®è¯
        
        Returns:
            Dict[str, List[str]]: {åˆ†ç±»å: [å…³é”®è¯åˆ—è¡¨]}
        """
        logger.info("ğŸ” åˆ†æç‰©æ–™æ•°æ®ï¼Œç”Ÿæˆåˆ†ç±»å…³é”®è¯...")
        
        category_keywords = {}
        category_materials = defaultdict(list)
        
        # æŒ‰åˆ†ç±»ç»„ç»‡ç‰©æ–™æè¿°
        for material in self.materials_data:
            # å…¼å®¹Oracleï¼ˆå¤§å†™ï¼‰å’ŒPostgreSQLï¼ˆå°å†™ï¼‰å­—æ®µå
            category_name = material.get('CATEGORY_NAME') or material.get('category_name') or ''
            if category_name:
                # ç»„åˆå®Œæ•´æè¿°ï¼ˆå…¼å®¹ä¸¤ç§æ•°æ®æºï¼‰
                name = material.get('MATERIAL_NAME') or material.get('material_name') or ''
                spec = material.get('SPECIFICATION') or material.get('specification') or ''
                material_type = material.get('MATERIAL_TYPE') or material.get('material_type') or ''
                # å¦‚æœæœ‰normalized_nameï¼ˆPostgreSQLæ¸…æ´—åæ•°æ®ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨
                normalized = material.get('normalized_name') or ''
                full_desc = normalized if normalized else f"{name} {spec} {material_type}".strip()
                
                if full_desc:
                    category_materials[category_name].append(full_desc)
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»æå–å…³é”®è¯
        for category_name, descriptions in category_materials.items():
            if len(descriptions) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬æ‰åˆ†æ
                keywords = self._extract_category_keywords(category_name, descriptions)
                if keywords:
                    category_keywords[category_name] = keywords
        
        logger.info(f"âœ… ä¸º {len(category_keywords)} ä¸ªåˆ†ç±»ç”Ÿæˆäº†å…³é”®è¯")
        return category_keywords
    
    def _extract_category_keywords(self, category_name: str, descriptions: List[str]) -> List[str]:
        """
        ä¸ºç‰¹å®šåˆ†ç±»æå–å…³é”®è¯ï¼ˆæ”¹è¿›ç‰ˆv2.0ï¼‰
        
        æ”¹è¿›ç‚¹ï¼š
        1. æ‹†åˆ†åˆ†ç±»åï¼Œæå–æœ‰æ„ä¹‰çš„è¯æ ¹ï¼ˆå¦‚"ç»´ä¿®ç±»"â†’["ç»´ä¿®","ç±»"]ï¼‰
        2. é™ä½è¯é¢‘é˜ˆå€¼åˆ°15%ï¼ŒåŒ…å«æ›´å¤šç‰¹å¾è¯
        3. å¢åŠ ç‰¹æ®Šåˆ†ç±»çš„æ‰‹åŠ¨å…³é”®è¯è¡¥å……
        
        Args:
            category_name: åˆ†ç±»åç§°
            descriptions: è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰ç‰©æ–™æè¿°
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨ï¼ˆæœ€å¤š15ä¸ªï¼‰
        """
        keywords = set()
        
        # 1. æ·»åŠ åˆ†ç±»åæœ¬èº«
        keywords.add(category_name)
        
        # 2. æ‹†åˆ†åˆ†ç±»åï¼Œæå–è¯æ ¹ï¼ˆå»é™¤"ç±»"ç­‰åç¼€ï¼‰
        # ä¾‹å¦‚ï¼š"ç»´ä¿®ç±»" â†’ ["ç»´ä¿®", "ç»´ä¿®ç±»"]
        #      "åŠŸç‡å•å…ƒ" â†’ ["åŠŸç‡", "å•å…ƒ", "åŠŸç‡å•å…ƒ"]
        category_words = re.findall(r'[\u4e00-\u9fff]+', category_name)
        for word in category_words:
            if len(word) >= 2:
                keywords.add(word)
                # å»é™¤"ç±»"åç¼€
                if word.endswith('ç±»') and len(word) > 2:
                    keywords.add(word[:-1])
        
        # 3. ç‰¹æ®Šåˆ†ç±»çš„æ‰‹åŠ¨å…³é”®è¯è¡¥å……ï¼ˆåŸºäºä¸šåŠ¡çŸ¥è¯†ï¼‰
        special_keywords = {
            'ç»´ä¿®ç±»': ['ç»´ä¿®', 'åŠŸç‡å•å…ƒ', 'åŠŸç‡æ¨¡å—', 'æ§åˆ¶å•å…ƒ', 'é©±åŠ¨å•å…ƒ', 'ç”µæ§å•å…ƒ', 'å˜é¢‘å•å…ƒ', 'æ•´æµå•å…ƒ', 'é€†å˜å•å…ƒ'],
            'ç”µæ°”ç±»': ['ç”µæ°”', 'ç”µæœº', 'å˜é¢‘å™¨', 'æ¥è§¦å™¨', 'æ–­è·¯å™¨', 'ç»§ç”µå™¨'],
            'æ¶²å‹ç±»': ['æ¶²å‹', 'æ²¹æ³µ', 'æ²¹ç¼¸', 'æ¶²å‹é˜€', 'æº¢æµé˜€'],
            'æ°”åŠ¨ç±»': ['æ°”åŠ¨', 'æ°”ç¼¸', 'æ°”åŠ¨é˜€', 'ç”µç£é˜€'],
            'è½´æ‰¿ç±»': ['è½´æ‰¿', 'æ»šåŠ¨è½´æ‰¿', 'æ»‘åŠ¨è½´æ‰¿', 'æ·±æ²Ÿçƒ'],
            'å¯†å°ç±»': ['å¯†å°', 'å¯†å°åœˆ', 'Oå‹åœˆ', 'éª¨æ¶æ²¹å°'],
        }
        
        if category_name in special_keywords:
            keywords.update(special_keywords[category_name])
        
        # 4. ä»ç‰©æ–™æè¿°ä¸­æå–é«˜é¢‘è¯
        all_words = []
        for desc in descriptions:
            words = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z]+', desc)
            all_words.extend(words)
        
        # ç»Ÿè®¡è¯é¢‘
        word_counter = Counter(all_words)
        
        # 5. é€‰æ‹©å‡ºç°é¢‘ç‡ >= 15% çš„è¯ä½œä¸ºå…³é”®è¯ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        threshold = max(1, len(descriptions) * 0.15)
        for word, count in word_counter.items():
            if count >= threshold and len(word) >= 2:
                # è¿‡æ»¤æ‰å¤ªå¸¸è§çš„é€šç”¨è¯å’Œå•ä½
                if word not in {'mm', 'MM', 'kg', 'KG', 'm', 'M', 'g', 'G', 'cm', 'CM', 'L', 'l', 'mpa', 'MPa'}:
                    keywords.add(word)
        
        # 6. é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œè¿”å›æœ€é«˜é¢‘çš„15ä¸ªï¼ˆå¢åŠ åˆ°15ä¸ªï¼‰
        sorted_keywords = sorted(keywords, 
                                key=lambda w: word_counter.get(w, 0), 
                                reverse=True)
        return sorted_keywords[:15]
    
    def detect_material_category(self, description: str) -> Tuple[str, float]:
        """æ£€æµ‹ç‰©æ–™ç±»åˆ«"""
        description_lower = description.lower()
        category_scores = {}
        
        for category, keywords in self.category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in description_lower)
            if score > 0:
                category_scores[category] = score / len(keywords)
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            confidence = category_scores[best_category]
            return best_category, confidence
        
        return 'general', 0.1
    
    def generate_category_statistics(self) -> Dict[str, any]:
        """ç”Ÿæˆç±»åˆ«ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("ğŸ“ˆ ç”Ÿæˆç±»åˆ«ç»Ÿè®¡ä¿¡æ¯...")
        
        category_stats = defaultdict(int)
        detection_results = []
        
        for material in self.materials_data:
            name = material.get('MATERIAL_NAME', '')
            spec = material.get('SPECIFICATION', '')
            model = material.get('MODEL', '')
            
            full_desc = f"{name} {spec} {model}".strip()
            category, confidence = self.detect_material_category(full_desc)
            
            category_stats[category] += 1
            detection_results.append({
                'erp_code': material.get('ERP_CODE'),
                'description': full_desc,
                'detected_category': category,
                'confidence': confidence
            })
        
        return {
            'category_distribution': dict(category_stats),
            'total_materials': len(self.materials_data),
            'detection_results': detection_results,
            'coverage_rate': len([r for r in detection_results if r['confidence'] > 0.3]) / len(detection_results)
        }
    
    def save_knowledge_base(self, output_dir: str = '.'):
        """
        ä¿å­˜ç”Ÿæˆçš„çŸ¥è¯†åº“
        
        è¿™æ˜¯é¡¹ç›®çš„æ ¸å¿ƒè¾“å‡ºï¼ŒåŒ…å«ï¼š
        1. æå–è§„åˆ™
        2. åŒä¹‰è¯è¯å…¸
        3. åˆ†ç±»å…³é”®è¯
        4. ç»Ÿè®¡æŠ¥å‘Š
        """
        import os
        # ä¸åˆ›å»ºå­ç›®å½•ï¼Œç›´æ¥åœ¨å½“å‰ç›®å½•ç”Ÿæˆ
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç”Ÿæˆå¹¶ä¿å­˜æå–è§„åˆ™
        logger.info("ğŸ”§ ç”Ÿæˆæå–è§„åˆ™...")
        rules = self.generate_extraction_rules()
        rules_file = f"{output_dir}/standardized_extraction_rules_{timestamp}.json"
        with open(rules_file, 'w', encoding='utf-8') as f:
            json.dump(rules, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ æå–è§„åˆ™å·²ä¿å­˜åˆ°: {rules_file}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜åŒä¹‰è¯å…¸ï¼ˆå­—å…¸æ ¼å¼ï¼Œä¾›APIä½¿ç”¨ï¼‰
        logger.info("ğŸ“š ç”ŸæˆåŒä¹‰è¯å…¸...")
        synonyms = self.generate_synonym_dictionary()
        dict_file = f"{output_dir}/standardized_synonym_dictionary_{timestamp}.json"
        with open(dict_file, 'w', encoding='utf-8') as f:
            json.dump(synonyms, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ åŒä¹‰è¯å…¸å·²ä¿å­˜åˆ°: {dict_file}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜åŒä¹‰è¯è®°å½•ï¼ˆæ‰å¹³æ ¼å¼ï¼Œä¾›æ•°æ®åº“å¯¼å…¥ï¼‰
        logger.info("ğŸ”„ ç”ŸæˆåŒä¹‰è¯è®°å½•...")
        synonym_records = self.generate_synonym_records()
        records_file = f"{output_dir}/standardized_synonym_records_{timestamp}.json"
        with open(records_file, 'w', encoding='utf-8') as f:
            json.dump(synonym_records, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ åŒä¹‰è¯è®°å½•å·²ä¿å­˜åˆ°: {records_file}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜åˆ†ç±»å…³é”®è¯
        logger.info("ğŸ·ï¸ ç”Ÿæˆåˆ†ç±»å…³é”®è¯...")
        category_keywords_file = f"{output_dir}/standardized_category_keywords_{timestamp}.json"
        with open(category_keywords_file, 'w', encoding='utf-8') as f:
            json.dump(self.category_keywords, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ åˆ†ç±»å…³é”®è¯å·²ä¿å­˜åˆ°: {category_keywords_file}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        logger.info("ğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        stats = self.generate_category_statistics()
        stats_file = f"{output_dir}/category_statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {stats_file}")
        
        # ç”Ÿæˆå¯è¯»çš„è§„åˆ™æ–‡æ¡£
        self._generate_rules_documentation(rules, f"{output_dir}/rules_documentation_{timestamp}.md")
        
        # æ¸…ç†æ—§æ–‡ä»¶
        self._cleanup_old_files(output_dir)
        
        return {
            'rules_file': rules_file,
            'dictionary_file': dict_file,
            'synonym_records_file': records_file,  # æ–°å¢ï¼šæ•°æ®åº“å¯¼å…¥ç”¨
            'category_keywords_file': category_keywords_file,
            'statistics_file': stats_file,
            'total_rules': len(rules),
            'total_synonyms': len(synonyms),
            'total_synonym_records': len(synonym_records),  # æ–°å¢
            'total_materials_analyzed': len(self.materials_data)
        }
    
    def _cleanup_old_files(self, output_dir: str):
        """æ¸…ç†æ—§çš„ç”Ÿæˆæ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°ç‰ˆæœ¬"""
        import glob
        import os
        
        file_patterns = [
            'standardized_extraction_rules_*.json',
            'standardized_synonym_dictionary_*.json',
            'standardized_synonym_records_*.json',  # æ–°å¢ï¼šåŒä¹‰è¯è®°å½•æ–‡ä»¶
            'standardized_category_keywords_*.json',
            'category_statistics_*.json',
            'rules_documentation_*.md'
        ]
        
        for pattern in file_patterns:
            files = glob.glob(os.path.join(output_dir, pattern))
            if len(files) > 1:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤é™¤æœ€æ–°æ–‡ä»¶å¤–çš„æ‰€æœ‰æ–‡ä»¶
                files.sort(key=os.path.getmtime)
                for old_file in files[:-1]:
                    try:
                        os.remove(old_file)
                        logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {old_file}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥ {old_file}: {e}")
    
    def _generate_rules_documentation(self, rules: List[Dict], output_file: str):
        """ç”Ÿæˆè§„åˆ™æ–‡æ¡£"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# ç‰©æ–™å±æ€§æå–è§„åˆ™æ–‡æ¡£\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ•°æ®æ¥æº:** Oracle ERPç³»ç»Ÿ\n")
            f.write(f"**è§„åˆ™æ€»æ•°:** {len(rules)}\n")
            f.write(f"**æ•°æ®å¤„ç†æ ‡å‡†:** ç»Ÿä¸€æ ‡å‡†åŒ–å¤„ç†ï¼ˆå…¨è§’è½¬åŠè§’ã€ç©ºæ ¼æ¸…ç†ã€å¤§å°å†™å˜ä½“ï¼‰\n\n")
            
            # æŒ‰ç±»åˆ«åˆ†ç»„
            category_rules = defaultdict(list)
            for rule in rules:
                category_rules[rule['material_category']].append(rule)
            
            for category, cat_rules in category_rules.items():
                f.write(f"## {category.upper()} ç±»åˆ«è§„åˆ™\n\n")
                
                for rule in sorted(cat_rules, key=lambda x: x['priority'], reverse=True):
                    f.write(f"### {rule['rule_name']}\n")
                    f.write(f"- **å±æ€§åç§°:** {rule['attribute_name']}\n")
                    f.write(f"- **æ­£åˆ™è¡¨è¾¾å¼:** `{rule['regex_pattern']}`\n")
                    f.write(f"- **ä¼˜å…ˆçº§:** {rule['priority']}\n")
                    f.write(f"- **ç½®ä¿¡åº¦:** {rule.get('confidence', 'N/A')}\n")
                    f.write(f"- **æè¿°:** {rule['description']}\n")
                    if rule.get('example_input'):
                        f.write(f"- **ç¤ºä¾‹è¾“å…¥:** {rule['example_input']}\n")
                        f.write(f"- **ç¤ºä¾‹è¾“å‡º:** {rule['example_output']}\n")
                    f.write("\n")


async def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†åº“ç”Ÿæˆæµç¨‹
    
    è¿™æ˜¯é¡¹ç›®æ•°æ®åŸºç¡€è®¾æ–½çš„ç»Ÿä¸€å…¥å£
    """
    logger.info("ğŸš€ å¯åŠ¨ç‰©æ–™çŸ¥è¯†åº“ç”Ÿæˆå™¨ (ç»Ÿä¸€ç‰ˆæœ¬)")
    
    try:
        # æ£€æŸ¥ä¾èµ–
        logger.info("ğŸ” æ£€æŸ¥ä¾èµ–æ¨¡å—...")
        
        try:
            import oracledb
            logger.info(f"âœ… oracledbæ¨¡å—å·²å®‰è£…ï¼Œç‰ˆæœ¬: {oracledb.__version__}")
        except ImportError:
            logger.error("âŒ oracledbæ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install oracledb")
            return False
        
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = MaterialKnowledgeGenerator()
        
        # æ£€æŸ¥Oracleè¿æ¥
        logger.info("ğŸ”— æµ‹è¯•Oracleæ•°æ®åº“è¿æ¥...")
        if not generator.oracle.test_connection():
            logger.error("âŒ Oracleæ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return False
        
        logger.info("âœ… Oracleæ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # åŠ è½½æ•°æ®
        await generator.load_all_data()
        
        # ç”Ÿæˆå¹¶ä¿å­˜çŸ¥è¯†åº“
        logger.info("ğŸ’¾ ä¿å­˜çŸ¥è¯†åº“...")
        result = generator.save_knowledge_base()
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        logger.info("=" * 80)
        logger.info("ğŸ‰ çŸ¥è¯†åº“ç”Ÿæˆå®Œæˆï¼ç»“æœæ‘˜è¦:")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š æ•°æ®åˆ†æ:")
        logger.info(f"  - ç‰©æ–™æ€»æ•°: {result['total_materials_analyzed']:,}")
        logger.info(f"  - åˆ†ç±»æ€»æ•°: {len(generator.categories_data)}")
        logger.info(f"  - å•ä½æ€»æ•°: {len(generator.units_data)}")
        
        logger.info(f"ğŸ”§ çŸ¥è¯†åº“ç”Ÿæˆ:")
        logger.info(f"  - æå–è§„åˆ™: {result['total_rules']} æ¡")
        logger.info(f"  - åŒä¹‰è¯ç»„: {result['total_synonyms']} ç»„")
        
        logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        logger.info(f"  - æå–è§„åˆ™: {result['rules_file']}")
        logger.info(f"  - åŒä¹‰è¯å…¸: {result['dictionary_file']}")
        logger.info(f"  - åˆ†ç±»å…³é”®è¯: {result['category_keywords_file']}")
        logger.info(f"  - ç»Ÿè®¡æŠ¥å‘Š: {result['statistics_file']}")
        
        # ç”Ÿæˆç±»åˆ«ç»Ÿè®¡
        stats = generator.generate_category_statistics()
        logger.info(f"ğŸ“ˆ ç±»åˆ«æ£€æµ‹ç»Ÿè®¡:")
        logger.info(f"  - æ£€æµ‹è¦†ç›–ç‡: {stats['coverage_rate']:.1%}")
        
        top_categories = sorted(stats['category_distribution'].items(), 
                               key=lambda x: x[1], reverse=True)[:10]
        logger.info(f"  - å‰10å¤§ç±»åˆ«:")
        for category, count in top_categories:
            logger.info(f"    * {category}: {count:,} æ¡")
        
        logger.info("=" * 80)
        logger.info("âœ… ç‰©æ–™çŸ¥è¯†åº“ç”Ÿæˆå®Œæˆï¼")
        logger.info("ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
        logger.info("  1. æŸ¥çœ‹ç”Ÿæˆçš„è§„åˆ™æ–‡æ¡£ï¼Œäº†è§£æå–è§„åˆ™è¯¦æƒ…")
        logger.info("  2. æ£€æŸ¥åŒä¹‰è¯å…¸ï¼Œæ ¹æ®éœ€è¦è¿›è¡Œå¾®è°ƒ")
        logger.info("  3. è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯è§„åˆ™æ•ˆæœ")
        logger.info("  4. å°†çŸ¥è¯†åº“å¯¼å…¥åˆ°æŸ¥é‡ç³»ç»Ÿæ•°æ®åº“")
        logger.info("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False


def quick_generate():
    """å¿«é€Ÿç”Ÿæˆæ¥å£ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    return asyncio.run(main())


if __name__ == "__main__":
    import asyncio
    success = asyncio.run(main())
    
    if success:
        print("\nğŸŠ æ­å–œï¼ç‰©æ–™çŸ¥è¯†åº“ç”ŸæˆæˆåŠŸï¼")
    else:
        print("\nğŸ’¥ ç”Ÿæˆå¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
    
    sys.exit(0 if success else 1)
