"""
åŸºäºOracleçœŸå®æ•°æ®ç”Ÿæˆæ ‡å‡†åŒ–çš„è§„åˆ™å’Œè¯å…¸
å°†åˆ†æç»“æœè½¬æ¢ä¸ºå¯ç›´æ¥ä½¿ç”¨çš„è§„åˆ™å’Œè¯å…¸æ–‡ä»¶
"""

import json
import logging
from datetime import datetime
from typing import Dict, List
import re
import glob
import os

# å…¨è§’åŠè§’å­—ç¬¦æ˜ å°„è¡¨
FULLWIDTH_TO_HALFWIDTH = {
    # å…¨è§’æ•°å­— â†’ åŠè§’æ•°å­—
    'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
    'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9',
    # å…¨è§’å­—æ¯ â†’ åŠè§’å­—æ¯
    'ï¼¡': 'A', 'ï¼¢': 'B', 'ï¼£': 'C', 'ï¼¤': 'D', 'ï¼¥': 'E', 'ï¼¦': 'F',
    'ï¼§': 'G', 'ï¼¨': 'H', 'ï¼©': 'I', 'ï¼ª': 'J', 'ï¼«': 'K', 'ï¼¬': 'L',
    'ï¼­': 'M', 'ï¼®': 'N', 'ï¼¯': 'O', 'ï¼°': 'P', 'ï¼±': 'Q', 'ï¼²': 'R',
    'ï¼³': 'S', 'ï¼´': 'T', 'ï¼µ': 'U', 'ï¼¶': 'V', 'ï¼·': 'W', 'ï¼¸': 'X',
    'ï¼¹': 'Y', 'ï¼º': 'Z',
    'ï½': 'a', 'ï½‚': 'b', 'ï½ƒ': 'c', 'ï½„': 'd', 'ï½…': 'e', 'ï½†': 'f',
    'ï½‡': 'g', 'ï½ˆ': 'h', 'ï½‰': 'i', 'ï½Š': 'j', 'ï½‹': 'k', 'ï½Œ': 'l',
    'ï½': 'm', 'ï½': 'n', 'ï½': 'o', 'ï½': 'p', 'ï½‘': 'q', 'ï½’': 'r',
    'ï½“': 's', 'ï½”': 't', 'ï½•': 'u', 'ï½–': 'v', 'ï½—': 'w', 'ï½˜': 'x',
    'ï½™': 'y', 'ï½š': 'z',
    # å…¨è§’ç¬¦å· â†’ åŠè§’ç¬¦å·
    'Ã—': 'x',      # å…¨è§’ä¹˜å· â†’ åŠè§’x
    'ï¼Š': '*',     # å…¨è§’æ˜Ÿå· â†’ åŠè§’æ˜Ÿå·
    'ï¼ˆ': '(',     # å…¨è§’å·¦æ‹¬å· â†’ åŠè§’å·¦æ‹¬å·
    'ï¼‰': ')',     # å…¨è§’å³æ‹¬å· â†’ åŠè§’å³æ‹¬å·
    'ï¼»': '[',     # å…¨è§’å·¦æ–¹æ‹¬å· â†’ åŠè§’å·¦æ–¹æ‹¬å·
    'ï¼½': ']',     # å…¨è§’å³æ–¹æ‹¬å· â†’ åŠè§’å³æ–¹æ‹¬å·
    'ï¼': '-',     # å…¨è§’å‡å· â†’ åŠè§’å‡å·
    'ï¼‹': '+',     # å…¨è§’åŠ å· â†’ åŠè§’åŠ å·
    'ï¼': '=',     # å…¨è§’ç­‰å· â†’ åŠè§’ç­‰å·
    'ï¼': '/',     # å…¨è§’æ–œæ  â†’ åŠè§’æ–œæ 
    'ï¼š': ':',     # å…¨è§’å†’å· â†’ åŠè§’å†’å·
    'ï¼›': ';',     # å…¨è§’åˆ†å· â†’ åŠè§’åˆ†å·
    'ï¼Œ': ',',     # å…¨è§’é€—å· â†’ åŠè§’é€—å·
    'ï¼': '.',     # å…¨è§’å¥å· â†’ åŠè§’å¥å·
    'ã€€': ' ',     # å…¨è§’ç©ºæ ¼ â†’ åŠè§’ç©ºæ ¼
}

def cleanup_old_files():
    """æ¸…ç†æ—§çš„ç”Ÿæˆæ–‡ä»¶ï¼Œä¿æŒç›®å½•æ•´æ´"""
    logger.info("ğŸ§¹ æ¸…ç†æ—§çš„ç”Ÿæˆæ–‡ä»¶...")
    
    # å®šä¹‰è¦æ¸…ç†çš„æ–‡ä»¶æ¨¡å¼
    patterns = [
        'standardized_extraction_rules_*.json',
        'standardized_synonym_dictionary_*.json', 
        'standardized_category_keywords_*.json',
        'standardized_rules_usage_*.md'
    ]
    
    cleaned_count = 0
    for pattern in patterns:
        files = glob.glob(pattern)
        logger.info(f"  ğŸ” æ‰¾åˆ° {pattern} åŒ¹é…çš„æ–‡ä»¶: {files}")
        
        # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼Œæœ€åä¸€ä¸ªæ˜¯æœ€æ–°çš„ï¼‰
        if len(files) > 1:
            files.sort()
            old_files = files[:-1]  # é™¤äº†æœ€æ–°çš„æ–‡ä»¶å¤–ï¼Œå…¶ä»–éƒ½æ˜¯æ—§æ–‡ä»¶
            logger.info(f"  ğŸ“‹ å°†ä¿ç•™æœ€æ–°æ–‡ä»¶: {files[-1]}")
            logger.info(f"  ğŸ—‘ï¸ å°†åˆ é™¤æ—§æ–‡ä»¶: {old_files}")
            
            for old_file in old_files:
                try:
                    os.remove(old_file)
                    logger.info(f"  âœ… åˆ é™¤æ—§æ–‡ä»¶: {old_file}")
                    cleaned_count += 1
                except Exception as e:
                    logger.warning(f"  âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {old_file}: {e}")
        elif len(files) == 1:
            logger.info(f"  âœ¨ åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œæ— éœ€æ¸…ç†: {files[0]}")
        else:
            logger.info(f"  ğŸ“­ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶")
    
    if cleaned_count > 0:
        logger.info(f"ğŸ‰ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ—§æ–‡ä»¶")
    else:
        logger.info("âœ¨ ç›®å½•å·²ç»æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ¸…ç†")

def normalize_fullwidth_to_halfwidth(text: str) -> str:
    """
    å°†æ–‡æœ¬ä¸­çš„å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        è½¬æ¢åçš„æ–‡æœ¬
    """
    if not text:
        return text
        
    result = text
    for fullwidth, halfwidth in FULLWIDTH_TO_HALFWIDTH.items():
        result = result.replace(fullwidth, halfwidth)
    
    return result

def normalize_text_standard(text: str) -> str:
    """
    æ ‡å‡†æ–‡æœ¬æ ‡å‡†åŒ–å¤„ç†ï¼šå…¨è§’åŠè§’è½¬æ¢ + ç©ºæ ¼æ¸…ç† + å¤§å°å†™å¤„ç†
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        æ ‡å‡†åŒ–åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # 1. å…¨è§’åŠè§’è½¬æ¢
    result = normalize_fullwidth_to_halfwidth(text)
    
    # 2. å»é™¤é¦–å°¾ç©ºæ ¼
    result = result.strip()
    
    # 3. å°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    result = re.sub(r'\s+', ' ', result)
    
    return result

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def generate_standardized_rules():
    """ç”Ÿæˆæ ‡å‡†åŒ–çš„è§„åˆ™å’Œè¯å…¸ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå…¨è§’åŠè§’å­—ç¬¦å¤„ç†ï¼‰"""
    logger.info("ğŸ”§ å¼€å§‹ç”Ÿæˆæ ‡å‡†åŒ–è§„åˆ™å’Œè¯å…¸ï¼ˆå…¨è§’åŠè§’å¢å¼ºç‰ˆï¼‰...")
    
    # è¯»å–æœ€æ–°çš„åˆ†æç»“æœ
    analysis_file = "oracle_data_analysis_20251002_184248.json"
    
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    logger.info(f"âœ… å·²åŠ è½½åˆ†ææ•°æ®: {analysis_file}")
    logger.info("ğŸˆ¶ å¯ç”¨å…¨è§’åŠè§’å­—ç¬¦æ ‡å‡†åŒ–å¤„ç†...")
    
    # 1. ç”Ÿæˆæ ‡å‡†åŒ–æå–è§„åˆ™
    extraction_rules = generate_extraction_rules(analysis_data)
    
    # 2. ç”Ÿæˆæ ‡å‡†åŒ–åŒä¹‰è¯å…¸
    synonym_dictionary = generate_synonym_dictionary(analysis_data)
    
    # 3. ç”Ÿæˆç‰©æ–™ç±»åˆ«å…³é”®è¯
    category_keywords = generate_category_keywords(analysis_data)
    
    # 4. ä¿å­˜æ ‡å‡†åŒ–æ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜æå–è§„åˆ™
    rules_file = f"standardized_extraction_rules_{timestamp}.json"
    with open(rules_file, 'w', encoding='utf-8') as f:
        json.dump(extraction_rules, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ æå–è§„åˆ™å·²ä¿å­˜: {rules_file}")
    
    # ä¿å­˜åŒä¹‰è¯å…¸
    dict_file = f"standardized_synonym_dictionary_{timestamp}.json"
    with open(dict_file, 'w', encoding='utf-8') as f:
        json.dump(synonym_dictionary, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ åŒä¹‰è¯å…¸å·²ä¿å­˜: {dict_file}")
    
    # ä¿å­˜ç±»åˆ«å…³é”®è¯
    keywords_file = f"standardized_category_keywords_{timestamp}.json"
    with open(keywords_file, 'w', encoding='utf-8') as f:
        json.dump(category_keywords, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ ç±»åˆ«å…³é”®è¯å·²ä¿å­˜: {keywords_file}")
    
    # ç”Ÿæˆä½¿ç”¨è¯´æ˜æ–‡æ¡£
    generate_usage_documentation(extraction_rules, synonym_dictionary, category_keywords, timestamp)
    
    return {
        'rules_file': rules_file,
        'dictionary_file': dict_file,
        'keywords_file': keywords_file,
        'total_rules': len(extraction_rules),
        'total_synonyms': sum(len(variants) for variants in synonym_dictionary.values()),
        'total_categories': len(category_keywords)
    }

def generate_extraction_rules(analysis_data: Dict) -> List[Dict]:
    """ç”Ÿæˆæ ‡å‡†åŒ–çš„æå–è§„åˆ™"""
    logger.info("ğŸ”§ ç”Ÿæˆæ ‡å‡†åŒ–æå–è§„åˆ™...")
    
    rules = []
    
    # åŸºäºçœŸå®æ•°æ®çš„å°ºå¯¸æ¨¡å¼
    size_patterns = analysis_data['material_patterns']['size_patterns']
    if size_patterns:
        # åˆ†ææœ€å¸¸è§çš„å°ºå¯¸æ ¼å¼
        common_sizes = [pattern[0] for pattern in size_patterns[:100]]  # å–å‰100ä¸ª
        
        rules.append({
            'id': 'size_spec_metric',
            'name': 'å…¬åˆ¶å°ºå¯¸è§„æ ¼æå–',
            'category': 'general',
            'attribute': 'size_specification',
            'pattern': r'(?:M|Î¦|Ï†|DN|ï¼­|ï¼¤ï¼®)?(\d+(?:\.\d+)?[Ã—*xXÃ—ï¼Šï½˜ï¼¸]\d+(?:\.\d+)?(?:[Ã—*xXÃ—ï¼Šï½˜ï¼¸]\d+(?:\.\d+)?)?)',
            'priority': 100,
            'description': f'åŸºäº{len(size_patterns)}ä¸ªçœŸå®å°ºå¯¸æ ·æœ¬ï¼Œæ”¯æŒå…¨è§’åŠè§’å­—ç¬¦çš„å…¬åˆ¶å°ºå¯¸è§„æ ¼æå–',
            'examples': common_sizes[:5],
            'data_source': 'oracle_real_data',
            'confidence': 0.95
        })
        
        rules.append({
            'id': 'thread_spec',
            'name': 'èºçº¹è§„æ ¼æå–',
            'category': 'fastener',
            'attribute': 'thread_specification',
            'pattern': r'(M|ï¼­)(\d+(?:\.\d+)?)[Ã—*xXÃ—ï¼Šï½˜ï¼¸](\d+(?:\.\d+)?)',
            'priority': 95,
            'description': 'æå–èºçº¹è§„æ ¼å¦‚M20*1.5ï¼Œæ”¯æŒå…¨è§’åŠè§’å­—ç¬¦',
            'examples': [p[0] for p in size_patterns if p[0].startswith('M')][:5],
            'data_source': 'oracle_real_data',
            'confidence': 0.98
        })
    
    # åŸºäºçœŸå®æ•°æ®çš„æè´¨æ¨¡å¼
    material_patterns = analysis_data['material_patterns']['material_patterns']
    if material_patterns:
        common_materials = [pattern[0] for pattern in material_patterns[:50]]
        material_regex = '|'.join(re.escape(m) for m in common_materials)
        
        rules.append({
            'id': 'material_type',
            'name': 'æè´¨ç±»å‹æå–',
            'category': 'general',
            'attribute': 'material_type',
            'pattern': f'({material_regex})',
            'priority': 90,
            'description': f'åŸºäº{len(material_patterns)}ç§çœŸå®æè´¨æ ·æœ¬',
            'examples': common_materials[:5],
            'data_source': 'oracle_real_data',
            'confidence': 0.92
        })
    
    # åŸºäºçœŸå®æ•°æ®çš„å“ç‰Œæ¨¡å¼
    brand_patterns = analysis_data['material_patterns']['brand_patterns']
    if brand_patterns:
        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å“ç‰Œçš„è¯ï¼ˆå¦‚DNã€PNç­‰è§„æ ¼æ ‡è¯†ï¼‰
        exclude_words = {'DN', 'PN', 'MPa', 'BAR', 'MM', 'KG', 'PCS', 'SET', 'GB', 'JB', 'HG'}
        real_brands = [p[0] for p in brand_patterns if p[0] not in exclude_words and len(p[0]) >= 3][:100]
        
        if real_brands:
            brand_regex = '|'.join(re.escape(b) for b in real_brands)
            rules.append({
                'id': 'brand_name',
                'name': 'å“ç‰Œåç§°æå–',
                'category': 'general',
                'attribute': 'brand_name',
                'pattern': f'\\b({brand_regex})\\b',
                'priority': 85,
                'description': f'åŸºäº{len(real_brands)}ä¸ªçœŸå®å“ç‰Œæ ·æœ¬',
                'examples': real_brands[:5],
                'data_source': 'oracle_real_data',
                'confidence': 0.88
            })
    
    # å‹åŠ›ç­‰çº§æå–è§„åˆ™
    rules.append({
        'id': 'pressure_rating',
        'name': 'å‹åŠ›ç­‰çº§æå–',
        'category': 'valve',
        'attribute': 'pressure_rating',
        'pattern': r'(PN\d+|(?:\d+(?:\.\d+)?(?:MPa|bar|å…¬æ–¤|kg)))',
        'priority': 88,
        'description': 'æå–å‹åŠ›ç­‰çº§å¦‚PN16, 1.6MPa',
        'examples': ['PN16', '1.6MPa', '10bar'],
        'data_source': 'pattern_analysis',
        'confidence': 0.90
    })
    
    # å…¬ç§°ç›´å¾„æå–è§„åˆ™
    rules.append({
        'id': 'nominal_diameter',
        'name': 'å…¬ç§°ç›´å¾„æå–',
        'category': 'pipe',
        'attribute': 'nominal_diameter',
        'pattern': r'(DN\d+|Î¦\d+)',
        'priority': 87,
        'description': 'æå–å…¬ç§°ç›´å¾„å¦‚DN50, Î¦100',
        'examples': ['DN50', 'DN100', 'Î¦50'],
        'data_source': 'pattern_analysis',
        'confidence': 0.95
    })
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(rules)} æ¡æ ‡å‡†åŒ–æå–è§„åˆ™")
    return rules

def generate_synonym_dictionary(analysis_data: Dict) -> Dict[str, List[str]]:
    """ç”Ÿæˆæ ‡å‡†åŒ–åŒä¹‰è¯å…¸"""
    logger.info("ğŸ“š ç”Ÿæˆæ ‡å‡†åŒ–åŒä¹‰è¯å…¸...")
    
    synonyms = {}
    
    # ä»åˆ†ææ•°æ®ä¸­æå–åŒä¹‰è¯æ˜ å°„ï¼ˆåº”ç”¨æ–‡æœ¬æ ‡å‡†åŒ–ï¼‰
    if 'synonym_mappings' in analysis_data:
        base_synonyms = analysis_data['synonym_mappings']
        # å¯¹åŒä¹‰è¯è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
        normalized_synonyms = {}
        for key, values in base_synonyms.items():
            normalized_key = normalize_text_standard(key)
            normalized_values = [normalize_text_standard(v) for v in values if normalize_text_standard(v)]
            if normalized_key and normalized_values:
                normalized_synonyms[normalized_key] = normalized_values
        synonyms.update(normalized_synonyms)
    
    # æ·»åŠ åŸºäºå°ºå¯¸æ¨¡å¼çš„åŒä¹‰è¯
    size_patterns = analysis_data['material_patterns']['size_patterns']
    size_synonyms = {}
    
    for pattern, count in size_patterns:
        if count >= 10:  # åªå¤„ç†å‡ºç°é¢‘ç‡è¾ƒé«˜çš„æ¨¡å¼
            # æ ‡å‡†åŒ–å°ºå¯¸è¡¨ç¤ºï¼ˆæ”¯æŒå…¨è§’åŠè§’ï¼‰
            normalized = re.sub(r'[Ã—*Xxï¼¸ï½˜ï¼Š]', 'x', pattern)
            variants = []
            
            # ç”Ÿæˆå˜ä½“ï¼ˆåŒ…æ‹¬å…¨è§’å­—ç¬¦ï¼‰
            if 'x' in normalized:
                variants.append(pattern.replace('x', 'Ã—'))  # å…¨è§’ä¹˜å·
                variants.append(pattern.replace('x', '*'))  # åŠè§’æ˜Ÿå·
                variants.append(pattern.replace('x', 'X'))  # åŠè§’å¤§å†™X
                variants.append(pattern.replace('x', 'ï¼Š')) # å…¨è§’æ˜Ÿå·
                variants.append(pattern.replace('x', 'ï¼¸')) # å…¨è§’å¤§å†™X
                variants.append(pattern.replace('x', 'ï½˜')) # å…¨è§’å°å†™x
            
            if variants:
                size_synonyms[normalized] = list(set(variants))
    
    synonyms.update(size_synonyms)
    
    # æ·»åŠ å…¨è§’åŠè§’å­—ç¬¦åŒä¹‰è¯
    fullwidth_halfwidth_synonyms = {}
    for fullwidth, halfwidth in FULLWIDTH_TO_HALFWIDTH.items():
        fullwidth_halfwidth_synonyms[halfwidth] = [fullwidth]
    
    synonyms.update(fullwidth_halfwidth_synonyms)
    
    # æ·»åŠ å¸¸è§çš„æè´¨åŒä¹‰è¯ï¼ˆåŒ…å«å¤§å°å†™å˜ä½“ï¼‰
    material_synonyms = {
        'ä¸é”ˆé’¢': ['304', '316', '316L', 'SS', 'ss', 'stainless steel', 'Stainless Steel', 'STAINLESS STEEL'],
        'ç¢³é’¢': ['CS', 'cs', 'carbon steel', 'Carbon Steel', 'CARBON STEEL', 'A105', '20#'],
        'åˆé‡‘é’¢': ['alloy steel', 'Alloy Steel', 'ALLOY STEEL', '40Cr', '40cr', '42CrMo', '42crmo'],
        'é“¸é“': ['cast iron', 'Cast Iron', 'CAST IRON', 'CI', 'ci', 'HT200', 'ht200', 'HT250', 'ht250'],
        'é“œ': ['é»„é“œ', 'Cu', 'cu', 'CU', 'copper', 'Copper', 'COPPER', 'brass', 'Brass', 'BRASS'],
        'é“': ['é“åˆé‡‘', 'Al', 'al', 'AL', 'aluminum', 'Aluminum', 'ALUMINUM', 'aluminium', 'Aluminium', 'ALUMINIUM']
    }
    synonyms.update(material_synonyms)
    
    # æ·»åŠ å“ç‰Œåç§°å¤§å°å†™å˜ä½“åŒä¹‰è¯
    brand_case_synonyms = {}
    common_brands = ['SKF', 'NSK', 'FAG', 'NTN', 'TIMKEN', 'INA', 'KOYO', 'NACHI', 'THK', 'IKO']
    for brand in common_brands:
        variants = []
        if brand.lower() != brand:
            variants.append(brand.lower())
        if brand.title() != brand:
            variants.append(brand.title())
        if variants:
            brand_case_synonyms[brand] = variants
    
    synonyms.update(brand_case_synonyms)
    
    # æ·»åŠ å•ä½åŒä¹‰è¯ï¼ˆåŒ…æ‹¬å…¨è§’åŠè§’å˜ä½“ï¼‰
    unit_synonyms = {
        'mm': ['æ¯«ç±³', 'MM', 'ï½ï½', 'ï¼­ï¼­'],
        'kg': ['å…¬æ–¤', 'åƒå…‹', 'KG', 'ï½‹ï½‡', 'ï¼«ï¼§'],
        'MPa': ['å…†å¸•', 'Mpa', 'mpa', 'ï¼­ï¼°ï½', 'ï½ï½ï½'],
        'bar': ['å·´', 'Bar', 'BAR', 'ï½‚ï½ï½’', 'ï¼¢ï¼¡ï¼²'],
        'ä¸ª': ['åª', 'ä»¶', 'å¥—', 'pcs', 'PCS', 'ï½ï½ƒï½“', 'ï¼°ï¼£ï¼³'],
        'DN': ['å…¬ç§°ç›´å¾„', 'dn', 'ï¼¤ï¼®', 'ï½„ï½'],
        'PN': ['å…¬ç§°å‹åŠ›', 'pn', 'ï¼°ï¼®', 'ï½ï½']
    }
    synonyms.update(unit_synonyms)
    
    # ç»Ÿè®¡å…¨è§’åŠè§’åŒä¹‰è¯æ•°é‡
    fullwidth_count = sum(1 for v in synonyms.values() if any(char in FULLWIDTH_TO_HALFWIDTH for char in str(v)))
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(synonyms)} ä¸ªåŒä¹‰è¯ç»„")
    logger.info(f"ğŸˆ¶ å…¶ä¸­åŒ…å« {len(fullwidth_halfwidth_synonyms)} ä¸ªå…¨è§’åŠè§’å­—ç¬¦æ˜ å°„")
    return synonyms

def generate_category_keywords(analysis_data: Dict) -> Dict[str, Dict]:
    """ç”Ÿæˆæ ‡å‡†åŒ–ç±»åˆ«å…³é”®è¯"""
    logger.info("ğŸ·ï¸ ç”Ÿæˆæ ‡å‡†åŒ–ç±»åˆ«å…³é”®è¯...")
    
    category_keywords = {}
    
    # ä»åˆ†ææ•°æ®ä¸­æå–ç±»åˆ«å…³é”®è¯
    if 'category_keywords' in analysis_data:
        raw_keywords = analysis_data['category_keywords']
        
        for category, keywords in raw_keywords.items():
            if len(keywords) >= 2:  # è‡³å°‘æœ‰2ä¸ªå…³é”®è¯çš„åˆ†ç±»
                category_keywords[category] = {
                    'keywords': keywords,
                    'detection_confidence': 0.8,
                    'category_type': detect_category_type(category, keywords),
                    'priority': calculate_category_priority(category, keywords)
                }
    
    # æ·»åŠ æ ‡å‡†çš„ç‰©æ–™ç±»åˆ«
    standard_categories = {
        'è½´æ‰¿': {
            'keywords': ['è½´æ‰¿', 'bearing', 'æ»šåŠ¨è½´æ‰¿', 'æ»‘åŠ¨è½´æ‰¿', 'æ·±æ²Ÿçƒ', 'åœ†é”¥æ»šå­'],
            'detection_confidence': 0.95,
            'category_type': 'mechanical',
            'priority': 100
        },
        'èºæ “': {
            'keywords': ['èºæ “', 'èºé’‰', 'èºä¸', 'bolt', 'screw', 'å†…å…­è§’', 'å¤–å…­è§’'],
            'detection_confidence': 0.95,
            'category_type': 'fastener',
            'priority': 100
        },
        'é˜€é—¨': {
            'keywords': ['é˜€', 'é˜€é—¨', 'valve', 'çƒé˜€', 'é—¸é˜€', 'æˆªæ­¢é˜€', 'è¶é˜€'],
            'detection_confidence': 0.95,
            'category_type': 'valve',
            'priority': 100
        },
        'ç®¡ä»¶': {
            'keywords': ['ç®¡', 'ç®¡é“', 'ç®¡ä»¶', 'pipe', 'tube', 'å¼¯å¤´', 'ä¸‰é€š', 'å››é€š'],
            'detection_confidence': 0.95,
            'category_type': 'pipe',
            'priority': 100
        },
        'ç”µæ°”': {
            'keywords': ['æ¥è§¦å™¨', 'ç»§ç”µå™¨', 'æ–­è·¯å™¨', 'å˜é¢‘å™¨', 'contactor', 'relay'],
            'detection_confidence': 0.90,
            'category_type': 'electrical',
            'priority': 90
        }
    }
    
    # åˆå¹¶æ ‡å‡†ç±»åˆ«
    for category, info in standard_categories.items():
        if category not in category_keywords:
            category_keywords[category] = info
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(category_keywords)} ä¸ªç±»åˆ«å…³é”®è¯ç»„")
    return category_keywords

def detect_category_type(category: str, keywords: List[str]) -> str:
    """æ£€æµ‹ç±»åˆ«ç±»å‹"""
    category_lower = category.lower()
    keywords_str = ' '.join(keywords).lower()
    
    if any(word in category_lower or word in keywords_str for word in ['è½´æ‰¿', 'bearing']):
        return 'bearing'
    elif any(word in category_lower or word in keywords_str for word in ['èºæ “', 'èºé’‰', 'bolt', 'screw']):
        return 'fastener'
    elif any(word in category_lower or word in keywords_str for word in ['é˜€', 'valve']):
        return 'valve'
    elif any(word in category_lower or word in keywords_str for word in ['ç®¡', 'pipe', 'tube']):
        return 'pipe'
    elif any(word in category_lower or word in keywords_str for word in ['ç”µ', 'electrical']):
        return 'electrical'
    else:
        return 'general'

def calculate_category_priority(category: str, keywords: List[str]) -> int:
    """è®¡ç®—ç±»åˆ«ä¼˜å…ˆçº§"""
    # åŸºäºå…³é”®è¯æ•°é‡å’Œç±»åˆ«é‡è¦æ€§
    base_priority = len(keywords) * 10
    
    # é‡è¦ç±»åˆ«åŠ æƒ
    important_categories = ['è½´æ‰¿', 'èºæ “', 'é˜€é—¨', 'ç®¡ä»¶', 'ç”µæ°”']
    if any(imp in category for imp in important_categories):
        base_priority += 20
    
    return min(base_priority, 100)

def generate_usage_documentation(rules: List[Dict], synonyms: Dict, categories: Dict, timestamp: str):
    """ç”Ÿæˆä½¿ç”¨è¯´æ˜æ–‡æ¡£"""
    doc_file = f"standardized_rules_usage_{timestamp}.md"
    
    with open(doc_file, 'w', encoding='utf-8') as f:
        f.write("# æ ‡å‡†åŒ–è§„åˆ™å’Œè¯å…¸ä½¿ç”¨è¯´æ˜\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®æ¥æº:** Oracle ERPç³»ç»Ÿ (230,421æ¡ç‰©æ–™æ•°æ®)\n\n")
        
        f.write("## ğŸ“Š ç”Ÿæˆç»Ÿè®¡\n\n")
        f.write(f"- **æå–è§„åˆ™**: {len(rules)} æ¡\n")
        f.write(f"- **åŒä¹‰è¯ç»„**: {len(synonyms)} ç»„\n")
        f.write(f"- **ç±»åˆ«å…³é”®è¯**: {len(categories)} ä¸ªç±»åˆ«\n\n")
        
        f.write("## ğŸ”§ æå–è§„åˆ™è¯´æ˜\n\n")
        for rule in rules:
            f.write(f"### {rule['name']}\n")
            f.write(f"- **ID**: `{rule['id']}`\n")
            f.write(f"- **ç±»åˆ«**: {rule['category']}\n")
            f.write(f"- **å±æ€§**: {rule['attribute']}\n")
            f.write(f"- **æ­£åˆ™è¡¨è¾¾å¼**: `{rule['pattern']}`\n")
            f.write(f"- **ä¼˜å…ˆçº§**: {rule['priority']}\n")
            f.write(f"- **ç½®ä¿¡åº¦**: {rule['confidence']}\n")
            f.write(f"- **æè¿°**: {rule['description']}\n")
            if 'examples' in rule:
                f.write(f"- **ç¤ºä¾‹**: {', '.join(rule['examples'])}\n")
            f.write("\n")
        
        f.write("## ğŸ“š åŒä¹‰è¯å…¸ä½¿ç”¨\n\n")
        f.write("åŒä¹‰è¯å…¸æŒ‰ç±»å‹ç»„ç»‡ï¼Œæ”¯æŒä»¥ä¸‹è½¬æ¢ï¼š\n\n")
        
        # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºåŒä¹‰è¯
        size_synonyms = {k: v for k, v in synonyms.items() if 'x' in k or 'Ã—' in k or '*' in k}
        material_synonyms = {k: v for k, v in synonyms.items() if k in ['ä¸é”ˆé’¢', 'ç¢³é’¢', 'åˆé‡‘é’¢', 'é“¸é“', 'é“œ', 'é“']}
        unit_synonyms = {k: v for k, v in synonyms.items() if k in ['mm', 'kg', 'MPa', 'bar', 'ä¸ª', 'DN', 'PN']}
        
        if size_synonyms:
            f.write("### å°ºå¯¸è§„æ ¼åŒä¹‰è¯\n")
            for standard, variants in list(size_synonyms.items())[:10]:
                f.write(f"- `{standard}` â† {', '.join(variants)}\n")
            f.write("\n")
        
        if material_synonyms:
            f.write("### æè´¨åŒä¹‰è¯\n")
            for standard, variants in material_synonyms.items():
                f.write(f"- `{standard}` â† {', '.join(variants)}\n")
            f.write("\n")
        
        if unit_synonyms:
            f.write("### å•ä½åŒä¹‰è¯\n")
            for standard, variants in unit_synonyms.items():
                f.write(f"- `{standard}` â† {', '.join(variants)}\n")
            f.write("\n")
        
        f.write("## ğŸ·ï¸ ç±»åˆ«æ£€æµ‹ä½¿ç”¨\n\n")
        high_priority_categories = {k: v for k, v in categories.items() if v['priority'] >= 90}
        
        for category, info in high_priority_categories.items():
            f.write(f"### {category}\n")
            f.write(f"- **å…³é”®è¯**: {', '.join(info['keywords'])}\n")
            f.write(f"- **ç±»å‹**: {info['category_type']}\n")
            f.write(f"- **ç½®ä¿¡åº¦**: {info['detection_confidence']}\n")
            f.write(f"- **ä¼˜å…ˆçº§**: {info['priority']}\n")
            f.write("\n")
    
    logger.info(f"ğŸ“„ ä½¿ç”¨è¯´æ˜æ–‡æ¡£å·²ä¿å­˜: {doc_file}")

if __name__ == "__main__":
    logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆæ ‡å‡†åŒ–è§„åˆ™å’Œè¯å…¸")
    
    # æ¸…ç†æ—§æ–‡ä»¶
    cleanup_old_files()
    
    result = generate_standardized_rules()
    
    logger.info("ğŸ‰ æ ‡å‡†åŒ–è§„åˆ™å’Œè¯å…¸ç”Ÿæˆå®Œæˆï¼")
    logger.info(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    logger.info(f"  - æå–è§„åˆ™: {result['total_rules']} æ¡")
    logger.info(f"  - åŒä¹‰è¯: {result['total_synonyms']} ä¸ª")
    logger.info(f"  - ç±»åˆ«: {result['total_categories']} ä¸ª")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  - è§„åˆ™æ–‡ä»¶: {result['rules_file']}")
    logger.info(f"  - è¯å…¸æ–‡ä»¶: {result['dictionary_file']}")
    logger.info(f"  - å…³é”®è¯æ–‡ä»¶: {result['keywords_file']}")
